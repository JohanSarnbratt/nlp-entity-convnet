{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "import json\n",
    "import re\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links5-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs, length=200)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                wordvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content):\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "    \n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "    \n",
    "    def __init__(self): #, wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "        #self.wordvecs = wordvec\n",
    "        #self.queries = queries\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        self.num_words_to_use_conv = 5\n",
    "        #self.redirects = redirects\n",
    "        #self.page_content = {}\n",
    "        #self.wikipedia_dump_fname = wikipedia_dump_fname\n",
    "        \n",
    "        #self._process_queries()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "#     def _process_queries(self):\n",
    "#         queried_pages = set()\n",
    "#         for docs, q in self.queries.iteritems():\n",
    "#             self.wordvecs.tokenize(docs)\n",
    "#             for sur, v in q.iteritems():\n",
    "#                 self.wordvecs.tokenize(sur)\n",
    "#                 for link in v['vals'].keys():\n",
    "#                     self.wordvecs.tokenize(link)\n",
    "#                     tt = WikiRegexes.convertToTitle(link)\n",
    "#                     #self.wordvecs.tokenize(tt)\n",
    "#                     queried_pages.add(tt)\n",
    "\n",
    "#         added_pages = set()\n",
    "#         for title in queried_pages:\n",
    "#             if title in self.redirects:\n",
    "#                 #self.wordvecs.tokenize(self.redirects[title])\n",
    "#                 added_pages.add(self.redirects[title])\n",
    "#         queried_pages |= added_pages\n",
    "        \n",
    "#         self.queried_pages = queried_pages\n",
    "                \n",
    "#         class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "            \n",
    "#             def readPage(ss, title, content):\n",
    "#                 tt = ss.convertToTitle(title)\n",
    "#                 if tt in queried_pages:\n",
    "#                     cnt = ss._wikiToText(content)\n",
    "#                     self.page_content[tt] = self.wordvecs.tokenize(cnt)\n",
    "        \n",
    "#         GetWikipediaWords(self.wikipedia_dump_fname).read()\n",
    "               \n",
    "        \n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_doc')\n",
    "        \n",
    "        self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "        \n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "        \n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer(\n",
    "            (None,self.document_length), \n",
    "            input_var=self.x_document_input\n",
    "        )\n",
    "    \n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=500,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=250,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.document_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        document_output_length = 200\n",
    "        \n",
    "        self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.document_drop1,\n",
    "            num_units=225,\n",
    "            name='document_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "            self.document_dens2,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "            self.document_drop2,\n",
    "            num_units=document_output_length,\n",
    "            name='document_dens3',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens3)\n",
    "        \n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "        \n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "        \n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "                \n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length), \n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "        \n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "        \n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "        \n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "        \n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "        \n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens1]\n",
    "        )\n",
    "        \n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "        \n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "        \n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "        \n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length),\n",
    "            input_var=self.x_target_words,\n",
    "        )\n",
    "        \n",
    "        self.target_words_embedding_l = EmbeddingLayer(\n",
    "            self.target_words_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_words_embedding_l,\n",
    "            name='target_wrds_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=350,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.target_words_conv1_l,\n",
    "            name='target_wrds_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "        \n",
    "        self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "            [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "             lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "        )\n",
    "        \n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_merge_l,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_embedding_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "        \n",
    "    \n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "        \n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape((self.x_matches_surface.shape[0], 1)).astype(theano.config.floatX)\n",
    "        \n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input], \n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "        \n",
    "        \n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "        \n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "        \n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "        \n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "    \n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) * \n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "    \n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff, \n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "        \n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "        \n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "        \n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "        self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "        \n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "        \n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "        \n",
    "        self.func_inputs = [\n",
    "            self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_matches_surface, self.x_link_id, self.y_answer]  # self.x_target_words,\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "        \n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "        \n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.learning_targets = []\n",
    "        \n",
    "    def compute_batch(self, isTraining=True):\n",
    "        if isTraining:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        \n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "        \n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] is not isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_gold_loc = None\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    cnt = self.wordvecs.get_location(WikiRegexes.convertToTitle(target))\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                if target_gold_loc is not None:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    self.current_target_goal += [(len(self.current_target_goal) + target_gold_loc)] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                \n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                #return\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links\n",
    "        \n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "            \n",
    "        return self.total_loss / self.total_links\n",
    "        \n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_target_id, self.current_target_goal  # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = res_vec[i]\n",
    "        self.reset_accums()\n",
    "        \n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "        \n",
    "\n",
    "        \n",
    "queries_exp = EntityVectorLinkExp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "CPU times: user 51 ms, sys: 7.05 ms, total: 58.1 ms\n",
      "Wall time: 43.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(100):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.30209956923440201),\n",
       " (1, 0.28197504320688149),\n",
       " (2, 0.27625327977931868),\n",
       " (3, 0.27066594001249433),\n",
       " (4, 0.26564457238009909),\n",
       " (5, 0.26154175658924267),\n",
       " (6, 0.25857829023985618),\n",
       " (7, 0.25554483639989822),\n",
       " (8, 0.25314297074099318),\n",
       " (9, 0.25096759465902685),\n",
       " (10, 0.24867440500802893),\n",
       " (11, 0.24699623003476359),\n",
       " (12, 0.24561750492780313),\n",
       " (13, 0.2437228883832975),\n",
       " (14, 0.24221156900446342)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.25193260436527221)\n",
      "(1, 0.25044544424895365)\n",
      "(2, 0.24861832749520102)\n",
      "(3, 0.24680458788548709)\n",
      "(4, 0.24596285032174101)\n",
      "(5, 0.24384991506907869)\n",
      "(6, 0.2437858940689287)\n",
      "(7, 0.24187204418354905)\n",
      "(8, 0.24013947445857298)\n",
      "(9, 0.23896747579956618)\n",
      "(10, 0.23744868123049398)\n",
      "(11, 0.23544743766594531)\n",
      "(12, 0.23446426546527424)\n",
      "(13, 0.23326840963182313)\n",
      "(14, 0.23203740957061897)\n",
      "(15, 0.23043360718066389)\n",
      "(16, 0.22962830958825009)\n",
      "(17, 0.22864763873605373)\n",
      "(18, 0.22630063163105227)\n",
      "(19, 0.22634391228343279)\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.22548462021102186)\n",
      "(1, 0.22753476401151757)\n",
      "(2, 0.22298776317791502)\n",
      "(3, 0.22604879541592512)\n",
      "(4, 0.22105116172538422)\n",
      "(5, 0.21943825814520188)\n",
      "(6, 0.21875800825868649)\n",
      "(7, 0.21848983722395929)\n",
      "(8, 0.21799020695030547)\n",
      "(9, 0.22275115899192863)\n",
      "(10, 0.21617527863669928)\n",
      "(11, 0.21316958732527516)\n",
      "(12, 0.21270735192963486)\n",
      "(13, 0.21063031581620217)\n",
      "(14, 0.20969470163014006)\n",
      "(15, 0.2088916856731593)\n",
      "(16, 0.21122537337921632)\n",
      "(17, 0.2081131099752335)\n",
      "(18, 0.20845197649796071)\n",
      "(19, 0.20761137411826339)\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.20535454935796019)\n",
      "(1, 0.20411033271224777)\n",
      "(2, 0.20391712185199662)\n",
      "(3, 0.20471953359811271)\n",
      "(4, 0.20291538812455293)\n",
      "(5, 0.20140318917198244)\n",
      "(6, 0.19897731353551321)\n",
      "(7, 0.19744544463204308)\n",
      "(8, 0.20007354640555775)\n",
      "(9, 0.19819391260117167)\n",
      "(10, 0.19856921768681424)\n",
      "(11, 0.19809518961618588)\n",
      "(12, 0.1943694366967742)\n",
      "(13, 0.19219715638818261)\n",
      "(14, 0.19213015940305117)\n",
      "(15, 0.19119296779002301)\n",
      "(16, 0.19165825240832535)\n",
      "(17, 0.19095094401567475)\n",
      "(18, 0.18898030394268406)\n",
      "(19, 0.18836112160600815)\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256091, 256091)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_target_goal), len(queries_exp.current_target_inputa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.2287285061402835e-05), (1, 1.1753841393782684e-05)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_measured = 0\n",
    "all_correct = 0\n",
    "all_trained = 0\n",
    "for qu in queries.values():\n",
    "    for en in qu.values():\n",
    "        for e in en:\n",
    "            if en['gold']:\n",
    "                if all_trained > 10000:\n",
    "                    break\n",
    "                all_measured += 1\n",
    "                all_trained += len(en['vals'].values())\n",
    "                m = max(en['vals'].values())\n",
    "                if en['vals'][en['gold']] == m:\n",
    "                    all_correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3221830985915493"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(all_correct) / all_measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u', 1921 August 8 , 1984 -RRB- , born in [Philadelphia] , was an American television and motion picture actor .': {u'gold': u'Philadelphia',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.15610045,\n",
       "   u'2014\\u201315 Philadelphia Flyers season': 0,\n",
       "   u'Advanta Championships of Philadelphia': 0.27314246,\n",
       "   u'Ala\\u015fehir': 0.55046105,\n",
       "   u'Philadelphia': 0.81635225,\n",
       "   u'Philadelphia (film)': 0.36648035,\n",
       "   u'Philadelphia (magazine)': 0.71393275,\n",
       "   u'Philadelphia 76ers': 0.48888707,\n",
       "   u'Philadelphia County, Pennsylvania': 0.35949296,\n",
       "   u'Philadelphia Eagles': 0.65391666,\n",
       "   u'Philadelphia Flyers': 0.52336681,\n",
       "   u'Philadelphia International Airport': 0.60837102,\n",
       "   u'Philadelphia Phantoms': 0.45331526,\n",
       "   u'Philadelphia Phillies': 0.66046309,\n",
       "   u'Philadelphia Union': 0.41878968,\n",
       "   u'Philadelphia, Mississippi': 0.6015842,\n",
       "   u'Philadelphia, Pennsylvania': 0.81635225,\n",
       "   u'Roman Catholic Archdiocese of Philadelphia': 0.40621081,\n",
       "   u'U.S. Pro Indoor': 0.25876719,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"-RRB- and as Roger '' Cutes '' Buell on '' [The Mothers-in-Law] '' .\": {u'gold': u'The Mothers-in-Law',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.19010547,\n",
       "   u'Mothers-in-Law': 0.60679632,\n",
       "   u'The Mothers-in-Law': 0.44943121,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Bascombe '' -RRB- about [Gordon MacRae] -LRB- '' Billy Bigelow '' -RRB- in the famous ''\": {u'gold': u'Gordon MacRae',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.27002835,\n",
       "   u'Calum MacRae': 0,\n",
       "   u'Gordon MacRae': 0.84788364,\n",
       "   u'Jade MacRae': 0.66468281,\n",
       "   u'MacRae': 0.63091677,\n",
       "   u'MacRae (surname)': 0,\n",
       "   u'William MacRae': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Birds '' and a larger role in the original '' [Invasion of the Body Snatchers] '' , having portrayed a physician in the '' book-end\": {u'gold': u'Invasion of the Body Snatchers',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.34645525,\n",
       "   u'2003 invasion of Iraq': 0.69917756,\n",
       "   u'Battle of France': 0.58124548,\n",
       "   u'Invasion': 0.78025007,\n",
       "   u'Invasion (1965 film)': 0,\n",
       "   u'Invasion (1997 film)': 0,\n",
       "   u'Invasion (TV series)': 0.54831666,\n",
       "   u'Invasion Body Snatchers': 0,\n",
       "   u'Invasion of': 0,\n",
       "   u'Invasion of Normandy': 0.54813266,\n",
       "   u'Invasion of Poland': 0.65013593,\n",
       "   u'Invasion of Poland (1939)': 0.65013593,\n",
       "   u'Invasion of the': 0,\n",
       "   u'Invasion of the Body': 0,\n",
       "   u'Invasion of the Body Snatchers': 0.79118353,\n",
       "   u'Invasion of the Body Snatchers (1956 film)': 0.79118353,\n",
       "   u'Invasion of the Body Snatchers (1978 film)': 0.41384944,\n",
       "   u'Invasion of the Body Snatchers (1978 film) ': 0,\n",
       "   u'The Invasion (professional wrestling)': 0.29262704,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Cooley on '' The Dick Van Dyke Show '' , [Fred Rutherford] on '' Leave It to Beaver '' -LRB- Mr.\": {u'gold': u'Fred Rutherford',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.26388314,\n",
       "   u'Alexander Cameron Rutherford': 0.34331977,\n",
       "   u'Ernest Rutherford': 0.53544527,\n",
       "   u'Fred Rutherford': 0.47453675,\n",
       "   u'Jock Rutherford': 0.36497545,\n",
       "   u'John Rutherford (rugby union)': 0,\n",
       "   u'Johnny Rutherford': 0.62397367,\n",
       "   u'Rutherford': 0.19731358,\n",
       "   u'Rutherford (NJT station)': 0.23570213,\n",
       "   u'Rutherford AVA': 0.22080871,\n",
       "   u'Rutherford County, North Carolina': 0.23377585,\n",
       "   u'Rutherford County, Tennessee': 0.28741845,\n",
       "   u'Rutherford GO Station': 0.24944615,\n",
       "   u'Rutherford, California': 0.35625112,\n",
       "   u'Rutherford, Edmonton': 0,\n",
       "   u'Rutherford, New Jersey': 0.25327325,\n",
       "   u'Rutherford, New South Wales': 0.16758162,\n",
       "   u'Rutherford, Pennsylvania': 0,\n",
       "   u'Rutherford, Tennessee': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Deacon also appeared on '' [The Addams Family] '' in April 1965 , and puts Cousin Itt through\": {u'gold': u'The Addams Family',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.27634072,\n",
       "   u'Addams Family': 0.79966414,\n",
       "   u'Addams Family (pinball)': 0.26838148,\n",
       "   u\"Children's film\": 0,\n",
       "   u'Family': 0.25356933,\n",
       "   u'Family (1976 TV series)': 0,\n",
       "   u'Family (band)': 0.62389475,\n",
       "   u'Family (biology)': 0.29164195,\n",
       "   u'Family Channel': 0.49958351,\n",
       "   u'The Addams Family': 0.79966414,\n",
       "   u'The Addams Family (1964 TV series)': 0.28186393,\n",
       "   u'The Addams Family (1973 animated series)': 0,\n",
       "   u'The Addams Family (1992 animated series)': 0,\n",
       "   u'The Addams Family (film)': 0.67108941,\n",
       "   u'The Addams Family (musical)': 0.76460654,\n",
       "   u'The Addams Family (pinball)': 0.26838148,\n",
       "   u'The Addams Family (video game)': 0.44378841,\n",
       "   u'The Addams Family Theme': 0.52871501,\n",
       "   u'XXNILXX': 0,\n",
       "   u'family (biology)': 0.29164195}},\n",
       " u\"Game '' , and played '' Horace Vandergelder '' opposite [Phyllis Diller] 's '' Dolly Gallagher Levi '' in a touring production\": {u'gold': u'Phyllis Diller',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.22339648,\n",
       "   u'Diller': 0.6126219,\n",
       "   u'Diller Glacier': 0,\n",
       "   u'Diller, Nebraska': 0.25839138,\n",
       "   u'LeGrande A. Diller': 0,\n",
       "   u'Phyllis Diller': 0.85498059,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"He had a brief role in [Alfred Hitchcock] 's film '' The Birds '' and a larger role\": {u'gold': u'Alfred Hitchcock',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.20041147,\n",
       "   u'Alfred Hitchcock': 0.84129226,\n",
       "   u'Alfred Hitchcock (book)': 0,\n",
       "   u'Alfred Hitchcock (police officer)': 0,\n",
       "   u'Cameron Hitchcock': 0.2123026,\n",
       "   u'E. Hitchcock': 0.74574947,\n",
       "   u'Edward Hitchcock': 0.74574947,\n",
       "   u'Hitchcock': 0.84129226,\n",
       "   u'Hitchcock (automobile)': 0,\n",
       "   u'Hitchcock (film)': 0,\n",
       "   u'Hitchcock County, Nebraska': 0.19265226,\n",
       "   u'Hitchcock, Oklahoma': 0,\n",
       "   u'Hitchcock, South Dakota': 0.14590648,\n",
       "   u'Hitchcock, Texas': 0.15503487,\n",
       "   u'Ken Hitchcock': 0.44189554,\n",
       "   u'Sterling Hitchcock': 0.40794837,\n",
       "   u'The Birds (film)': 0.74568379,\n",
       "   u'Tom Hitchcock': 0,\n",
       "   u'Tommy Hitchcock (racing driver)': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"His best-known roles are Mel Cooley on '' [The Dick Van Dyke Show] '' , Fred Rutherford on '' Leave It to Beaver\": {u'gold': u'The Dick Van Dyke Show',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.46474677,\n",
       "   u'Agricultural show': 0.26981091,\n",
       "   u'Days of Our Lives': 0.78586513,\n",
       "   u'Days of our Lives': 0.78586513,\n",
       "   u'Dick Van Dyke Show': 0.81170464,\n",
       "   u'Dyke Show': 0,\n",
       "   u'Eric Show': 0.70435196,\n",
       "   u'General Hospital': 0.79486471,\n",
       "   u'Show': 0.70647085,\n",
       "   u'Show (The Cure album)': 0,\n",
       "   u'Show (The Jesus Lizard album)': 0,\n",
       "   u'Show (film)': 0,\n",
       "   u'Show (magazine)': 0,\n",
       "   u'Showbiz (producer)': 0.69024992,\n",
       "   u'Showbiz and A.G.': 0.69024992,\n",
       "   u'The Dick Van Dyke Show': 0.81170464,\n",
       "   u'The Dick Van Dyke Show ': 0,\n",
       "   u'The Van Dyke Show': 0.6556747,\n",
       "   u'Van Dyke Show': 0.6556747,\n",
       "   u'XXNILXX': 0}},\n",
       " u'In real life , he was a gourmet [chef] .': {u'gold': u'Chef',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.20062894,\n",
       "   u'Celebrity chef': 0.71184027,\n",
       "   u'Chef': 0.76216328,\n",
       "   u'Chef (2014 film)': 0.226935,\n",
       "   u'Chef (South Park character)': 0.19323406,\n",
       "   u'Chef (South Park)': 0.19323406,\n",
       "   u'Chef (company)': 0,\n",
       "   u'Chef (film)': 0.226935,\n",
       "   u'Chef (programming language)': 0.35155916,\n",
       "   u'Chef (software)': 0.2295706,\n",
       "   u\"Chef's uniform\": 0.2183544,\n",
       "   u'Cook (profession)': 0.50711137,\n",
       "   u'Isaac Hayes': 0.828578,\n",
       "   u'Japanese cuisine': 0.22854877,\n",
       "   u'Magician': 0.80157435,\n",
       "   u'Michael Smith (chef)': 0,\n",
       "   u'Swedish Chef': 0.75719023,\n",
       "   u'XXNILXX': 0,\n",
       "   u'celebrity chef': 0.71184027,\n",
       "   u'chef': 0.76216328}},\n",
       " u\"In the 1956 motion picture '' [Carousel] '' , adapted from the classic Rodgers and Hammerstein stage\": {u'gold': u'Carousel (film)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.24266344,\n",
       "   u'Carousel': 0.52266419,\n",
       "   u'Carousel (1923 film)': 0,\n",
       "   u'Carousel (1956 film)': 0.25489241,\n",
       "   u'Carousel (Blink-182 song)': 0,\n",
       "   u'Carousel (Leila K album)': 0,\n",
       "   u'Carousel (TV channel)': 0.28873307,\n",
       "   u'Carousel (Vanessa Carlton song)': 0.37353444,\n",
       "   u'Carousel (advert)': 0,\n",
       "   u'Carousel (advertisement)': 0,\n",
       "   u'Carousel (album)': 0,\n",
       "   u'Carousel (ballet)': 0.41007155,\n",
       "   u'Carousel (film)': 0.25489241,\n",
       "   u'Carousel (musical)': 0.76599771,\n",
       "   u'Carousel slide projector': 0.27359825,\n",
       "   u'Cheshire Cat (Blink-182 album)': 0.26036787,\n",
       "   u'Ford Carousel': 0,\n",
       "   u'Rabbits on the Run': 0.37353444,\n",
       "   u'Westfield Carousel': 0.23108113,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Invasion of the Body Snatchers '' , having portrayed a [physician] in the '' book-end '' sequences added to the beginning\": {u'gold': u'Physician',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.2230123,\n",
       "   u'Doctor of Medicine': 0.41356993,\n",
       "   u'General practitioner': 0.43982166,\n",
       "   u'Internal medicine': 0.25438839,\n",
       "   u'Islamic medicine': 0.41623008,\n",
       "   u'John Snow (physician)': 0.78351939,\n",
       "   u'Medicine': 0.5228591,\n",
       "   u'Medicine in medieval Islam': 0.41623008,\n",
       "   u'Medicine in the medieval Islamic world': 0.41623008,\n",
       "   u'Physician': 0.740807,\n",
       "   u'Physician (horse)': 0,\n",
       "   u'Physics': 0.32432213,\n",
       "   u'Physiology': 0.62672019,\n",
       "   u\"Ship's doctor\": 0,\n",
       "   u'Traditional Chinese medicine': 0.59677309,\n",
       "   u'XXNILXX': 0,\n",
       "   u'medical doctor': 0.740807,\n",
       "   u'medicine': 0.5228591,\n",
       "   u'physician': 0.740807,\n",
       "   u'physiology': 0.62672019}},\n",
       " u\"Levi '' in a touring production of the musical '' [Hello , Dolly !] '' .\": {u'gold': u'Hello, Dolly! (musical)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.28391945,\n",
       "   u'Hello': 0.56360638,\n",
       "   u'Hello (2008 film)': 0,\n",
       "   u'Hello (Ice Cube song)': 0,\n",
       "   u'Hello (Lionel Richie song)': 0,\n",
       "   u'Hello (Martin Solveig song)': 0,\n",
       "   u'Hello (band)': 0.64221311,\n",
       "   u'Hello (magazine)': 0.56360638,\n",
       "   u'Hello ,': 0,\n",
       "   u'Hello , Dolly': 0,\n",
       "   u'Hello , Dolly !': 0,\n",
       "   u'Hello Dolly': 0.79969788,\n",
       "   u'Hello Dolly (film)': 0,\n",
       "   u'Hello Dolly (movie)': 0,\n",
       "   u'Hello Dolly (song)': 0,\n",
       "   u'Hello, Dolly!': 0.79969788,\n",
       "   u'Hello, Dolly! (film)': 0,\n",
       "   u'Hello, Dolly! (musical)': 0.42128509,\n",
       "   u'Hello, Dolly! (song)': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , 1921 August 8 , [1984] -RRB- , born in Philadelphia , was an American television': {u'gold': u'1984',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.17541993,\n",
       "   u'1984': 0.82861155,\n",
       "   u'1984 Grand Prix motorcycle racing season': 0,\n",
       "   u'1984 NBA draft': 0.21940646,\n",
       "   u'1984 NCAA Division I-A football season': 0.1771746,\n",
       "   u'1984 NFL season': 0.22086561,\n",
       "   u'1984 NHL Entry Draft': 0.23028883,\n",
       "   u'1984 Summer Olympics': 0.5080632,\n",
       "   u'1984 Winter Olympics': 0.43313229,\n",
       "   u'1984 in film': 0.31365502,\n",
       "   u'1984 in literature': 0.16618425,\n",
       "   u'1984 in music': 0.19146386,\n",
       "   u'1984 in television': 0.22054791,\n",
       "   u'1984 in video gaming': 0.16469094,\n",
       "   u'Australian federal election, 1984': 0.15758318,\n",
       "   u'Nineteen Eighty-Four': 0.50355923,\n",
       "   u'UEFA Euro 1984': 0.29010132,\n",
       "   u'United States House of Representatives elections, 1984': 0.25985253,\n",
       "   u'United States presidential election, 1984': 0.31446368,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , 1921 [August 8] , 1984 -RRB- , born in Philadelphia , was an': {u'gold': u'August 8',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.17866629,\n",
       "   u'/August': 0.28515142,\n",
       "   u'August': 0.28515142,\n",
       "   u'August (1996 film)': 0,\n",
       "   u'August (2008 film)': 0,\n",
       "   u'August (Eric Clapton album)': 0,\n",
       "   u'August (Fringe)': 0.17418551,\n",
       "   u'August (company)': 0.20927,\n",
       "   u'August 1966': 0,\n",
       "   u'August 1971': 0,\n",
       "   u'August 1972': 0,\n",
       "   u'August 1973': 0,\n",
       "   u'August 1975': 0,\n",
       "   u'August 2007': 0.18860933,\n",
       "   u'August 8': 0.81647551,\n",
       "   u'August 8 (Eastern Orthodox liturgics)': 0,\n",
       "   u'August, California': 0,\n",
       "   u'CMLL Super Viernes (August 2014)': 0,\n",
       "   u'Papal conclave, August 1978': 0.33134207,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , [1921] August 8 , 1984 -RRB- , born in Philadelphia ,': {u'gold': u'1921',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.19462401,\n",
       "   u'144th New York State Legislature': 0.25641561,\n",
       "   u'1921': 0.83976746,\n",
       "   u'1921 APFA season': 0.17074525,\n",
       "   u'1921 Green Bay Packers season': 0,\n",
       "   u'1921 Indianapolis 500': 0.19817176,\n",
       "   u'1921 NFL season': 0.17074525,\n",
       "   u'1921 VFL season': 0.17387801,\n",
       "   u'1921 World Series': 0.18619794,\n",
       "   u'1921 college football season': 0.15940911,\n",
       "   u'1921 in Canada': 0.20676684,\n",
       "   u'1921 in aviation': 0.19101077,\n",
       "   u'1921 in film': 0.23451006,\n",
       "   u'1921 in literature': 0,\n",
       "   u'1921 in poetry': 0,\n",
       "   u'1921 in the United States': 0.25799787,\n",
       "   u'Canadian federal election, 1921': 0,\n",
       "   u'Irish elections, 1921': 0.16775158,\n",
       "   u'Norwegian parliamentary election, 1921': 0.21334133,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- [May 14] , 1921 August 8 , 1984 -RRB- , born in': {u'gold': u'May 14',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.16297933,\n",
       "   u'/May': 0.37543663,\n",
       "   u'Ben May': 0.28319141,\n",
       "   u'Brian May': 0.56964684,\n",
       "   u'CMLL Super Viernes (May 2014)': 0,\n",
       "   u'Darrell May': 0.25999069,\n",
       "   u'David May (footballer)': 0.2139169,\n",
       "   u'Jonny May': 0.14551425,\n",
       "   u'Kathy May': 0,\n",
       "   u'List of Pok\\xe9mon anime characters': 0.24577692,\n",
       "   u'May': 0.37543663,\n",
       "   u'May (Pok\\xe9mon)': 0.24577692,\n",
       "   u'May (film)': 0.29310337,\n",
       "   u'May 14': 0.83753979,\n",
       "   u'May 14 (Eastern Orthodox liturgics)': 0,\n",
       "   u'May, Oklahoma': 0,\n",
       "   u'May, Texas': 0,\n",
       "   u'Sean May': 0.23419508,\n",
       "   u'Stevie May': 0.18301404,\n",
       "   u'XXNILXX': 0}},\n",
       " u'The bald and usually bespectacled [character actor] often portrayed imperious authority figures .': {u'gold': u'Character actor',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.25979298,\n",
       "   u'Academy Award for Best Actor': 0.72782016,\n",
       "   u'Acting': 0.80366552,\n",
       "   u'Actor': 0.836272,\n",
       "   u'Actor (UML)': 0,\n",
       "   u'Actor (album)': 0,\n",
       "   u'Actor (mythology)': 0,\n",
       "   u'Character actor': 0.77258468,\n",
       "   u'Film actor': 0.836272,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Miniseries or Television Film': 0.48749438,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Motion Picture Drama': 0.55223143,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Motion Picture Musical or Comedy': 0.59809339,\n",
       "   u'Pornographic film actor': 0.72518677,\n",
       "   u'XXNILXX': 0,\n",
       "   u'acting': 0.80366552,\n",
       "   u'actor': 0.836272,\n",
       "   u'character actor': 0.77258468,\n",
       "   u'film actor': 0.836272,\n",
       "   u'pornographic actor': 0.72518677,\n",
       "   u'television actor': 0.836272}},\n",
       " u\"had a brief role in Alfred Hitchcock 's film '' [The Birds] '' and a larger role in the original '' Invasion\": {u'gold': u'The Birds (film)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.18334687,\n",
       "   u'Bird': 0.66192824,\n",
       "   u'Bird (disambiguation)': 0.66192824,\n",
       "   u'Bird (film)': 0,\n",
       "   u'Birds': 0.66192824,\n",
       "   u'Birds (Anouk song)': 0,\n",
       "   u'Birds (Bic Runga album)': 0,\n",
       "   u'Birds (Kate Nash song)': 0,\n",
       "   u'Birds, Illinois': 0.22828373,\n",
       "   u'Greg Bird': 0.39773971,\n",
       "   u'Larry Bird': 0.64157575,\n",
       "   u'Ryan Bird': 0,\n",
       "   u'Sue Bird': 0.31006935,\n",
       "   u'The Birds': 0.66192824,\n",
       "   u'The Birds (Respighi)': 0.28246608,\n",
       "   u'The Birds (band)': 0.22557378,\n",
       "   u'The Birds (film)': 0.73049456,\n",
       "   u'The Birds (play)': 0,\n",
       "   u'The Birds (story)': 0.29870057,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"he had a bit role as the policeman who admonishes [Shirley Jones] -LRB- '' Julie '' -RRB- and John Dehner -LRB- ''\": {u'gold': u'Shirley Jones',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.27200007,\n",
       "   u'Asjha Jones': 0.29579675,\n",
       "   u'Jermaine Jones': 0.44057083,\n",
       "   u'Jone': 0.40198532,\n",
       "   u'Jone (band)': 0,\n",
       "   u'Jone (opera)': 0,\n",
       "   u'Jone Pedro': 0,\n",
       "   u'Jone da Silva Pinto': 0,\n",
       "   u'Jones': 0.7006793,\n",
       "   u'Jones County, Texas': 0.29224551,\n",
       "   u'Kenwyne Jones': 0.30803466,\n",
       "   u'Nathan Jones (Australian rules footballer)': 0,\n",
       "   u'Shirley Jones': 0.8478266,\n",
       "   u'Shirley Jones (R&amp;B singer)': 0,\n",
       "   u'Shirley Jones (horse)': 0,\n",
       "   u'Shirley Jones (politician)': 0,\n",
       "   u'Stacey Jones': 0.40126258,\n",
       "   u'The Jones Girls': 0.78231996,\n",
       "   u'Todd Jones': 0.46565202,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"revealed to have been gay , and his interview with [Boze Hadleigh] was published in Hadleigh 's '' Hollywood Gays '' ,\": {u'gold': u'Boze Hadleigh',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.23193312,\n",
       "   u'Boze Hadleigh': 0.75621951,\n",
       "   u'Hadleigh': 0.31541237,\n",
       "   u'Hadleigh (TV series)': 0,\n",
       "   u'Hadleigh (disambiguation)': 0.31541237,\n",
       "   u'Hadleigh Airfield': 0,\n",
       "   u'Hadleigh Castle': 0.21795014,\n",
       "   u'Hadleigh railway station': 0,\n",
       "   u'Hadleigh, Essex': 0,\n",
       "   u'Hadleigh, Suffolk': 0.28984982,\n",
       "   u'RAF Hadleigh': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'series of cookbook s and hosted a television series on [microwave] cooking .': {u'gold': u'Microwave oven',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.19295797,\n",
       "   u'British Telecom microwave network': 0.29098669,\n",
       "   u'Cavalier Computer': 0,\n",
       "   u'EM spectrum': 0.4718045,\n",
       "   u'Electromagnetic spectrum': 0.4718045,\n",
       "   u'Microwave': 0.70932579,\n",
       "   u'Microwave (game)': 0,\n",
       "   u'Microwave chemistry': 0.35720611,\n",
       "   u'Microwave oven': 0.48438805,\n",
       "   u'Microwave radio relay': 0.42403156,\n",
       "   u'Microwave radiometer': 0.58210367,\n",
       "   u'Microwave spectroscopy': 0.56896859,\n",
       "   u'Microwave transmission': 0.42403156,\n",
       "   u'Microwaves': 0.70932579,\n",
       "   u'Rotational spectroscopy': 0.56896859,\n",
       "   u'Waldorf Microwave': 0,\n",
       "   u'XXNILXX': 0,\n",
       "   u'microwave': 0.70932579,\n",
       "   u'microwave oven': 0.48438805,\n",
       "   u'microwave transmission': 0.42403156}},\n",
       " u'the 1970s and 1980s , he wrote a series of [cookbook] s and hosted a television series on microwave cooking .': {u'gold': u'cookbook',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.26561064,\n",
       "   u'Cookbook': 0.76608467,\n",
       "   u\"Nanny Ogg's Cookbook\": 0.64256698,\n",
       "   u'XXNILXX': 0,\n",
       "   u'cookbook': 0.76608467}},\n",
       " u\"who admonishes Shirley Jones -LRB- '' Julie '' -RRB- and [John Dehner] -LRB- '' Mr.\": {u'gold': u'John Dehner',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.24671459,\n",
       "   u'Dehner': 0.40936095,\n",
       "   u'John Dehner': 0.79180747,\n",
       "   u'XXNILXX': 0}}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "    queries_exp.func_inputs,\n",
    "    [queries_exp.loss_vec, queries_exp.source_out, queries_exp.target_out] + T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params),\n",
    "    updates=queries_exp.updates,\n",
    "    on_unused_input='ignore',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = gg_func(\n",
    "    queries_exp.current_documents,\n",
    "    queries_exp.current_surface_link, queries_exp.current_surface_context, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_matches_surface, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24814822, 0.24238527, 0.23680143)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0].max(), res1[0].mean(), res1[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  76.69731903,  172.44116211,  139.30476379, -182.91343689,\n",
       "        172.44116211,  172.44116211,  -13.79522324,   97.6740036 ,\n",
       "         11.84405231,   36.98944092], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res1[1][0], res1[2][0:10,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24216416,  0.24294616,  0.24274334,  0.23833458,  0.24294616,\n",
       "        0.24294616,  0.24024701,  0.24193516,  0.24127102,  0.24146625], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.7740622e-06"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[3:][0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[surface_cxt_conv1.W,\n",
       " surface_cxt_conv1.b,\n",
       " surface_conv1.W,\n",
       " surface_conv1.b,\n",
       " surface_dens1.W,\n",
       " surface_dens1.b,\n",
       " source_dens1.W,\n",
       " source_dens1.b,\n",
       " source_dens12.W,\n",
       " source_dens12.b,\n",
       " source_dens2.W,\n",
       " source_dens2.b,\n",
       " document_conv1.W,\n",
       " document_conv1.b,\n",
       " doucment_dens1.W,\n",
       " doucment_dens1.b,\n",
       " document_dens2.W,\n",
       " document_dens2.b]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24253285"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.04426765,  -8.85146523,  -6.5927825 ,  -5.64352655,\n",
       "        -6.2810235 , -10.34654236,  -6.09899616,   1.8380394 ,\n",
       "        -3.23892975,  -6.04222679,  -8.91859531,   1.09592962,\n",
       "        -5.27822304, -12.33982086,  -5.90433264], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res1[2][0,:],res1[1][0:15,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.4103508"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[2][0,:].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.82824624"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[1][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(v, np.isnan(v.get_value(borrow=True)).all()) for v in queries_exp.all_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_score],\n",
    "            T.grad(queries_exp.loss_vec.mean(), lasagne.layers.get_all_params(queries_exp.target_dens2)),\n",
    "#           T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params),\n",
    "    #             [queries_exp.target_out, queries_exp.source_aligned_l, \n",
    "#              T.dot(queries_exp.target_out, queries_exp.source_aligned_l.T).diagonal(),\n",
    "#              queries_exp.target_out.norm(2, axis=1) * queries_exp.source_aligned_l.norm(2, axis=1),\n",
    "#              T.batched_dot(queries_exp.target_out, queries_exp.source_aligned_l),\n",
    "#              lasagne.layers.get_output(queries_exp.target_dens2),\n",
    "#              queries_exp.target_out.norm(2, axis=1),\n",
    "#              #T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params)\n",
    "#             ],\n",
    "        #[queries_exp.res_l, queries_exp.loss_vec.sum(), queries_exp.loss_vec],\n",
    "    on_unused_input='ignore',\n",
    "    mode='DebugMode'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_answer],\n",
    "            #[self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            [queries_exp.res_cap],\n",
    "        mode='DebugMode',\n",
    "        on_unused_input='ignore',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gg_grad_res = gg_func(\n",
    "        queries_exp.current_documents,\n",
    "        queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "        queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    "    )\n",
    "except Exception as e:\n",
    "    eeee = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueError('Bad input argument to theano function with name \"<ipython-input-19-c6e818cc55a5>:8\"  at index 4(0-based)',\n",
       "           'setting an array element with a sequence.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.isnan(v).any() for v in gg_grad_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res = gg_func(\n",
    "    queries_exp.current_documents,\n",
    "    queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[5]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].shape, gg_res[1].shape, gg_res[2].shape, gg_res[3].shape, gg_res[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.inner(gg_res[0], gg_res[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0] * gg_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.dot(gg_res[0], gg_res[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa.diagonal().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.queried_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_link_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(5):\n",
    "    exp_results.append((i, queries_exp.compute_batch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss / queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time queries_exp.run_batch(queries_exp.test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
