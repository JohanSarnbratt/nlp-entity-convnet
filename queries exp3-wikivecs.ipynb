{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "import json\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki1.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993444276348966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8917/9915."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links5-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs)\n",
    "        for sur, v in q.iteritems():\n",
    "            wordvec.tokenize(sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wordvec.tokenize(link)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                wordvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content):\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447343, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.get_numpy_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "    \n",
    "    batch_size = 20000\n",
    "    num_training_items = 200000\n",
    "    \n",
    "    def __init__(self): #, wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "        #self.wordvecs = wordvec\n",
    "        #self.queries = queries\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.num_words_to_use_conv = 5\n",
    "        #self.redirects = redirects\n",
    "        #self.page_content = {}\n",
    "        #self.wikipedia_dump_fname = wikipedia_dump_fname\n",
    "        \n",
    "        #self._process_queries()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "#     def _process_queries(self):\n",
    "#         queried_pages = set()\n",
    "#         for docs, q in self.queries.iteritems():\n",
    "#             self.wordvecs.tokenize(docs)\n",
    "#             for sur, v in q.iteritems():\n",
    "#                 self.wordvecs.tokenize(sur)\n",
    "#                 for link in v['vals'].keys():\n",
    "#                     self.wordvecs.tokenize(link)\n",
    "#                     tt = WikiRegexes.convertToTitle(link)\n",
    "#                     #self.wordvecs.tokenize(tt)\n",
    "#                     queried_pages.add(tt)\n",
    "\n",
    "#         added_pages = set()\n",
    "#         for title in queried_pages:\n",
    "#             if title in self.redirects:\n",
    "#                 #self.wordvecs.tokenize(self.redirects[title])\n",
    "#                 added_pages.add(self.redirects[title])\n",
    "#         queried_pages |= added_pages\n",
    "        \n",
    "#         self.queried_pages = queried_pages\n",
    "                \n",
    "#         class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "            \n",
    "#             def readPage(ss, title, content):\n",
    "#                 tt = ss.convertToTitle(title)\n",
    "#                 if tt in queried_pages:\n",
    "#                     cnt = ss._wikiToText(content)\n",
    "#                     self.page_content[tt] = self.wordvecs.tokenize(cnt)\n",
    "        \n",
    "#         GetWikipediaWords(self.wikipedia_dump_fname).read()\n",
    "               \n",
    "        \n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_sent')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface')\n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_document_id = T.ivector('x_sent_id')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "        self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length), \n",
    "            input_var=self.x_document_input\n",
    "        )\n",
    "    \n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=500,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='max',\n",
    "        )\n",
    "\n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=400,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.document_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        document_output_length = 350\n",
    "        \n",
    "        self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.document_drop1,\n",
    "            num_units=document_output_length,\n",
    "            name='document_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens2)\n",
    "                \n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length), \n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "        \n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=350,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.surface_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.surface_drop1,\n",
    "            name='surface_dens2',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "        \n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens2]\n",
    "        )\n",
    "        \n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=450,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=350,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=self.wordvecs.vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "        \n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "        \n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            self.target_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "#         self.target_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.target_embedding_l,\n",
    "#             name='target_conv1',\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             num_filters=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "        self.target_out = self.embedding_W[self.x_target_input]\n",
    "        \n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "        \n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "        \n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "    \n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) * \n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "    \n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) + \n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "        \n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "        \n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "        \n",
    "        self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "        \n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_answer],\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates\n",
    "        )\n",
    "        \n",
    "        self.test_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_answer],\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "        )\n",
    "        \n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_text = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.learning_targets = []\n",
    "        \n",
    "    def compute_batch(self, isTraining=True):\n",
    "        if isTraining:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        \n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] is not isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_text)\n",
    "                self.current_surface_text.append(self.wordvecs.tokenize(surtxt))\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_gold_loc = None\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    cnt = self.wordvecs.get_location(WikiRegexes.convertToTitle(target))\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_learings.append((targets, target))\n",
    "                if target_gold_loc is not None:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    self.current_target_goal += [(len(self.current_target_goal) + target_gold_loc)] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                \n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "#                 if self.total_links > self.num_training_items:\n",
    "#                     return self.total_loss / self.total_links\n",
    "        \n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "            \n",
    "        return self.total_loss / self.total_links\n",
    "        \n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            self.current_documents,\n",
    "            self.current_surface_text, self.current_link_id,\n",
    "            self.current_target_input, self.current_target_id, self.current_target_goal\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = res_vec[i]\n",
    "        self.reset_accums()\n",
    "        \n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "        \n",
    "\n",
    "        \n",
    "queries_exp = EntityVectorLinkExp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256091, 256091)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_target_goal), len(queries_exp.current_target_inputa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.2808422057445896e-05),\n",
       " (1, 1.178938553597662e-05),\n",
       " (2, 1.1554431539786954e-05),\n",
       " (3, 1.1366898304850509e-05),\n",
       " (4, 1.1251370763512869e-05),\n",
       " (0, 1.1193861397317441e-05),\n",
       " (1, 1.1125029957606146e-05)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Arab': {u'gold': u'Arab people',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.50193685,\n",
       "   u'Arab': 0.49251989,\n",
       "   u'Arab American': 0.48268604,\n",
       "   u'Arab Canadian': 0.47710776,\n",
       "   u'Arab Canadians': 0.47459191,\n",
       "   u'Arab League': 0.4570426,\n",
       "   u'Arab citizens of Israel': 0.47061372,\n",
       "   u'Arab cuisine': 0.4744601,\n",
       "   u'Arab culture': 0.49504513,\n",
       "   u'Arab people': 0.47710776,\n",
       "   u'Arab world': 0.46541506,\n",
       "   u'Arab, Alabama': 0.48090824,\n",
       "   u'Arabian horse': 0.50524002,\n",
       "   u'Arabic language': 0.47843751,\n",
       "   u'Arabic music': 0.47710776,\n",
       "   u'Arabs': 0.46808359,\n",
       "   u'British Arabs': 0.51197815,\n",
       "   u'History of Arabs in Afghanistan': 0.48268604,\n",
       "   u'Palestinian people': 0.50918639,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Buddhism': {u'gold': u'Buddhism',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.47233197,\n",
       "   u'Buddhism': 0.47615919,\n",
       "   u'Buddhism in Australia': 0.5114935,\n",
       "   u'Buddhism in China': 0.5114935,\n",
       "   u'Buddhism in Indonesia': 0.48624432,\n",
       "   u'Buddhism in Japan': 0.51741838,\n",
       "   u'Buddhism in Norway': 0.5114935,\n",
       "   u'Buddhism in Sri Lanka': 0.49804127,\n",
       "   u'Buddhism in the Philippines': 0.5114935,\n",
       "   u'Buddhist philosophy': 0.5114935,\n",
       "   u'Buddhist pilgrimage': 0.49804127,\n",
       "   u'Chinese Buddhism': 0.47615919,\n",
       "   u'Criticism of Buddhism': 0.51741838,\n",
       "   u'God in Buddhism': 0.5114935,\n",
       "   u'History of Buddhism in India': 0.47477019,\n",
       "   u'Korean Buddhism': 0.47233197,\n",
       "   u'Outline of Buddhism': 0.5114935,\n",
       "   u'Tibetan Buddhism': 0.5114935,\n",
       "   u'Women in Buddhism': 0.5114935,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Chinese': {u'gold': u'Overseas Chinese',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.475833,\n",
       "   u'China': 0.47357756,\n",
       "   u'Chinese': 0.50523907,\n",
       "   u'Chinese American': 0.475833,\n",
       "   u'Chinese Canadian': 0.52832031,\n",
       "   u'Chinese character': 0.51555514,\n",
       "   u'Chinese characters': 0.4877778,\n",
       "   u'Chinese cuisine': 0.50611544,\n",
       "   u'Chinese language': 0.46597627,\n",
       "   u'Chinese languages': 0.44250762,\n",
       "   u'Chinese people': 0.48716453,\n",
       "   u'Han Chinese': 0.48825759,\n",
       "   u'History of China': 0.49546608,\n",
       "   u'Overseas Chinese': 0.52962059,\n",
       "   u'PR China': 0.46573016,\n",
       "   u\"People's Republic of China\": 0.51851922,\n",
       "   u'Simplified Chinese': 0.4877778,\n",
       "   u'Simplified Chinese characters': 0.49507073,\n",
       "   u'Varieties of Chinese': 0.4840523,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Christian': {u'gold': u'Christian',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.47786129,\n",
       "   u'Arab Christians': 0.51272047,\n",
       "   u'Christian': 0.48724881,\n",
       "   u'Christian (wrestler)': 0.51272047,\n",
       "   u'Christian Corr\\xeaa Dionisio': 0,\n",
       "   u'Christian County, Illinois': 0.51272047,\n",
       "   u'Christian County, Kentucky': 0.47624457,\n",
       "   u'Christian County, Missouri': 0.46770355,\n",
       "   u'Christian Fern\\xe1ndez Salas': 0,\n",
       "   u'Christian denomination': 0.51272047,\n",
       "   u'Christian metal': 0.51272047,\n",
       "   u'Christian music': 0.46770355,\n",
       "   u'Christian radio': 0.48724881,\n",
       "   u'Christian rock': 0.47615644,\n",
       "   u'Christian school': 0.51272047,\n",
       "   u'Christianity': 0.47463188,\n",
       "   u'Christianity in Lebanon': 0.51272047,\n",
       "   u'Christianity in Sri Lanka': 0.51272047,\n",
       "   u'Contemporary Christian music': 0.47786129,\n",
       "   u'XXNILXX': 0}},\n",
       " u'East Africa': {u'gold': u'East Africa',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.51784515,\n",
       "   u'Africa': 0.48333094,\n",
       "   u'Africa (Karl Wolf song)': 0,\n",
       "   u'Africa (Petrarch)': 0.47469929,\n",
       "   u'Africa (Roman province)': 0.51014972,\n",
       "   u'Africa (Toto song)': 0.50611544,\n",
       "   u'Africa Province': 0.52311325,\n",
       "   u'Confederation of African Athletics': 0.47419649,\n",
       "   u'Confederation of African Football': 0.50444281,\n",
       "   u'East Africa': 0.50406784,\n",
       "   u'East Africa Protectorate': 0.53512019,\n",
       "   u'East Africa cricket team': 0.52146214,\n",
       "   u'East Africa rugby union team': 0.48957312,\n",
       "   u'East African Campaign (World War I)': 0.47832727,\n",
       "   u'East African Campaign (World War II)': 0.51212204,\n",
       "   u'East African cricket team': 0.47630036,\n",
       "   u'Eastern Africa': 0.50489956,\n",
       "   u'German East Africa': 0.46597627,\n",
       "   u'United Nations geoscheme for Africa': 0.52606845,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Hindu': {u'gold': u'Hindu',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.46815813,\n",
       "   u'Hindu': 0.52100664,\n",
       "   u'Hindu architecture': 0.52027243,\n",
       "   u'Hindu calendar': 0.52027243,\n",
       "   u'Hindu cosmology': 0.47357953,\n",
       "   u'Hindu music': 0.4877778,\n",
       "   u'Hindu mythology': 0.53752297,\n",
       "   u'Hindu philosophy': 0.52027243,\n",
       "   u'Hindu temple architecture': 0.44250762,\n",
       "   u'Hinduism': 0.54611236,\n",
       "   u'Hinduism in Bangladesh': 0.54611236,\n",
       "   u'Hinduism in India': 0.49350813,\n",
       "   u'Hinduism in Indonesia': 0.49050841,\n",
       "   u'Hinduism in Nepal': 0.52112323,\n",
       "   u'Hinduism in Pakistan': 0.50212437,\n",
       "   u'Hinduism in Southeast Asia': 0.53597987,\n",
       "   u'Hinduism in Sri Lanka': 0.53752297,\n",
       "   u'Hinduism in the Philippines': 0.51214111,\n",
       "   u'The Hindu': 0.52027243,\n",
       "   u'XXNILXX': 0}},\n",
       " u'India': {u'gold': u'India',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.47611174,\n",
       "   u'A1 Team India': 0.50323439,\n",
       "   u'All India Football Federation': 0.50991136,\n",
       "   u'British India': 0.50323439,\n",
       "   u'British Raj': 0.50323439,\n",
       "   u'Cinema of India': 0.50323439,\n",
       "   u'Company Rule in India': 0.50323439,\n",
       "   u'Company rule in India': 0.50506628,\n",
       "   u'History of India': 0.48641753,\n",
       "   u'India': 0.47555926,\n",
       "   u'India cricket team': 0.48641753,\n",
       "   u\"India men's national field hockey team\": 0.4986473,\n",
       "   u'India national cricket team': 0.5231598,\n",
       "   u'India national football team': 0.47306412,\n",
       "   u\"India national women's cricket team\": 0.5351544,\n",
       "   u'Indian cricket team': 0.48068002,\n",
       "   u'Indian poetry': 0.50235784,\n",
       "   u'Indian subcontinent': 0.48692229,\n",
       "   u'Presidencies and provinces of British India': 0.47555926,\n",
       "   u'XXNILXX': 0}},\n",
       " u'La Digue': {u'gold': u'La Digue',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.45523196,\n",
       "   u'Digue': 0.48270562,\n",
       "   u'La Digue': 0.49387336,\n",
       "   u'La Digue (film)': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Muslim': {u'gold': u'Muslim',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.47301865,\n",
       "   u'Bosniaks': 0.46721563,\n",
       "   u'Islam': 0.50323439,\n",
       "   u'Islam in Bulgaria': 0.50036615,\n",
       "   u'Islam in China': 0.53073686,\n",
       "   u'Islam in Ethiopia': 0.43725166,\n",
       "   u'Islam in India': 0.50797206,\n",
       "   u'Islam in Indonesia': 0.50721145,\n",
       "   u'Islam in Lebanon': 0.48823348,\n",
       "   u'Islam in Sri Lanka': 0.50797206,\n",
       "   u'Islam in the Philippines': 0.49160707,\n",
       "   u'Moors': 0.51988852,\n",
       "   u'Muslim': 0.45921588,\n",
       "   u'Muslim ibn al-Hajjaj': 0.48120058,\n",
       "   u'Muslim world': 0.53298628,\n",
       "   u'Muslims': 0.46157774,\n",
       "   u'Petar Muslim': 0.51462579,\n",
       "   u'Religion in Lebanon': 0.48684031,\n",
       "   u'Sri Lankan Moors': 0.51978946,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Praslin': {u'gold': u'Praslin',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.52261418,\n",
       "   u'Canaries Quarter': 0,\n",
       "   u'Canaries, Saint Lucia': 0,\n",
       "   u'Praslin': 0.47409078,\n",
       "   u'Praslin (Aube)': 0,\n",
       "   u'Praslin Island Airport': 0.48605052,\n",
       "   u'Praslin National Park': 0,\n",
       "   u'Praslin Quarter': 0,\n",
       "   u'Praslin, Aube': 0,\n",
       "   u'Praslin, Port-Salut, Haiti': 0,\n",
       "   u'Praslin, Saint Lucia': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Roman Catholic': {u'gold': u'Catholic Church',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.48354879,\n",
       "   u'Catholic': 0.47378576,\n",
       "   u'Catholic Church': 0.51272047,\n",
       "   u'Catholic Church in the United States': 0.48908368,\n",
       "   u'Catholic High School for Boys (Little Rock, Arkansas)': 0.47530344,\n",
       "   u'Catholic school': 0.52481002,\n",
       "   u'Catholicism': 0.50862563,\n",
       "   u'Roman Catholic': 0.48829383,\n",
       "   u'Roman Catholic Church': 0.50448179,\n",
       "   u'Roman Catholic Church in Australia': 0.48782122,\n",
       "   u'Roman Catholicism': 0.49503997,\n",
       "   u'Roman Catholicism in Australia': 0.44564843,\n",
       "   u'Roman Catholicism in Ethiopia': 0.50862563,\n",
       "   u'Roman Catholicism in Romania': 0.518686,\n",
       "   u'Roman Catholicism in Sri Lanka': 0.48829383,\n",
       "   u'Roman Catholicism in the Philippines': 0.47340485,\n",
       "   u'Roman Catholicism in the United States': 0.47328991,\n",
       "   u'Separate school': 0.47624457,\n",
       "   u'The Catholic University of America': 0.50992608,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Seventh-day Adventist': {u'gold': u'Seventh-day Adventist Church',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.51741838,\n",
       "   u'Adventist Education': 0.51741838,\n",
       "   u'Criticism of the Seventh-day Adventist Church': 0.49398717,\n",
       "   u'History of the Seventh-day Adventist Church': 0.47618437,\n",
       "   u'Romanian Union Conference of Seventh-day Adventists': 0,\n",
       "   u'Sabbath in seventh-day churches': 0.51741838,\n",
       "   u'Seventh-day': 0.47618437,\n",
       "   u'Seventh-day Adventism': 0.48624432,\n",
       "   u'Seventh-day Adventist': 0.51741838,\n",
       "   u'Seventh-day Adventist Church': 0.51741838,\n",
       "   u'Seventh-day Adventist Church ': 0,\n",
       "   u'Seventh-day Adventist Church in Hawaii': 0,\n",
       "   u'Seventh-day Adventist Church in India': 0,\n",
       "   u'Seventh-day Adventist Church in Nigeria': 0,\n",
       "   u'Seventh-day Adventist Church, Jamaica': 0,\n",
       "   u'Seventh-day Adventist church': 0.47358122,\n",
       "   u'Seventh-day Adventist education': 0.49398717,\n",
       "   u'Seventh-day Sabbath': 0.51741838,\n",
       "   u'South Pacific Division of Seventh-day Adventists': 0.51741838,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Seychellois': {u'gold': u'Seychelles',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.49491283,\n",
       "   u'Seychelles': 0.521433,\n",
       "   u'Seychelloi': 0,\n",
       "   u'Seychellois': 0.49377197,\n",
       "   u'Seychellois (cat)': 0,\n",
       "   u'Seychellois Canadian': 0,\n",
       "   u'Seychellois Creole': 0.53136194,\n",
       "   u'Seychellois People': 0,\n",
       "   u'Seychellois people': 0,\n",
       "   u'Visa requirements for Seychellois citizens': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Seychellois Creole -LRB- '' Kreol\": {u'gold': u'Seychellois Creole',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.49563688,\n",
       "   u'Creole': 0.51066172,\n",
       "   u'Creole (markup)': 0,\n",
       "   u'Creole -LRB-': 0,\n",
       "   u\"Creole -LRB- ''\": 0,\n",
       "   u'Creole language': 0.53941309,\n",
       "   u'Creole peoples': 0.50742406,\n",
       "   u'Criollo people': 0.50091249,\n",
       "   u'Haitian Creole': 0.51212204,\n",
       "   u'Haitian Creole language': 0.50742406,\n",
       "   u'Louisiana Creole cuisine': 0.49219996,\n",
       "   u'Louisiana Creole people': 0.51212204,\n",
       "   u'Mauritian Creole': 0.47381771,\n",
       "   u'Mauritian creole': 0.50444281,\n",
       "   u'Seychellois Creole': 0.48541477,\n",
       "   u\"Seychellois Creole -LRB- '' Kreol\": 0,\n",
       "   u'Seychellois Creole -LRB- Kreol': 0,\n",
       "   u'Seychellois Creole people': 0,\n",
       "   u'Sierra Leone Creole people': 0.48734811,\n",
       "   u'XXNILXX': 0}},\n",
       " u'South Asian': {u'gold': u'South Asia',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.47319239,\n",
       "   u'Asia': 0.4672063,\n",
       "   u'Asian': 0.47787708,\n",
       "   u'Asian (U.S. Census)': 0.47424483,\n",
       "   u'Asian American': 0.47382778,\n",
       "   u'Asian South African': 0.47382778,\n",
       "   u'Asian people': 0.50252157,\n",
       "   u'British Asian': 0.50312847,\n",
       "   u'Hindi': 0.47407806,\n",
       "   u'Indo-Canadians': 0.47787708,\n",
       "   u'Race (United States Census)': 0.4672063,\n",
       "   u'Race and ethnicity in the United States Census': 0.4672063,\n",
       "   u'South Asia': 0.47948769,\n",
       "   u'South Asian': 0.50252157,\n",
       "   u'South Asian Canadian': 0.51153624,\n",
       "   u'South Asian cuisine': 0.50312847,\n",
       "   u'South Asian ethnic groups': 0.4672063,\n",
       "   u'South Asian literature': 0,\n",
       "   u'South Asians in Hong Kong': 0.49451646,\n",
       "   u'XXNILXX': 0}},\n",
       " u'adult education': {u'gold': u'Adult education',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.49644366,\n",
       "   u'Adult education': 0.46825176,\n",
       "   u'Education': 0.49986842,\n",
       "   u'Education (constituency)': 0,\n",
       "   u'Education in Ethiopia': 0.49727017,\n",
       "   u'Education in India': 0.5033586,\n",
       "   u'Education in Pakistan': 0.4824425,\n",
       "   u'Education in Puerto Rico': 0.47352332,\n",
       "   u'Education in Russia': 0.5062083,\n",
       "   u'Education in Scotland': 0.51921535,\n",
       "   u'Education in the United States': 0.482196,\n",
       "   u'Local Education Authority': 0.5127883,\n",
       "   u'Local education authority': 0.51545107,\n",
       "   u'Ministry of Education (Iran)': 0,\n",
       "   u'Ministry of National Education (Romania)': 0.49782458,\n",
       "   u'Outline of education': 0.51154298,\n",
       "   u'United States Department of Education': 0.50499684,\n",
       "   u'XXNILXX': 0,\n",
       "   u'adult education': 0.49604377,\n",
       "   u'education': 0.49591362}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Bad input argument to theano function with name \"<ipython-input-35-a7ca19795a86>:277\"  at index 4(0-based)', 'setting an array element with a sequence.')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-c78fdc0ec5ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_documents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_surface_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_link_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_target_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_target_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_target_goal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[0;32m    533\u001b[0m                             \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[1;31m# data has to be converted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                     \u001b[1;31m# Check that this conversion is lossless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mconverted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                     \u001b[1;31m# We use the `values_eq` static function from TensorType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[1;31m# to handle NaN values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/misc/safe_asarray.pyc\u001b[0m in \u001b[0;36m_asarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert into dtype object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Note that dtype comparison must be done by comparing their `num`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# attribute. One cannot assume that two identical data types are pointers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \"\"\"\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Bad input argument to theano function with name \"<ipython-input-35-a7ca19795a86>:277\"  at index 4(0-based)', 'setting an array element with a sequence.')"
     ]
    }
   ],
   "source": [
    "gg_res = queries_exp.test_func(\n",
    "    queries_exp.current_documents, \n",
    "    queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(v, np.isnan(v.get_value(borrow=True)).all()) for v in queries_exp.all_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_score],\n",
    "            T.grad(queries_exp.loss_vec.mean(), lasagne.layers.get_all_params(queries_exp.target_dens2)),\n",
    "#           T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params),\n",
    "    #             [queries_exp.target_out, queries_exp.source_aligned_l, \n",
    "#              T.dot(queries_exp.target_out, queries_exp.source_aligned_l.T).diagonal(),\n",
    "#              queries_exp.target_out.norm(2, axis=1) * queries_exp.source_aligned_l.norm(2, axis=1),\n",
    "#              T.batched_dot(queries_exp.target_out, queries_exp.source_aligned_l),\n",
    "#              lasagne.layers.get_output(queries_exp.target_dens2),\n",
    "#              queries_exp.target_out.norm(2, axis=1),\n",
    "#              #T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params)\n",
    "#             ],\n",
    "        #[queries_exp.res_l, queries_exp.loss_vec.sum(), queries_exp.loss_vec],\n",
    "    on_unused_input='ignore',\n",
    "    mode='DebugMode'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_answer],\n",
    "            #[self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            [queries_exp.res_cap],\n",
    "        mode='DebugMode',\n",
    "        on_unused_input='ignore',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gg_grad_res = gg_func(\n",
    "        queries_exp.current_documents,\n",
    "        queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "        queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    "    )\n",
    "except Exception as e:\n",
    "    eeee = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueError('Bad input argument to theano function with name \"<ipython-input-19-c6e818cc55a5>:8\"  at index 4(0-based)',\n",
       "           'setting an array element with a sequence.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.isnan(v).any() for v in gg_grad_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res = gg_func(\n",
    "    queries_exp.current_documents,\n",
    "    queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[5]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].shape, gg_res[1].shape, gg_res[2].shape, gg_res[3].shape, gg_res[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.inner(gg_res[0], gg_res[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0] * gg_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.dot(gg_res[0], gg_res[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa.diagonal().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.queried_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_link_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(5):\n",
    "    exp_results.append((i, queries_exp.compute_batch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss / queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time queries_exp.run_batch(queries_exp.test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
