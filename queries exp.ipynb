{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import EmbeddingLayer, InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../external-wiki1.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993444276348966"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8917/9915."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "\n",
    "wordvectors = WordVectors(\n",
    "    fname=\"../GoogleNews-vectors-negative300.bin\",\n",
    "    negvectors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../enwiki-20141208-pages-articles-multistream-redirects4.json') as f:\n",
    "    page_redirects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(object):\n",
    "    \n",
    "    def __init__(self, wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "        self.wordvecs = wordvec\n",
    "        self.queries = queries\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.num_words_to_use_conv = 3\n",
    "        self.redirects = redirects\n",
    "        self.page_content = {}\n",
    "        self.wikipedia_dump_fname = wikipedia_dump_fname\n",
    "        \n",
    "        #self._process_queries()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "    def _process_queries(self):\n",
    "        queried_pages = set()\n",
    "        for docs, q in self.queries.iteritems():\n",
    "            self.wordvecs.tokenize(docs)\n",
    "            for sur, v in q.iteritems():\n",
    "                self.wordvecs.tokenize(sur)\n",
    "                for link in v.keys():\n",
    "                    self.wordvecs.tokenize(link)\n",
    "                    queried_pages.add(WikiRegexes.convertToTitle(link))            \n",
    "\n",
    "        added_pages = set()\n",
    "        for title in queried_pages:\n",
    "            if title in self.redirects:\n",
    "                self.wordvecs.tokenize(self.redirects[title])\n",
    "                added_pages.add(self.redirects[title])\n",
    "        queried_pages |= added_pages\n",
    "                \n",
    "        class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "            \n",
    "            def readPage(ss, title, content):\n",
    "                if title in queried_pages:\n",
    "                    cnt = ss._wikiToText(content)\n",
    "                    self.page_content[title] = self.wordvecs.tokenize(cnt)\n",
    "        \n",
    "        GetWikipediaWords(self.wikipedia_dump_fname).read()\n",
    "               \n",
    "        \n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_sent')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface')\n",
    "        self.x_target_input = T.imatrix('x_target')\n",
    "        self.x_document_id = T.ivector('x_sent_id')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "        self.y_score = T.vector('y')\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix())\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer((None,self.sentence_length), input_var=self.x_document_input)\n",
    "    \n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=350,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='max',\n",
    "        )\n",
    "\n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=300,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.document_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        document_output_length = 250\n",
    "        \n",
    "        self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.document_drop1,\n",
    "            num_units=document_output_length,\n",
    "            name='document_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens2)\n",
    "                \n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length), \n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "        \n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=350,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.surface_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.surface_drop1,\n",
    "            name='surface_dens2',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "        \n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens2]\n",
    "        )\n",
    "        \n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=300,\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "        \n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length), \n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "        \n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            self.target_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_embedding_l,\n",
    "            name='target_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_conv1_l,\n",
    "            name='target_dens1',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_dens2',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "        \n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "        \n",
    "        self.res_l = T.tensordot(self.target_out, self.source_aligned_l, axes=2) / (self.target_out.norm(2, axis=1)  * self.source_out.norm(2, axis=1))\n",
    "        \n",
    "        self.all_params = (\n",
    "            lasagne.layers.get_all_params(self.target_dens2) + \n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "        \n",
    "        self.loss_vec = T.nnet.binary_crossentropy(T.clip(self.res_l, .001, .999), self.y_score)\n",
    "        \n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_score],\n",
    "            [self.res_l, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates\n",
    "        )\n",
    "        \n",
    "        self.test_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_score],\n",
    "            [self.res_l, self.loss_vec.sum(), self.loss_vec],\n",
    "        )\n",
    "        \n",
    "    def compute_batch(self, isTraining=True):\n",
    "        if isTraining:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.current_documents = []\n",
    "        self.current_surface_text = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        \n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            if queries.values()[0]['training'] is not isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_text)\n",
    "                self.current_surface_text.append(self.wordvecs.tokenize(surtxt))\n",
    "                for target in targets['vals'].keys():\n",
    "                    isGold = target == targets['gold']\n",
    "                    cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    \n",
    "            \n",
    "\n",
    "queries_exp = EntityVectorLinkExp(\n",
    "    wikipedia_dump_fname='../enwiki-test-small.xml'\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.queries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'gold': u'Fred Rutherford',\n",
       " u'training': True,\n",
       " u'vals': {u'-NIL-': 0,\n",
       "  u'Alexander Cameron Rutherford': 0,\n",
       "  u'Ernest Rutherford': 0,\n",
       "  u'Fred Rutherford': 0,\n",
       "  u'Jock Rutherford': 0,\n",
       "  u'John Rutherford (rugby union)': 0,\n",
       "  u'Johnny Rutherford': 0,\n",
       "  u'Rutherford': 0,\n",
       "  u'Rutherford (NJT station)': 0,\n",
       "  u'Rutherford AVA': 0,\n",
       "  u'Rutherford County, North Carolina': 0,\n",
       "  u'Rutherford County, Tennessee': 0,\n",
       "  u'Rutherford GO Station': 0,\n",
       "  u'Rutherford, California': 0,\n",
       "  u'Rutherford, Edmonton': 0,\n",
       "  u'Rutherford, New Jersey': 0,\n",
       "  u'Rutherford, New South Wales': 0,\n",
       "  u'Rutherford, Pennsylvania': 0,\n",
       "  u'Rutherford, Tennessee': 0,\n",
       "  u'XXNILXX': 0}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[0].values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.get_numpy_matrix()[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_conv1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_conv1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_dens1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_dens1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_dens2.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (target_dens2.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_conv1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_conv1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_dens1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_dens1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_dens2.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (surface_dens2.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (source_dens1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (source_dens1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (source_dens2.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (source_dens2.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (document_conv1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, 4D)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (document_conv1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (doucment_dens1.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (doucment_dens1.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (document_dens2.W, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, matrix)>, Elemwise{add,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0),\n",
       "             (document_dens2.b, Elemwise{sub,no_inplace}.0),\n",
       "             (<TensorType(float64, vector)>, Elemwise{add,no_inplace}.0)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[target_conv1.W,\n",
       " target_conv1.b,\n",
       " target_dens1.W,\n",
       " target_dens1.b,\n",
       " target_dens2.W,\n",
       " target_dens2.b,\n",
       " surface_conv1.W,\n",
       " surface_conv1.b,\n",
       " surface_dens1.W,\n",
       " surface_dens1.b,\n",
       " surface_dens2.W,\n",
       " surface_dens2.b,\n",
       " source_dens1.W,\n",
       " source_dens1.b,\n",
       " source_dens2.W,\n",
       " source_dens2.b,\n",
       " document_conv1.W,\n",
       " document_conv1.b,\n",
       " doucment_dens1.W,\n",
       " doucment_dens1.b,\n",
       " document_dens2.W,\n",
       " document_dens2.b]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
