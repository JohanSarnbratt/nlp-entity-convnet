{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import EmbeddingLayer, InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "\n",
    "wordvectors = WordVectors(fname=\"/data/matthew/GoogleNews-vectors-negative300.bin\", negvectors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sentiment_sents import Sentiment\n",
    "\n",
    "# just load the sentences from the CNN system\n",
    "#sentiment = Sentiment(\"prevwork/CNN_sentence/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stanfordSentimentReader import StanfordSentimentReader\n",
    "\n",
    "sentiment = StanfordSentimentReader('/data/matthew/stanfordSentimentTreebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210, 8544, 1101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment.test_X), len(sentiment.train_X), len(sentiment.dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the labels binary since that is what we are currently targetting in the training\n",
    "sentiment.train_Y = (np.array(sentiment.train_Y) > .6).astype('int32')\n",
    "sentiment.test_Y = (np.array(sentiment.test_Y) > .6).astype('int32')\n",
    "sentiment.dev_Y = (np.array(sentiment.dev_Y) > .6).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2210, 8544, 1101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment.test_Y), len(sentiment.train_Y), len(sentiment.dev_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in sentiment.test_X:\n",
    "    wordvectors.tokenize(w)\n",
    "for w in sentiment.train_X:\n",
    "    wordvectors.tokenize(w)\n",
    "for w in sentiment.dev_X:\n",
    "    wordvectors.tokenize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003779"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19537"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.word_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentimentExp(object):\n",
    "    \n",
    "    def __init__(self, train_X, train_Y, wordvecs=wordvectors):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.wordvecs = wordvecs\n",
    "        \n",
    "        self.input_size = 10  # not used\n",
    "        self.batch_size = 50\n",
    "        \n",
    "        self.learning_rate = .01\n",
    "        self.momentum = .9\n",
    "        \n",
    "        #self.train_X_rep = np.array([[self.getRep(x)] for x in self.train_X])\n",
    "        self.train_X_rep = np.array([wordvecs.tokenize(x) for x in self.train_X])\n",
    "        \n",
    "        self._setup()\n",
    "\n",
    "    def getRep(self, sent):\n",
    "        ret = []\n",
    "        for i in xrange(self.input_size):\n",
    "            if i < len(sent):\n",
    "                ret.append(self.wordvecs[sent[i]])\n",
    "            else:\n",
    "                ret.append(np.zeros(self.wordvecs.vector_size))\n",
    "        return np.matrix(ret).reshape((1, self.input_size, self.wordvecs.vector_size))\n",
    "\n",
    "    def _setup(self):\n",
    "        self.x_batch = T.imatrix('x')\n",
    "        self.y_batch = T.ivector('y')\n",
    "        \n",
    "        self.input_l = lasagne.layers.InputLayer((None, 50))\n",
    "        \n",
    "        self.embedding_l = EmbeddingLayer(\n",
    "            self.input_l,\n",
    "            W=self.wordvecs.get_numpy_matrix(),\n",
    "            add_word_params=True,\n",
    "        ) \n",
    "        \n",
    "        self.first_l = lasagne.layers.Conv2DLayer(\n",
    "            self.embedding_l,\n",
    "            num_filters=80,\n",
    "            filter_size=(2, self.wordvecs.vector_size),\n",
    "            name='conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.first_l_max = lasagne.layers.Pool2DLayer(\n",
    "            self.first_l,\n",
    "            name='maxing1',\n",
    "            pool_size=(49,1),  # the number 9 should be 50-1 since that would mean it maxes over the whole input....\n",
    "            mode='max',\n",
    "        )\n",
    "        \n",
    "        self.first_l_max_simple = SimpleMaxingLayer(\n",
    "            self.first_l,\n",
    "            name='maxing1',\n",
    "        )\n",
    "        \n",
    "        self.hidden1_l = lasagne.layers.DenseLayer(\n",
    "            self.first_l_max,\n",
    "            num_units=40,\n",
    "            name='dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.hidden1_l_drop = lasagne.layers.DropoutLayer(\n",
    "            self.hidden1_l,\n",
    "            name='drop1',\n",
    "            p=.5,\n",
    "        )\n",
    "        \n",
    "        self.hidden2_l = lasagne.layers.DenseLayer(\n",
    "            self.hidden1_l_drop,\n",
    "            num_units=15,\n",
    "            name='dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.hidden2_l_drop = lasagne.layers.DropoutLayer(\n",
    "            self.hidden2_l,\n",
    "            name='drop2',\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.out_l = lasagne.layers.DenseLayer(\n",
    "            self.hidden1_l_drop,\n",
    "            num_units=1,\n",
    "            name='dens3',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.output = lasagne.layers.get_output(self.out_l, self.x_batch)\n",
    "        \n",
    "        self.loss_vec_old = (self.output.reshape((self.output.size,)) - self.y_batch) ** 2\n",
    "        self.output_diff = T.neq((self.output.flatten() > .5),(self.y_batch > .5))\n",
    "        self.loss_vec = lasagne.objectives.binary_crossentropy(T.clip(self.output.reshape((self.output.size,)), .01, .99), self.y_batch)\n",
    "        \n",
    "        self.loss_val = T.dot(self.loss_vec, self.output_diff)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(self.out_l)\n",
    "        \n",
    "        self.updates = lasagne.updates.adagrad(self.loss_val, self.all_params, .001)\n",
    "        #self.updates = lasagne.updates.apply_momentum(self.updates_adagrad)\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_batch, self.y_batch],\n",
    "            [self.loss_vec.mean(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "        )\n",
    "        \n",
    "        self.loss_func = theano.function(\n",
    "            [self.x_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.loss_vec, self.output_diff.sum()],\n",
    "        )\n",
    "        \n",
    "    def _make_zero(self):\n",
    "        self.embedding_l.W.get_value(borrow=True)[0,:] = 0\n",
    "        \n",
    "    def train(self):\n",
    "        for s in xrange(0, len(self.train_X_rep), self.batch_size):\n",
    "            end = s + self.batch_size\n",
    "            if end > len(self.train_X_rep):\n",
    "                end = len(self.train_X_rep)\n",
    "            X_vals = np.array(self.train_X_rep[s:end]).astype('int32')\n",
    "            y_vals = np.array(self.train_Y[s:end]).astype('int32')\n",
    "            loss, _ = self.train_func(X_vals, y_vals)\n",
    "            self._make_zero()\n",
    "            \n",
    "    def test_loss(self, test_X, test_Y):\n",
    "        test_X_rep = np.array([self.wordvecs.tokenize(x) for x in test_X])\n",
    "        loss_sum = 0.0\n",
    "        wrong = 0.0\n",
    "        for s in xrange(0, len(test_X_rep), self.batch_size):\n",
    "            end = s + self.batch_size\n",
    "            if end > len(test_X_rep):\n",
    "                end = len(test_X_rep)\n",
    "            X_vals = np.array(test_X_rep[s:end]).astype('int32')\n",
    "            y_vals = np.array(test_Y[s:end]).astype('int32')\n",
    "            loss, _, output_diff = self.loss_func(X_vals, y_vals)\n",
    "            wrong += output_diff\n",
    "            loss_sum += loss\n",
    "        return loss_sum / len(test_X_rep), wrong / len(test_X_rep)\n",
    "    \n",
    "experiment = SentimentExp(sentiment.train_X, sentiment.train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embeddings, conv1.W, conv1.b, dens1.W, dens1.b, dens3.W, dens3.b]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94516480938609315, 0.22975187265917604) (1.1210415627551737, 0.27520435967302453) (0.98793939293940025, 0.24253393665158371)\n"
     ]
    }
   ],
   "source": [
    "print experiment.test_loss(sentiment.train_X, sentiment.train_Y), experiment.test_loss(sentiment.dev_X, sentiment.dev_Y), experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, (0.61890593660937065, 0.2731741573033708), (0.66302997196318336, 0.38510445049954589))\n",
      "(19, (0.59905316823446575, 0.2247191011235955), (0.65877878242446353, 0.36784741144414168))\n",
      "(29, (0.58898645997067434, 0.18761704119850187), (0.65519232288619911, 0.37148047229791098))\n",
      "(39, (0.57921442715922844, 0.16982677902621723), (0.66763269934807135, 0.37420526793823794))\n",
      "(49, (0.56708638114222465, 0.1408005617977528), (0.65835189129929028, 0.38601271571298817))\n"
     ]
    }
   ],
   "source": [
    "p_res = []\n",
    "\n",
    "for i in xrange(50):\n",
    "    if i % 10 == 9:\n",
    "        r = i, experiment.test_loss(sentiment.train_X, sentiment.train_Y), experiment.test_loss(sentiment.dev_X, sentiment.dev_Y)\n",
    "        p_res.append(r)\n",
    "        print r\n",
    "    experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.56215729250109614, 0.14173689138576778)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.train_X, sentiment.train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65719801928959309, 0.35240690281562215)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.dev_X, sentiment.dev_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.64638981333090362, 0.3357466063348416)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  (0.61890593660937065, 0.2731741573033708),\n",
       "  (0.66302997196318336, 0.38510445049954589)),\n",
       " (19,\n",
       "  (0.59905316823446575, 0.2247191011235955),\n",
       "  (0.65877878242446353, 0.36784741144414168)),\n",
       " (29,\n",
       "  (0.58898645997067434, 0.18761704119850187),\n",
       "  (0.65519232288619911, 0.37148047229791098)),\n",
       " (39,\n",
       "  (0.57921442715922844, 0.16982677902621723),\n",
       "  (0.66763269934807135, 0.37420526793823794)),\n",
       " (49,\n",
       "  (0.56708638114222465, 0.1408005617977528),\n",
       "  (0.65835189129929028, 0.38601271571298817))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment.first_l.b.get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_func2 = theano.function(\n",
    "    [experiment.x_batch],\n",
    "    [\n",
    "        lasagne.layers.get_output(experiment.first_l, experiment.x_batch),\n",
    "        lasagne.layers.get_output(experiment.first_l_max, experiment.x_batch),\n",
    "        T.max(lasagne.layers.get_output(experiment.first_l, experiment.x_batch), axis=2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 80, 1, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_func2(np.array(experiment.train_X_rep[0:1]).astype('int32'))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_func = theano.function(\n",
    "    [experiment.x_batch, experiment.y_batch],\n",
    "    [experiment.loss_vec.mean(), experiment.loss_vec, experiment.output, \n",
    "     T.grad(experiment.loss_vec.mean(), experiment.out_l.get_params()[0]),\n",
    "     experiment.out_l.get_params()[0], experiment.y_batch, \n",
    "     #lasagne.layers.get_output(experiment.first_l, experiment.x_batch)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_func(np.array(experiment.train_X_rep[0:50]).astype('int32'),np.array(experiment.train_Y[0:50]).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_k = theano.shared(0.)\n",
    "gg_i = T.iscalar('x')\n",
    "\n",
    "gg_res, gg_update = theano.scan(lambda: {gg_k:(gg_k + 1)}, sequences=[range(gg_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = T.matrix()\n",
    "f1 = theano.function([a1], [a1.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = np.array([[1,2,3,0,0],[4,5,6,7,0],[2,2,2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
