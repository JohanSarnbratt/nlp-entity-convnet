{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import EmbeddingLayer, InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../external-wiki1.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993444276348966"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8917/9915."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "\n",
    "wordvectors = WordVectors(\n",
    "    fname=\"../GoogleNews-vectors-negative300.bin\",\n",
    "    negvectors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../enwiki-20141208-pages-articles-multistream-redirects4.json') as f:\n",
    "    page_redirects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(object):\n",
    "    \n",
    "    batch_size = 2000\n",
    "    \n",
    "    def __init__(self, wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "        self.wordvecs = wordvec\n",
    "        self.queries = queries\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.num_words_to_use_conv = 3\n",
    "        self.redirects = redirects\n",
    "        self.page_content = {}\n",
    "        self.wikipedia_dump_fname = wikipedia_dump_fname\n",
    "        \n",
    "        self._process_queries()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "    def _process_queries(self):\n",
    "        queried_pages = set()\n",
    "        for docs, q in self.queries.iteritems():\n",
    "            self.wordvecs.tokenize(docs)\n",
    "            for sur, v in q.iteritems():\n",
    "                self.wordvecs.tokenize(sur)\n",
    "                for link in v['vals'].keys():\n",
    "                    self.wordvecs.tokenize(link)\n",
    "                    queried_pages.add(WikiRegexes.convertToTitle(link))            \n",
    "\n",
    "        added_pages = set()\n",
    "        for title in queried_pages:\n",
    "            if title in self.redirects:\n",
    "                self.wordvecs.tokenize(self.redirects[title])\n",
    "                added_pages.add(self.redirects[title])\n",
    "        queried_pages |= added_pages\n",
    "        \n",
    "        self.queried_pages = queried_pages\n",
    "                \n",
    "        class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "            \n",
    "            def readPage(ss, title, content):\n",
    "                tt = ss.convertToTitle(title)\n",
    "                if tt in queried_pages:\n",
    "                    cnt = ss._wikiToText(content)\n",
    "                    self.page_content[tt] = self.wordvecs.tokenize(cnt)\n",
    "        \n",
    "        GetWikipediaWords(self.wikipedia_dump_fname).read()\n",
    "               \n",
    "        \n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_sent')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface')\n",
    "        self.x_target_input = T.imatrix('x_target')\n",
    "        self.x_document_id = T.ivector('x_sent_id')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "        self.y_score = T.vector('y')\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix())\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer((None,self.sentence_length), input_var=self.x_document_input)\n",
    "    \n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=350,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='max',\n",
    "        )\n",
    "\n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=300,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.document_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        document_output_length = 250\n",
    "        \n",
    "        self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.document_drop1,\n",
    "            num_units=document_output_length,\n",
    "            name='document_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens2)\n",
    "                \n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length), \n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "        \n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=350,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.surface_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.surface_drop1,\n",
    "            name='surface_dens2',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "        \n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens2]\n",
    "        )\n",
    "        \n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=300,\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "        \n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length), \n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "        \n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            self.target_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_embedding_l,\n",
    "            name='target_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_conv1_l,\n",
    "            name='target_dens1',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_dens2',\n",
    "            num_units=300,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "        \n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "        \n",
    "        self.res_l = T.tensordot(self.target_out, self.source_aligned_l, axes=2) / (self.target_out.norm(2, axis=1)  * self.source_out.norm(2, axis=1))\n",
    "        \n",
    "        self.all_params = (\n",
    "            lasagne.layers.get_all_params(self.target_dens2) + \n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "        \n",
    "        self.loss_vec = T.nnet.binary_crossentropy(T.clip(self.res_l, .001, .999), self.y_score)\n",
    "        \n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_score],\n",
    "            [self.res_l, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates\n",
    "        )\n",
    "        \n",
    "        self.test_func = theano.function(\n",
    "            [self.x_document_input,\n",
    "             self.x_surface_text_input, self.x_document_id,\n",
    "             self.x_target_input, self.x_link_id, self.y_score],\n",
    "            [self.res_l, self.loss_vec.sum(), self.loss_vec],\n",
    "        )\n",
    "        \n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_text = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.learning_targets = []\n",
    "        \n",
    "    def compute_batch(self, isTraining=True):\n",
    "        if isTraining:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        \n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] is not isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_text)\n",
    "                self.current_surface_text.append(self.wordvecs.tokenize(surtxt))\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    cnt = self.page_content.get(WikiRegexes.convertToTitle(target), [0]*self.sentence_length)\n",
    "                    self.current_target_input.append(self.wordvecs.tokenize(cnt))\n",
    "                    self.current_target_id.append(surid)\n",
    "                    self.current_target_goal.append(isGold)\n",
    "                    self.learning_targets.append((targets, target))\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "        \n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "            \n",
    "        return self.total_loss / self.total_links\n",
    "        \n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            self.current_documents,\n",
    "            self.current_surface_text, self.current_link_id,\n",
    "            self.current_target_input, self.current_target_id, self.current_target_goal\n",
    "        )\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = res_vec[i]\n",
    "        self.reset_accums()\n",
    "        \n",
    "            \n",
    "\n",
    "queries_exp = EntityVectorLinkExp(\n",
    "    wikipedia_dump_fname='../enwiki-test-small.xml'\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.wordvecs.get_numpy_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(queries_exp.current_documents).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(queries_exp.current_surface_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(queries_exp.current_link_id).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'kani_rakhine',\n",
       " u'computer_and_video_game_genres',\n",
       " u'basic_laws',\n",
       " u'adams_park__lrb_chicago_rrb_',\n",
       " u'crescentia',\n",
       " u'tail_assembly',\n",
       " u'eurasian_jay',\n",
       " u'camino__lrb_oderzo_rrb_',\n",
       " u'duchy_of_stettin',\n",
       " u'robert_randolph_amp_the_family_band',\n",
       " u'adam_hess',\n",
       " u'val_logsdon_fitch',\n",
       " u'oceanic_airlines',\n",
       " u'objectoriented_programming',\n",
       " u'home_riggs_popham',\n",
       " u'radio_paging',\n",
       " u'congress_of_guatemala',\n",
       " u'paul_magloire',\n",
       " u'broadcast_television',\n",
       " u'winwick_rail_crash',\n",
       " u'perth_east_ontario',\n",
       " u'advertisement',\n",
       " u'leigh_bracketts_solar_system_fictional_world',\n",
       " u'cuttingplane_method',\n",
       " u'gamma_ray',\n",
       " u'knockout__lrb_triple_8_song_rrb_',\n",
       " u'_harvest_moon',\n",
       " u'patricia_vickersrich',\n",
       " u'dominican_summer_league',\n",
       " u'holy_crown_of_hungary',\n",
       " u'james_small__lrb_rugby_player_rrb_',\n",
       " u'tomboy_colorado',\n",
       " u'buna_papua_new_guinea',\n",
       " u'orders_decorations_and_medals_of_bulgaria',\n",
       " u'belarusian_phonology',\n",
       " u'cross_river_state_nigeria',\n",
       " u'bismarck_archipelago',\n",
       " u'asdic',\n",
       " u'mechanized_infantry_division',\n",
       " u'1950_british_empire_games',\n",
       " u'united_nations_security_council_resolution_958',\n",
       " u'crawford_county_pennsylvania',\n",
       " u'airspace_class',\n",
       " u'strong_authentication',\n",
       " u'united_nations_security_council_resolution_953',\n",
       " u'united_nations_security_council_resolution_951',\n",
       " u'united_nations_security_council_resolution_957',\n",
       " u'best_costume_design_',\n",
       " u'roberto_cezar_lima_acunha',\n",
       " u'paulmark_elliott',\n",
       " u'egge__lrb_range_rrb_',\n",
       " u'ministry_of_education__lrb_iran_rrb_',\n",
       " u'aftermath__lrb_1994_film_rrb_',\n",
       " u'uprising_against_the',\n",
       " u'ethiopic__lrb_unicode_block_rrb__',\n",
       " u'gankyil',\n",
       " u'anthony_wong_chau_sang',\n",
       " u'fran_ois_darlan_lrb',\n",
       " u'upper_yemen',\n",
       " u'siege_of_gaeta__lrb_1707_rrb_',\n",
       " u'over_and_over__lrb_hot_chip_song_rrb_',\n",
       " u'erichsburg_theological_seminary',\n",
       " u'urban_bank',\n",
       " u'mansfield_railway_station',\n",
       " u'mavra',\n",
       " u'charles_f_voegelin',\n",
       " u'record_of_the',\n",
       " u'polish_national_catholic_church',\n",
       " u'boulevard_brewing_company',\n",
       " u'very_long_baseline_array',\n",
       " u'poltava_battalion',\n",
       " u'want_you',\n",
       " u'anping_henan',\n",
       " u'grgory_aranda',\n",
       " u'atlanta_metropolitan_area',\n",
       " u'grammatical_gender',\n",
       " u'evangelical_lutheran_church_in_america',\n",
       " u'coda__lrb_comics_rrb_',\n",
       " u'forza_italia__lrb_2013_rrb_',\n",
       " u'kids_amp_teens_tv',\n",
       " u'artin_reciprocity',\n",
       " u'squad_automatic_weapon',\n",
       " u'never_mind__lrb_song_rrb_',\n",
       " u'wiley_pennsylvania',\n",
       " u'battle_of_norwalk',\n",
       " u'newland_kingston_upon_hull',\n",
       " u'massachusetts_republican_party',\n",
       " u'do__lrb_company_rrb_',\n",
       " u'tung_chung__lrb_mtr_rrb_',\n",
       " u'acapulco_mexico',\n",
       " u'gann__lrb_ship_rrb_',\n",
       " u'peripheral_venous_catheter',\n",
       " u'portland_parish',\n",
       " u'integrative_psychotherapy',\n",
       " u'ha_derhovagimian',\n",
       " u'wive',\n",
       " u'battle_of_monterrey',\n",
       " u'windham__lrb_town_rrb__new_york',\n",
       " u'wanderer__lrb_unternehmen_rrb_',\n",
       " u'water_supply_and_sanitation_in_india',\n",
       " u'sergio_leone',\n",
       " u'aurelian_walls',\n",
       " u'sal_island',\n",
       " u'paul_watson__lrb_musician_rrb_',\n",
       " u'physical_education',\n",
       " u'scottish_heraldry',\n",
       " u'pure__lrb_lara_fabian_album_rrb_',\n",
       " u'template__lrb_racing_rrb_',\n",
       " u'russian_womens_football_championship',\n",
       " u'numbered_streets_of_st_louis',\n",
       " u'null_morpheme',\n",
       " u'canton_of_saintpierre',\n",
       " u'battle_of_sainteustache',\n",
       " u'atkin__lrb_disambiguation_rrb_',\n",
       " u'scourge_of_the_underworld',\n",
       " u'initiation__lrb_course_of_empire_album_rrb_',\n",
       " u'colombo_cricket_club',\n",
       " u'adam_parfitt',\n",
       " u'gino_silvestri',\n",
       " u'singaporean_general_election_1997',\n",
       " u'national_register_of_historic_places',\n",
       " u'buyeo_kingdom',\n",
       " u'ibrahim_moro',\n",
       " u'little_neck__lrb_new_york_and_flushing_railroad_station_rrb_',\n",
       " u'chris_flannery___lrb_rugby_league_rrb_',\n",
       " u'scott_thomas__lrb_actor_rrb_',\n",
       " u'scirocco',\n",
       " u'getting_married__lrb_strindberg_rrb_',\n",
       " u'john_greaves__lrb_musician_rrb_',\n",
       " u'expectation_value__lrb_quantum_physics_rrb_',\n",
       " u'disembowelment__lrb_band_rrb_',\n",
       " u'paris_match__lrb_band_rrb_',\n",
       " u'sarir',\n",
       " u'local_supercluster',\n",
       " u'terminal_railroad_association',\n",
       " u'doobie_brothers',\n",
       " u'borrisleigh__lrb_electoral_division_rrb_',\n",
       " u'mayan_revival_architecture',\n",
       " u'roger_craig__lrb_baseball_rrb_',\n",
       " u'rock__punk_and_hardcore',\n",
       " u'with',\n",
       " u'sarin',\n",
       " u'norristown_state_hospital',\n",
       " u'1867_in_art',\n",
       " u'guti__lrb_bulgarian_vj_rrb_',\n",
       " u'mike_small__lrb_footballer_rrb_',\n",
       " u'weidman',\n",
       " u'asura__lrb_moth_rrb_',\n",
       " u'patois',\n",
       " u'mundi',\n",
       " u'decomposition_reaction',\n",
       " u'dunstable_town_railway_station',\n",
       " u'cuckold__lrb_book_rrb_',\n",
       " u'western_freeway_brisbane',\n",
       " u'mundt',\n",
       " u'beaumont_mississippi',\n",
       " u'rathmore_county_meath',\n",
       " u'k_dick',\n",
       " u'cnb_bank_raceway_park',\n",
       " u'socrates',\n",
       " u'hofstra',\n",
       " u'the_lyre',\n",
       " u'marylands_5th_congressional_district',\n",
       " u'pope_benedict_viii',\n",
       " u'miguel_lopez_de_legazpi',\n",
       " u'190809_aston_villa_fc_season',\n",
       " u'vedette__lrb_album_rrb_',\n",
       " u'bigband_networks',\n",
       " u'grapefruitdrug_interactions',\n",
       " u'carsten_pump',\n",
       " u'vivant__lrb_grape_rrb_',\n",
       " u'delivery__lrb_song_rrb_',\n",
       " u'otho_iowa',\n",
       " u'halifax_county_high_school',\n",
       " u'natalie',\n",
       " u'december_31__lrb_eastern_orthodox_liturgics_rrb_',\n",
       " u'natalia',\n",
       " u'madrigal',\n",
       " u'vincenzo_sarno',\n",
       " u'groundskeeping',\n",
       " u'thorp',\n",
       " u'borghese_vase',\n",
       " u'myeongjangdong',\n",
       " u'amrum_lighthouse',\n",
       " u'municipality_of_macau',\n",
       " u'curate',\n",
       " u'church_and_oswaldtwistle_cricket_club',\n",
       " u'moriori_people',\n",
       " u'the_powers_that_be',\n",
       " u'the_land_that_time_forgot__lrb_disambiguation_rrb_',\n",
       " u'emt__lrb_mobile_operator_rrb_',\n",
       " u'confirmation__lrb_sacrament_rrb_',\n",
       " u'commando__lrb_1988_film_rrb_',\n",
       " u'south_mountain__lrb_new_haven_county_connecticut_rrb_',\n",
       " u'nort',\n",
       " u'bismarck',\n",
       " u'crawley_green',\n",
       " u'crazy_for_you__lrb_tv_series_rrb_',\n",
       " u'regulation__lrb_law_rrb_',\n",
       " u'wilton_redcar_and_cleveland',\n",
       " u'orpington_fc',\n",
       " u'lyon_county_kansas',\n",
       " u'nore',\n",
       " u'bank_cup',\n",
       " u'nuclear_artillery',\n",
       " u'cbu__lrb_am_rrb_',\n",
       " u'2008_arkansas_razorbacks_football_team',\n",
       " u'jack_osbourne',\n",
       " u'johann_georg_tralles',\n",
       " u'languages_of_south_africa',\n",
       " u'glr_parser',\n",
       " u'usa_falcons',\n",
       " u'unusual__lrb_song_rrb_',\n",
       " u'coke__lrb_fuel_rrb_',\n",
       " u'lasalle_records',\n",
       " u'connecticut_state_university_system',\n",
       " u'nama__lrb_plant_rrb_',\n",
       " u'tartar_guided_missile_fire_control_system',\n",
       " u'citizens_military_training_camp',\n",
       " u'oakey_army_aviation_centre',\n",
       " u'3_sessions',\n",
       " u'san_leandro__lrb_1787_rrb_',\n",
       " u'western_hockey_league__lrb_minor_pro_rrb_',\n",
       " u'phantom__lrb_horse_rrb_',\n",
       " u'grenfell__lrb_nwt_electoral_district_rrb_',\n",
       " u'mrs_tingle',\n",
       " u'royal_birmingham_society_of_artists',\n",
       " u'fiat_india_automobiles',\n",
       " u'amirkabir__lrb_publisher_rrb_',\n",
       " u'diabetes_forecast',\n",
       " u'munich_agreement',\n",
       " u'keith_jackson__lrb_cricketer_rrb_',\n",
       " u'1838_grand_liverpool_steeplechase',\n",
       " u'steve_swales',\n",
       " u'mike_kelley__lrb_writer_rrb_',\n",
       " u'obedience__lrb_human_behavior_rrb_',\n",
       " u'green_hornet__lrb_serial_rrb_',\n",
       " u'blood_oath',\n",
       " u'hebei',\n",
       " u'hash',\n",
       " u'state_route',\n",
       " u'metamorphism__lrb_album_rrb_',\n",
       " u'pennsylvania_route_443',\n",
       " u'udorn_royal_thai_air_force_base',\n",
       " u'michael_kiernan',\n",
       " u'hugh_mccann',\n",
       " u'gharana__lrb_2001_film_rrb_',\n",
       " u'celastrus',\n",
       " u'manambuchavadi_venkatasubbayyar',\n",
       " u'sammy_corporation',\n",
       " u'administration_mission',\n",
       " u'rock_creek_ohio',\n",
       " u'the_sporting_news_player_of_the_year_award',\n",
       " u'twenty20__lrb_film_rrb_',\n",
       " u'lier_il',\n",
       " u'bellini__lrb_cocktail_rrb_',\n",
       " u'scythia',\n",
       " u'machine__lrb_exo_song_rrb_',\n",
       " u'lord_lieutenant_nottinghamshire',\n",
       " u'us_military_pay_grades',\n",
       " u'3rd_army_corps__lrb_italy_rrb_',\n",
       " u'duncan_davidson__lrb_footballer_rrb_',\n",
       " u'elcho__lrb_cdp_rrb__wisconsin',\n",
       " u'heno',\n",
       " u'politics_of_hong_kong',\n",
       " u'sind_club',\n",
       " u'3rd_mechanized_infantry_brigade__lrb_greece_rrb_',\n",
       " u'realplayer',\n",
       " u'dan_rather_reports',\n",
       " u'pliny',\n",
       " u'emperor_xiaowen_of_northern_wei_china',\n",
       " u'347th_rescue_wing',\n",
       " u'silva',\n",
       " u'fort_bridger_wyoming',\n",
       " u'joint__lrb_cannabis_rrb_',\n",
       " u'ina_metro_station',\n",
       " u'los_lobos',\n",
       " u'ernest_gerrard',\n",
       " u'madden_nfl',\n",
       " u'boilerplate__lrb_spaceflight_rrb_',\n",
       " u'millet__lrb_ottoman_empire_rrb_',\n",
       " u'kombat__lrb_military_rank_rrb_',\n",
       " u'war_party__lrb_band_rrb_',\n",
       " u'city_of_little',\n",
       " u'khosrau_ii_persia',\n",
       " u'sepultura_discography',\n",
       " u'mahinthrathirat',\n",
       " u'la_rocheenardenne',\n",
       " u'just__lrb_mark_ronson_song_rrb_',\n",
       " u'crenshaw_los_angeles_california',\n",
       " u'social_vulnerability',\n",
       " u'belhaven_scotland',\n",
       " u'james_merriman',\n",
       " u'spline_interpolation',\n",
       " u'maryland',\n",
       " u'leary',\n",
       " u'gilmore_station',\n",
       " u'charles_rennie_mackintosh',\n",
       " u'barham_arkansas',\n",
       " u'social_democratic_party__lrb_romania_rrb_']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(queries_exp.queried_pages)[1000:1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.page_content.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.current_target_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()[0].values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors.get_numpy_matrix()[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
