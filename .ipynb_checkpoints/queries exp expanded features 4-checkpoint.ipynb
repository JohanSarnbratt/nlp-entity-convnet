{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Working on adding a lot of features just to see if it can get the score up regardless of how complicated or where the data is coming from\n",
    "\n",
    "Features that I will be adding\n",
    "\n",
    "* Taget given surface counts\n",
    "* words from the target and source document\n",
    "  * possible back prop into these vectors, idea is to replace tf-idf with some nn and back prop here\n",
    "* using a linear layer near the output to combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer, WordTokenizer\n",
    "import json\n",
    "import re\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links7-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirect7.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/enwiki-20141208-pages-articles-multistream-surface-counts7.json') as f:\n",
    "    surface_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and make the surfaces items match what we are looking for\n",
    "surface_counts_re = re.compile('([\\.,!\\?])')\n",
    "for sk in surface_counts.keys():\n",
    "    nsk = sk.replace('(', '-lrb-').replace(')', '-rrb-')\n",
    "    nsk = surface_counts_re.sub(' \\\\1', nsk)\n",
    "    if nsk != sk:\n",
    "        surface_counts[nsk] = surface_counts[sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'hello_dolly': 1,\n",
       " u'hello_dolly__lrb_album_rrb_': 6,\n",
       " u'hello_dolly__lrb_film_rrb_': 91,\n",
       " u'hello_dolly__lrb_musical_rrb_': 294,\n",
       " u'hello_dolly__lrb_song_rrb_': 55}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surface_counts.get('hello , dolly !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, vectors=wordvectors, queries=queries, redirects=page_redirects, surface=surface_counts):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    wordvec = WordTokenizer(vectors, sentence_length=200)\n",
    "    documentvec = WordTokenizer(vectors, sentence_length=1)\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                documentvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            documentvec.get_location(redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content):\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "    rs = surface\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        documentvecs = documentvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        surface_counts = rs\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "\n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        self.num_words_to_use_conv = 5\n",
    "\n",
    "        self._setup()\n",
    "\n",
    "\n",
    "    def _setup(self):\n",
    "        #self.x_document_input = T.imatrix('x_doc')\n",
    "\n",
    "        #self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "\n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_matches_counts = T.imatrix('x_matches_counts')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "\n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        self.y_grouping = T.imatrix('y_grouping')\n",
    "\n",
    "\n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        self.embedding_W_docs = theano.shared(self.documentvecs.get_numpy_matrix()).astype(theano.config.floatX)\n",
    "        \n",
    "#         self.document_l = lasagne.layers.InputLayer(\n",
    "#             (None,self.document_length),\n",
    "#             input_var=self.x_document_input\n",
    "#         )\n",
    "\n",
    "#         self.document_embedding_l = EmbeddingLayer(\n",
    "#             self.document_l,\n",
    "#             W=self.embedding_W,\n",
    "#             add_word_params=False,\n",
    "#         )\n",
    "\n",
    "#         self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.document_embedding_l,\n",
    "#             num_filters=500,\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             name='document_conv1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "#             self.document_conv1_l,\n",
    "#             name='document_pool1',\n",
    "#             pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "#             mode='sum',\n",
    "#         )\n",
    "\n",
    "#         self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.document_max_l,\n",
    "#             num_units=250,\n",
    "#             name='doucment_dens1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         document_output_length = 200\n",
    "\n",
    "#         self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop1,\n",
    "#             num_units=225,\n",
    "#             name='document_dens2',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens2,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop2,\n",
    "#             num_units=document_output_length,\n",
    "#             name='document_dens3',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_output = lasagne.layers.get_output(self.document_dens3)\n",
    "\n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "\n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "\n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "\n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_aligned_l = InputLayer(\n",
    "#             (None, document_output_length),\n",
    "#             input_var=self.document_output[self.x_document_id,:]\n",
    "#         )\n",
    "\n",
    "        ##############################################\n",
    "        ## changed to not use the documented\n",
    "\n",
    "#         self.source_l = lasagne.layers.ConcatLayer(\n",
    "#             [self.document_aligned_l, self.surface_dens1]\n",
    "#         )\n",
    "\n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_dens1,   # CHANGED\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "\n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "\n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape(\n",
    "            (self.x_matches_surface.shape[0], 1, 1, 1)).astype(theano.config.floatX)\n",
    "\n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "\n",
    "        self.target_matched_surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None,1,1,1),\n",
    "            input_var=matched_surface_reshaped,\n",
    "        )\n",
    "        \n",
    "        self.target_matched_counts_input_l = lasagne.layers.InputLayer(\n",
    "            (None,4),\n",
    "            input_var=self.x_matches_counts.astype(theano.config.floatX),\n",
    "        )\n",
    "\n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W_docs,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_combined_feats_l = lasagne.layers.ConcatLayer(\n",
    "            [self.target_embedding_l, self.target_matched_surface_input_l,\n",
    "            lasagne.layers.reshape(self.target_matched_counts_input_l, ([0],1,1,[1]))],\n",
    "            axis=3\n",
    "        )\n",
    "\n",
    "        self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length),\n",
    "            input_var=self.x_target_words,\n",
    "        )\n",
    "\n",
    "        self.target_words_embedding_l = EmbeddingLayer(\n",
    "            self.target_words_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_words_embedding_l,\n",
    "            name='target_wrds_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=350,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.target_words_conv1_l,\n",
    "            name='target_wrds_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "            [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "             lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "        )\n",
    "\n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_merge_l,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_embedding_l, #self.target_combined_feats_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "\n",
    "\n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input],\n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "\n",
    "\n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "\n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "\n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "\n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "\n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "\n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "        \n",
    "        #############################\n",
    "        ## Linear features combined\n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        self.linear_features_combined = lasagne.layers.concat(\n",
    "            [lasagne.layers.InputLayer(\n",
    "                    (None, 1), \n",
    "                    input_var=self.res_l.reshape((self.res_l.shape[0], 1)),\n",
    "                ),\n",
    "            lasagne.layers.reshape(self.target_matched_surface_input_l, ([0],1)),\n",
    "            self.target_matched_counts_input_l],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.linear_features_dens_l = lasagne.layers.DenseLayer(\n",
    "            self.linear_features_combined,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "            num_units=1,\n",
    "            name='linear_final_l',\n",
    "            W=lasagne.init.Normal(mean=1.0),\n",
    "        )\n",
    "        \n",
    "        self.linear_output = lasagne.layers.get_output(\n",
    "            lasagne.layers.reshape(self.linear_features_dens_l, ([0],))\n",
    "        )\n",
    "        \n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) *\n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "\n",
    "\n",
    "        #self.golds = self.res_cap[self.y_answer]\n",
    "\n",
    "#         def maxOverRange(indx):\n",
    "#             #return T.max(self.res_cap[T.arange(indx[0],indx[1])]) - self.res_cap[indx[2]]\n",
    "#             #return -( self.res_l[indx[2]] - T.log(T.exp(self.res_l[T.arange(indx[0],indx[1])]).sum()) )\n",
    "#             return -( self.res_l[indx[2]] - self.res_l[indx[0]])\n",
    "\n",
    "#         # build a tensor to make a matrix with one set on each dimention\n",
    "#         self.grouped, grouped_update = theano.scan(maxOverRange, sequences=self.y_grouping)\n",
    "\n",
    "        def setSubSelector(indx, outputs):\n",
    "            return T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "\n",
    "        num_target_samples = self.linear_output.shape[0]\n",
    "\n",
    "        select_seq = T.concatenate([\n",
    "            self.y_grouping,\n",
    "            T.arange(self.y_grouping.shape[0]).reshape((self.y_grouping.shape[0], 1))\n",
    "        ], axis=1)\n",
    "\n",
    "        self.selecting_matrix, _ = theano.scan(\n",
    "            setSubSelector,\n",
    "            outputs_info=T.zeros((num_target_samples, num_target_samples)),\n",
    "            #n_steps=self.y_grouping.shape[0]\n",
    "            sequences=select_seq,\n",
    "        )\n",
    "\n",
    "        self.groupped_elems = T.dot(self.selecting_matrix[-1], \n",
    "                                    T.diag(T.exp(self.linear_output)))\n",
    "        self.groupped_res = T.log(self.groupped_elems.sum(axis=0)[T.arange(self.y_grouping.shape[0])])\n",
    "        self.loss_vec = self.groupped_res - self.linear_output[self.y_grouping[:,2]]\n",
    "\n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff,\n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.linear_features_dens_l)\n",
    "            #lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "\n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "\n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "\n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "\n",
    "        # this one works reasonably well\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "\n",
    "        #self.loss_vec = self.grouped\n",
    "\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "\n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "\n",
    "        self.func_inputs = [\n",
    "            #self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, #self.x_document_id,\n",
    "            self.x_target_input, self.x_matches_surface, self.x_matches_counts, self.x_link_id,\n",
    "            self.y_answer, self.y_grouping\n",
    "        ]  # self.x_target_words,\n",
    "\n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.current_learning_groups = []\n",
    "        self.learning_targets = []\n",
    "        self.current_surface_target_counts = []\n",
    "        \n",
    "        self.failed_match = []\n",
    "            \n",
    "    def compute_batch(self, isTraining=True, useTrainingFunc=True):\n",
    "        if isTraining and useTrainingFunc:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "\n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "\n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] != isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                surcounts = self.surface_counts.get(surmatch)\n",
    "                if not surcounts:\n",
    "                    self.failed_match.append(surmatch)\n",
    "                    surcounts = {}\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_match_counts = []\n",
    "                target_gold_loc = -1\n",
    "                target_group_start = len(self.current_target_input)\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    wiki_title = WikiRegexes.convertToTitle(target)\n",
    "                    cnt = self.documentvecs.get_location(wiki_title)\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                    target_match_counts.append(surcounts.get(wiki_title, 0))\n",
    "                    #if wiki_title not in surcounts:\n",
    "                    #    print surcounts, wiki_title\n",
    "                if target_gold_loc is not None or not isTraining:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    gold_loc = (len(self.current_target_goal) + target_gold_loc)\n",
    "                    sorted_match_counts = [-4,-3,-2,-1] + sorted(set(target_match_counts))\n",
    "                    #print sorted_match_counts\n",
    "                    target_match_counts_indicators = [\n",
    "                        [\n",
    "                            int(s == sorted_match_counts[-1]),\n",
    "                            int(s == sorted_match_counts[-2]),\n",
    "                            int(s == sorted_match_counts[-3]),\n",
    "                            int(s <= sorted_match_counts[-4]),\n",
    "                        ]\n",
    "                        for s in target_match_counts\n",
    "                    ]\n",
    "                    self.current_target_goal += [gold_loc] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                    self.current_surface_target_counts += target_match_counts_indicators\n",
    "                    target_group_end = len(self.current_target_input)\n",
    "                    self.current_learning_groups.append(\n",
    "                        [target_group_start, target_group_end,\n",
    "                         gold_loc])\n",
    "\n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links\n",
    "\n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "\n",
    "        return self.total_loss / self.total_links\n",
    "\n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            #self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, #self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_surface_target_counts, self.current_target_id,\n",
    "            self.current_target_goal, self.current_learning_groups,\n",
    "            # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = float(res_vec[i])\n",
    "        self.reset_accums()\n",
    "\n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "\n",
    "\n",
    "\n",
    "queries_exp = EntityVectorLinkExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/layers/input.py\u001b[0m(62)\u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m     61 \u001b[1;33m                raise ValueError(\"shape has %d dimensions, but variable has \"\n",
      "\u001b[0m\u001b[1;32m---> 62 \u001b[1;33m                                 \"%d\" % (ndim, input_var.ndim))\n",
      "\u001b[0m\u001b[1;32m     63 \u001b[1;33m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentState(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct = 0\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if all_trained > numSamples:\n",
    "                        break\n",
    "                    all_measured += 1\n",
    "                    all_trained += len(en['vals'].values())\n",
    "                    m = max(en['vals'].values())\n",
    "                    if en['vals'][en['gold']] == m and m != 0:\n",
    "                        all_correct += 1\n",
    "           \n",
    "    r = all_measured, float(all_correct) / all_measured\n",
    "    print r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def augmentTrainingData():\n",
    "    for quk in queries.keys():\n",
    "        qu = queries[quk]\n",
    "        for enk in qu.keys():\n",
    "            en = qu[enk]\n",
    "            if not en['gold']:\n",
    "                del qu[enk]\n",
    "        if not qu:\n",
    "            del queries[quk]\n",
    "    for qu in queries.values():\n",
    "        training = random.random() > .15\n",
    "        for en in qu.values():\n",
    "            en['training'] = training\n",
    "augmentTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4, -3, -2, -1, 0, 2, 3, 4, 5, 35, 4738]\n",
      "[-4, -3, -2, -1, 0, 11]\n",
      "[-4, -3, -2, -1, 0, 2, 6, 21, 26, 84, 199]\n",
      "[-4, -3, -2, -1, 0, 48, 152]\n",
      "[-4, -3, -2, -1, 0, 2212]\n",
      "[-4, -3, -2, -1, 0, 55, 91, 294]\n",
      "[-4, -3, -2, -1, 0, 7]\n",
      "[-4, -3, -2, -1, 0, 11, 14, 18, 35, 72, 124, 168, 176]\n",
      "[-4, -3, -2, -1, 0, 244]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 4, 10, 15, 20, 74, 988]\n",
      "[-4, -3, -2, -1, 0, 360]\n",
      "[-4, -3, -2, -1, 0, 60]\n",
      "[-4, -3, -2, -1, 0, 1, 307]\n",
      "[-4, -3, -2, -1, 0, 3, 4, 8, 13, 24, 26, 38, 41, 45, 47, 54, 71, 81, 199, 665]\n",
      "[-4, -3, -2, -1, 0, 3, 5, 27, 32, 39, 40, 41, 42, 48, 64, 71, 185, 733, 20681]\n",
      "[-4, -3, -2, -1, 0, 21, 22, 25, 31, 36, 71, 72, 77, 83, 110, 122, 171, 175, 179, 312, 419, 756]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 6, 8, 9, 73, 337, 519]\n",
      "[-4, -3, -2, -1, 0, 1, 3, 5, 12, 21, 1076]\n",
      "[-4, -3, -2, -1, 0, 312]\n",
      "[-4, -3, -2, -1, 0, 379]\n",
      "[-4, -3, -2, -1, 0, 1021]\n",
      "[-4, -3, -2, -1, 0, 179]\n",
      "[-4, -3, -2, -1, 0, 3, 341]\n",
      "[-4, -3, -2, -1, 0, 5, 352]\n",
      "[-4, -3, -2, -1, 0, 95]\n",
      "[-4, -3, -2, -1, 0, 2, 4, 6, 7, 8, 2005]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 4, 6, 12, 13, 14, 7835]\n",
      "[-4, -3, -2, -1, 0, 12, 13, 16, 53, 4595]\n",
      "[-4, -3, -2, -1, 0, 55]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 5, 9, 10, 11, 12, 13, 14, 16, 19, 8224]\n",
      "[-4, -3, -2, -1, 0, 6, 14, 15, 16, 19, 24, 29, 45, 51, 74, 83, 129, 175, 180, 265, 346, 758]\n",
      "[-4, -3, -2, -1, 0, 1, 18, 23, 25, 30, 83, 166, 289, 686, 1813, 6300]\n",
      "[-4, -3, -2, -1, 0, 7, 15, 32, 34, 35, 95, 106, 122, 151, 194, 328, 491, 551, 943, 1120, 1764, 104478]\n",
      "[-4, -3, -2, -1, 0, 360]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 4, 9, 10, 307]\n",
      "[-4, -3, -2, -1, 0, 1, 10, 19, 20, 24, 34, 39, 56, 84, 92, 247, 569, 9208]\n",
      "[-4, -3, -2, -1, 0, 1, 13, 28, 31, 43, 88, 312, 1338, 1511]\n",
      "[-4, -3, -2, -1, 0, 3, 4, 6, 7, 10, 11, 31, 1911]\n",
      "[-4, -3, -2, -1, 0, 284]\n",
      "[-4, -3, -2, -1, 0, 1, 5, 424]\n",
      "[-4, -3, -2, -1, 0, 1]\n",
      "[-4, -3, -2, -1, 0, 13, 36, 41, 43, 50, 55, 82, 88, 123, 129, 135, 185, 190, 240, 474, 633]\n",
      "[-4, -3, -2, -1, 0, 7, 39, 109, 202, 236, 283, 396, 20279]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 7, 9, 14, 31, 5141]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 5, 7, 8, 10, 18, 4317]\n",
      "[-4, -3, -2, -1, 0, 6, 7, 9, 12, 13, 15, 16, 24, 27, 38, 42, 121, 150, 152, 332, 789]\n",
      "[-4, -3, -2, -1, 0, 7, 15, 32, 34, 35, 95, 106, 122, 151, 194, 328, 491, 551, 943, 1120, 1764, 104478]\n",
      "[-4, -3, -2, -1, 0, 1, 13, 14, 15, 19, 25, 28, 29, 64, 86, 245, 18722]\n",
      "[-4, -3, -2, -1, 0, 2, 3, 5, 6, 40, 77, 198, 505, 716]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 8, 12, 13, 20, 29, 47, 54, 101, 399, 41904]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 6, 11, 15, 16, 22, 51, 64, 83, 10244]\n",
      "[-4, -3, -2, -1, 0, 28]\n",
      "[-4, -3, -2, -1, 0, 15, 18, 21, 34, 37, 46, 55, 58, 4876]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 5, 7, 10, 13, 18, 29, 32, 52, 81, 99, 200, 1389]\n",
      "[-4, -3, -2, -1, 0, 4, 5, 8, 11, 12, 14, 15, 26, 35, 36, 59, 70, 265, 24060]\n",
      "[-4, -3, -2, -1, 0, 3, 8, 16, 19, 21, 22, 30, 32, 34, 66, 74, 102, 17864]\n",
      "[-4, -3, -2, -1, 0]\n",
      "[-4, -3, -2, -1, 0, 2, 4, 1636]\n",
      "[-4, -3, -2, -1, 0, 12, 31, 40, 41, 54, 69, 87, 110, 115, 124, 162, 169, 186, 247, 392, 414, 598, 845]\n",
      "[-4, -3, -2, -1, 0, 2, 7, 15, 21, 23, 24, 29, 72, 82, 146, 160, 195, 217, 251, 273, 705, 725, 80173]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 8, 10, 23]\n",
      "[-4, -3, -2, -1, 0, 223]\n",
      "[-4, -3, -2, -1, 0, 223]\n",
      "[-4, -3, -2, -1, 0, 1, 292]\n",
      "[-4, -3, -2, -1, 0, 28]\n",
      "[-4, -3, -2, -1, 0, 1, 258]\n",
      "[-4, -3, -2, -1, 0, 5, 10, 23, 26, 29, 32, 33, 35, 106, 134, 212, 4348]\n",
      "[-4, -3, -2, -1, 0, 9]\n",
      "[-4, -3, -2, -1, 0, 1, 643]\n",
      "[-4, -3, -2, -1, 0, 136]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 4, 5, 7, 32, 102, 103]\n",
      "[-4, -3, -2, -1, 0, 2, 5, 10, 11, 15, 46, 47, 498, 544]\n",
      "[-4, -3, -2, -1, 0, 2, 5, 10, 11, 15, 46, 47, 498, 544]\n",
      "[-4, -3, -2, -1, 0, 1, 2, 3, 6, 7, 9, 1835]\n",
      "[-4, -3, -2, -1, 0, 1, 5, 6, 12, 1545]\n",
      "[-4, -3, -2, -1, 0, 36]\n",
      "[-4, -3, -2, -1, 0, 10, 406]\n",
      "[-4, -3, -2, -1, 0, 3, 7, 17, 27, 34, 39, 59, 73]\n",
      "None\n",
      "CPU times: user 60.2 ms, sys: 6.37 ms, total: 66.5 ms\n",
      "Wall time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_surface_target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.16967829045756266)\n",
      "(1, 0.16571415255393174)\n",
      "(2, 0.16122683994735021)\n",
      "(3, 0.15552383769311379)\n",
      "(4, 0.14809821685109742)\n",
      "(5, 0.13916736203189845)\n",
      "(6, 0.1290000008617824)\n",
      "(7, 0.11912494987687226)"
     ]
    }
   ],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(8):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.16967829045756266),\n",
       " (1, 0.16571415255393174),\n",
       " (2, 0.16122683994735021),\n",
       " (3, 0.15552383769311379),\n",
       " (4, 0.14809821685109742),\n",
       " (5, 0.13916736203189845),\n",
       " (6, 0.1290000008617824),\n",
       " (7, 0.11912494987687226),\n",
       " ('testing run', 0.1389465785923488),\n",
       " ('training state', (272, 0.11029411764705882)),\n",
       " ('testing state', (275, 0.13818181818181818))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28318, 0.08740024012995268)\n",
      "(28234, 0.08712899341219806)\n"
     ]
    }
   ],
   "source": [
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.10936461746311023)\n",
      "(1, 0.1090357482384916)\n",
      "(2, 0.10882287650253342)\n",
      "(3, 0.10868383639710627)\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(4):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71991277],\n",
       "       [ 1.20530879],\n",
       "       [ 1.18919706],\n",
       "       [ 0.82219219],\n",
       "       [ 0.8795861 ],\n",
       "       [ 0.73494589]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.146415971022\n",
      "0.137583100279\n",
      "0.127909913088\n",
      "0.119306669506\n",
      "0.111491549222\n",
      "CPU times: user 12min 19s, sys: 3.81 s, total: 12min 23s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%time for i in range(5): print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u', 1921 August 8 , 1984 -RRB- , born in [Philadelphia] , was an American television and motion picture actor .': {u'gold': u'Philadelphia',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1375507116317749,\n",
       "   u'2014\\u201315 Philadelphia Flyers season': 0,\n",
       "   u'Advanta Championships of Philadelphia': 0.32732146978378296,\n",
       "   u'Ala\\u015fehir': 0.4266233742237091,\n",
       "   u'Philadelphia': 0.2944180965423584,\n",
       "   u'Philadelphia (film)': 0.31371015310287476,\n",
       "   u'Philadelphia (magazine)': 0.4119341969490051,\n",
       "   u'Philadelphia 76ers': 0.4266233742237091,\n",
       "   u'Philadelphia County, Pennsylvania': 0.3454357981681824,\n",
       "   u'Philadelphia Eagles': 0.3036160171031952,\n",
       "   u'Philadelphia Flyers': 0.38959649205207825,\n",
       "   u'Philadelphia International Airport': 0.27821075916290283,\n",
       "   u'Philadelphia Phantoms': 0.3531772494316101,\n",
       "   u'Philadelphia Phillies': 0.4097326397895813,\n",
       "   u'Philadelphia Union': 0.46683377027511597,\n",
       "   u'Philadelphia, Mississippi': 0.2954559922218323,\n",
       "   u'Philadelphia, Pennsylvania': 0.3304784893989563,\n",
       "   u'Roman Catholic Archdiocese of Philadelphia': 0.36522436141967773,\n",
       "   u'U.S. Pro Indoor': 0.4610070288181305,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"-RRB- and as Roger '' Cutes '' Buell on '' [The Mothers-in-Law] '' .\": {u'gold': u'The Mothers-in-Law',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14011892676353455,\n",
       "   u'Mothers-in-Law': 0.36822089552879333,\n",
       "   u'The Mothers-in-Law': 0.4538388252258301,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Bascombe '' -RRB- about [Gordon MacRae] -LRB- '' Billy Bigelow '' -RRB- in the famous ''\": {u'gold': u'Gordon MacRae',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.13730615377426147,\n",
       "   u'Calum MacRae': 0,\n",
       "   u'Gordon MacRae': 0.3723524808883667,\n",
       "   u'Jade MacRae': 0.3638676106929779,\n",
       "   u'MacRae': 0.4806947112083435,\n",
       "   u'MacRae (surname)': 0,\n",
       "   u'William MacRae': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Birds '' and a larger role in the original '' [Invasion of the Body Snatchers] '' , having portrayed a physician in the '' book-end\": {u'gold': u'Invasion of the Body Snatchers',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.143370121717453,\n",
       "   u'2003 invasion of Iraq': 0.22751200199127197,\n",
       "   u'Battle of France': 0.3860406279563904,\n",
       "   u'Invasion': 0.22362792491912842,\n",
       "   u'Invasion (1965 film)': 0.328948438167572,\n",
       "   u'Invasion (1997 film)': 0.22751200199127197,\n",
       "   u'Invasion (TV series)': 0.227253258228302,\n",
       "   u'Invasion Body Snatchers': 0,\n",
       "   u'Invasion of': 0,\n",
       "   u'Invasion of Normandy': 0.3830951750278473,\n",
       "   u'Invasion of Poland': 0.3080991804599762,\n",
       "   u'Invasion of Poland (1939)': 0.5427054762840271,\n",
       "   u'Invasion of the': 0,\n",
       "   u'Invasion of the Body': 0,\n",
       "   u'Invasion of the Body Snatchers': 0.4623206853866577,\n",
       "   u'Invasion of the Body Snatchers (1956 film)': 0.3587723970413208,\n",
       "   u'Invasion of the Body Snatchers (1978 film)': 0.3404694199562073,\n",
       "   u'Invasion of the Body Snatchers (1978 film) ': 0,\n",
       "   u'The Invasion (professional wrestling)': 0.3535470962524414,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Cooley on '' The Dick Van Dyke Show '' , [Fred Rutherford] on '' Leave It to Beaver '' -LRB- Mr.\": {u'gold': u'Fred Rutherford',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1420750916004181,\n",
       "   u'Alexander Cameron Rutherford': 0.45115038752555847,\n",
       "   u'Ernest Rutherford': 0.357097864151001,\n",
       "   u'Fred Rutherford': 0.2343652844429016,\n",
       "   u'Jock Rutherford': 0.36019033193588257,\n",
       "   u'John Rutherford (rugby union)': 0.21985900402069092,\n",
       "   u'Johnny Rutherford': 0.24439701437950134,\n",
       "   u'Rutherford': 0.4237435758113861,\n",
       "   u'Rutherford (NJT station)': 0.32063648104667664,\n",
       "   u'Rutherford AVA': 0.2981616258621216,\n",
       "   u'Rutherford County, North Carolina': 0.30143481492996216,\n",
       "   u'Rutherford County, Tennessee': 0.2618507146835327,\n",
       "   u'Rutherford GO Station': 0.2725502848625183,\n",
       "   u'Rutherford, California': 0.29584935307502747,\n",
       "   u'Rutherford, Edmonton': 0.41103607416152954,\n",
       "   u'Rutherford, New Jersey': 0.3028603792190552,\n",
       "   u'Rutherford, New South Wales': 0.3137529492378235,\n",
       "   u'Rutherford, Pennsylvania': 0,\n",
       "   u'Rutherford, Tennessee': 0.2672305107116699,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Deacon also appeared on '' [The Addams Family] '' in April 1965 , and puts Cousin Itt through\": {u'gold': u'The Addams Family',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14230084419250488,\n",
       "   u'Addams Family': 0.4054843485355377,\n",
       "   u'Addams Family (pinball)': 0.46983271837234497,\n",
       "   u\"Children's film\": 0,\n",
       "   u'Family': 0.3440818190574646,\n",
       "   u'Family (1976 TV series)': 0.4172076880931854,\n",
       "   u'Family (band)': 0.3440818190574646,\n",
       "   u'Family (biology)': 0.42798346281051636,\n",
       "   u'Family Channel': 0.49524110555648804,\n",
       "   u'The Addams Family': 0.4342707693576813,\n",
       "   u'The Addams Family (1964 TV series)': 0.39349472522735596,\n",
       "   u'The Addams Family (1973 animated series)': 0.45535576343536377,\n",
       "   u'The Addams Family (1992 animated series)': 0.2944346070289612,\n",
       "   u'The Addams Family (film)': 0.5724552869796753,\n",
       "   u'The Addams Family (musical)': 0.36424630880355835,\n",
       "   u'The Addams Family (pinball)': 0.4055261015892029,\n",
       "   u'The Addams Family (video game)': 0.49524110555648804,\n",
       "   u'The Addams Family Theme': 0.4974759519100189,\n",
       "   u'XXNILXX': 0,\n",
       "   u'family (biology)': 0.42798346281051636}},\n",
       " u\"Game '' , and played '' Horace Vandergelder '' opposite [Phyllis Diller] 's '' Dolly Gallagher Levi '' in a touring production\": {u'gold': u'Phyllis Diller',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.13885244727134705,\n",
       "   u'Diller': 0.38936978578567505,\n",
       "   u'Diller Glacier': 0,\n",
       "   u'Diller, Nebraska': 0.41487231850624084,\n",
       "   u'LeGrande A. Diller': 0,\n",
       "   u'Phyllis Diller': 0.4966685175895691,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"He had a brief role in [Alfred Hitchcock] 's film '' The Birds '' and a larger role\": {u'gold': u'Alfred Hitchcock',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1412067711353302,\n",
       "   u'Alfred Hitchcock': 0.3677777945995331,\n",
       "   u'Alfred Hitchcock (book)': 0,\n",
       "   u'Alfred Hitchcock (police officer)': 0,\n",
       "   u'Cameron Hitchcock': 0,\n",
       "   u'E. Hitchcock': 0.23738893866539001,\n",
       "   u'Edward Hitchcock': 0.21904999017715454,\n",
       "   u'Hitchcock': 0.25836825370788574,\n",
       "   u'Hitchcock (automobile)': 0,\n",
       "   u'Hitchcock (film)': 0.238288015127182,\n",
       "   u'Hitchcock County, Nebraska': 0.24673625826835632,\n",
       "   u'Hitchcock, Oklahoma': 0,\n",
       "   u'Hitchcock, South Dakota': 0.23412570357322693,\n",
       "   u'Hitchcock, Texas': 0.23412570357322693,\n",
       "   u'Ken Hitchcock': 0.4552902281284332,\n",
       "   u'Sterling Hitchcock': 0.33782461285591125,\n",
       "   u'The Birds (film)': 0.3313348889350891,\n",
       "   u'Tom Hitchcock': 0,\n",
       "   u'Tommy Hitchcock (racing driver)': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"His best-known roles are Mel Cooley on '' [The Dick Van Dyke Show] '' , Fred Rutherford on '' Leave It to Beaver\": {u'gold': u'The Dick Van Dyke Show',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14204716682434082,\n",
       "   u'Agricultural show': 0.32377535104751587,\n",
       "   u'Days of Our Lives': 0.3416615128517151,\n",
       "   u'Days of our Lives': 0.3416615128517151,\n",
       "   u'Dick Van Dyke Show': 0.32377535104751587,\n",
       "   u'Dyke Show': 0,\n",
       "   u'Eric Show': 0.3376252055168152,\n",
       "   u'General Hospital': 0.35137563943862915,\n",
       "   u'Show': 0.3277598023414612,\n",
       "   u'Show (The Cure album)': 0.3818715810775757,\n",
       "   u'Show (The Jesus Lizard album)': 0,\n",
       "   u'Show (film)': 0.3416615128517151,\n",
       "   u'Show (magazine)': 0.5407189130783081,\n",
       "   u'Showbiz (producer)': 0.3476346731185913,\n",
       "   u'Showbiz and A.G.': 0.2777496576309204,\n",
       "   u'The Dick Van Dyke Show': 0.3656986951828003,\n",
       "   u'The Dick Van Dyke Show ': 0,\n",
       "   u'The Van Dyke Show': 0.4664986729621887,\n",
       "   u'Van Dyke Show': 0.3818715810775757,\n",
       "   u'XXNILXX': 0}},\n",
       " u'In real life , he was a gourmet [chef] .': {u'gold': u'Chef',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14293155074119568,\n",
       "   u'Celebrity chef': 0.49146702885627747,\n",
       "   u'Chef': 0.26782551407814026,\n",
       "   u'Chef (2014 film)': 0.3101179003715515,\n",
       "   u'Chef (South Park character)': 0.3457689583301544,\n",
       "   u'Chef (South Park)': 0.25276029109954834,\n",
       "   u'Chef (company)': 0.3457689583301544,\n",
       "   u'Chef (film)': 0.39545679092407227,\n",
       "   u'Chef (programming language)': 0.4698203504085541,\n",
       "   u'Chef (software)': 0.3259720206260681,\n",
       "   u\"Chef's uniform\": 0.26782551407814026,\n",
       "   u'Cook (profession)': 0.4056297838687897,\n",
       "   u'Isaac Hayes': 0.3419841527938843,\n",
       "   u'Japanese cuisine': 0.3567725718021393,\n",
       "   u'Magician': 0.4068875014781952,\n",
       "   u'Michael Smith (chef)': 0.2975582778453827,\n",
       "   u'Swedish Chef': 0.49968546628952026,\n",
       "   u'XXNILXX': 0,\n",
       "   u'celebrity chef': 0.49146702885627747,\n",
       "   u'chef': 0.26782551407814026}},\n",
       " u\"In the 1956 motion picture '' [Carousel] '' , adapted from the classic Rodgers and Hammerstein stage\": {u'gold': u'Carousel (film)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14358368515968323,\n",
       "   u'Carousel': 0.3678213059902191,\n",
       "   u'Carousel (1923 film)': 0.4475274085998535,\n",
       "   u'Carousel (1956 film)': 0.34335166215896606,\n",
       "   u'Carousel (Blink-182 song)': 0,\n",
       "   u'Carousel (Leila K album)': 0,\n",
       "   u'Carousel (TV channel)': 0.42665737867355347,\n",
       "   u'Carousel (Vanessa Carlton song)': 0.3423123359680176,\n",
       "   u'Carousel (advert)': 0.4475274085998535,\n",
       "   u'Carousel (advertisement)': 0.5432241559028625,\n",
       "   u'Carousel (album)': 0,\n",
       "   u'Carousel (ballet)': 0.34335166215896606,\n",
       "   u'Carousel (film)': 0.5893577337265015,\n",
       "   u'Carousel (musical)': 0.3922433853149414,\n",
       "   u'Carousel slide projector': 0.4538213014602661,\n",
       "   u'Cheshire Cat (Blink-182 album)': 0.3423123359680176,\n",
       "   u'Ford Carousel': 0,\n",
       "   u'Rabbits on the Run': 0.49457746744155884,\n",
       "   u'Westfield Carousel': 0.30668511986732483,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Invasion of the Body Snatchers '' , having portrayed a [physician] in the '' book-end '' sequences added to the beginning\": {u'gold': u'Physician',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14295214414596558,\n",
       "   u'Doctor of Medicine': 0.19836732745170593,\n",
       "   u'General practitioner': 0.1582048535346985,\n",
       "   u'Internal medicine': 0.14295214414596558,\n",
       "   u'Islamic medicine': 0.29539158940315247,\n",
       "   u'John Snow (physician)': 0.19836732745170593,\n",
       "   u'Medicine': 0.22405856847763062,\n",
       "   u'Medicine in medieval Islam': 0.16689631342887878,\n",
       "   u'Medicine in the medieval Islamic world': 0.19836732745170593,\n",
       "   u'Physician': 0.1803392469882965,\n",
       "   u'Physician (horse)': 0,\n",
       "   u'Physics': 0.3089442849159241,\n",
       "   u'Physiology': 0.33513063192367554,\n",
       "   u\"Ship's doctor\": 0,\n",
       "   u'Traditional Chinese medicine': 0.180734783411026,\n",
       "   u'XXNILXX': 0,\n",
       "   u'medical doctor': 0.22959184646606445,\n",
       "   u'medicine': 0.22405856847763062,\n",
       "   u'physician': 0.1803392469882965,\n",
       "   u'physiology': 0.33513063192367554}},\n",
       " u\"Levi '' in a touring production of the musical '' [Hello , Dolly !] '' .\": {u'gold': u'Hello, Dolly! (musical)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.13848063349723816,\n",
       "   u'Hello': 0.43076568841934204,\n",
       "   u'Hello (2008 film)': 0.37521594762802124,\n",
       "   u'Hello (Ice Cube song)': 0.41621917486190796,\n",
       "   u'Hello (Lionel Richie song)': 0.28791576623916626,\n",
       "   u'Hello (Martin Solveig song)': 0.5072103142738342,\n",
       "   u'Hello (band)': 0.3589651584625244,\n",
       "   u'Hello (magazine)': 0.4575439691543579,\n",
       "   u'Hello ,': 0,\n",
       "   u'Hello , Dolly': 0,\n",
       "   u'Hello , Dolly !': 0,\n",
       "   u'Hello Dolly': 0,\n",
       "   u'Hello Dolly (film)': 0.5072103142738342,\n",
       "   u'Hello Dolly (movie)': 0.3643835186958313,\n",
       "   u'Hello Dolly (song)': 0.3953426778316498,\n",
       "   u'Hello, Dolly!': 0,\n",
       "   u'Hello, Dolly! (film)': 0.5072103142738342,\n",
       "   u'Hello, Dolly! (musical)': 0.3953426778316498,\n",
       "   u'Hello, Dolly! (song)': 0.3953426778316498,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , 1921 August 8 , [1984] -RRB- , born in Philadelphia , was an American television': {u'gold': u'1984',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.13988032937049866,\n",
       "   u'1984': 0.27177873253822327,\n",
       "   u'1984 Grand Prix motorcycle racing season': 0.3690181076526642,\n",
       "   u'1984 NBA draft': 0.5514883399009705,\n",
       "   u'1984 NCAA Division I-A football season': 0.4930170476436615,\n",
       "   u'1984 NFL season': 0.3363446593284607,\n",
       "   u'1984 NHL Entry Draft': 0.45707789063453674,\n",
       "   u'1984 Summer Olympics': 0.34161919355392456,\n",
       "   u'1984 Winter Olympics': 0.3587482273578644,\n",
       "   u'1984 in film': 0.29305484890937805,\n",
       "   u'1984 in literature': 0.3190072178840637,\n",
       "   u'1984 in music': 0.38914740085601807,\n",
       "   u'1984 in television': 0.5108427405357361,\n",
       "   u'1984 in video gaming': 0.46174073219299316,\n",
       "   u'Australian federal election, 1984': 0.5268444418907166,\n",
       "   u'Nineteen Eighty-Four': 0.2846713066101074,\n",
       "   u'UEFA Euro 1984': 0.41721102595329285,\n",
       "   u'United States House of Representatives elections, 1984': 0.4850303530693054,\n",
       "   u'United States presidential election, 1984': 0.31587404012680054,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , 1921 [August 8] , 1984 -RRB- , born in Philadelphia , was an': {u'gold': u'August 8',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1419651210308075,\n",
       "   u'/August': 0.4071792960166931,\n",
       "   u'August': 0.4071792960166931,\n",
       "   u'August (1996 film)': 0.5610852837562561,\n",
       "   u'August (2008 film)': 0.267762690782547,\n",
       "   u'August (Eric Clapton album)': 0.30487877130508423,\n",
       "   u'August (Fringe)': 0.4699925482273102,\n",
       "   u'August (company)': 0.30788612365722656,\n",
       "   u'August 1966': 0.45177680253982544,\n",
       "   u'August 1971': 0.3508235216140747,\n",
       "   u'August 1972': 0.5769028663635254,\n",
       "   u'August 1973': 0.5664682984352112,\n",
       "   u'August 1975': 0.4432606101036072,\n",
       "   u'August 2007': 0.5340166091918945,\n",
       "   u'August 8': 0.4218462407588959,\n",
       "   u'August 8 (Eastern Orthodox liturgics)': 0.5170281529426575,\n",
       "   u'August, California': 0,\n",
       "   u'CMLL Super Viernes (August 2014)': 0,\n",
       "   u'Papal conclave, August 1978': 0.2564549744129181,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- May 14 , [1921] August 8 , 1984 -RRB- , born in Philadelphia ,': {u'gold': u'1921',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14340811967849731,\n",
       "   u'144th New York State Legislature': 0.33044642210006714,\n",
       "   u'1921': 0.3997463583946228,\n",
       "   u'1921 APFA season': 0.30719083547592163,\n",
       "   u'1921 Green Bay Packers season': 0,\n",
       "   u'1921 Indianapolis 500': 0.523318886756897,\n",
       "   u'1921 NFL season': 0.4499818980693817,\n",
       "   u'1921 VFL season': 0.29532691836357117,\n",
       "   u'1921 World Series': 0.3651772737503052,\n",
       "   u'1921 college football season': 0.5429131388664246,\n",
       "   u'1921 in Canada': 0.3621627688407898,\n",
       "   u'1921 in aviation': 0.3345567584037781,\n",
       "   u'1921 in film': 0.27560293674468994,\n",
       "   u'1921 in literature': 0.3345567584037781,\n",
       "   u'1921 in poetry': 0.4747360646724701,\n",
       "   u'1921 in the United States': 0.39124763011932373,\n",
       "   u'Canadian federal election, 1921': 0.2863687574863434,\n",
       "   u'Irish elections, 1921': 0.34314054250717163,\n",
       "   u'Norwegian parliamentary election, 1921': 0.4175400137901306,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Richard Deacon -LRB- [May 14] , 1921 August 8 , 1984 -RRB- , born in': {u'gold': u'May 14',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.13893386721611023,\n",
       "   u'/May': 0.25247299671173096,\n",
       "   u'Ben May': 0.22071290016174316,\n",
       "   u'Brian May': 0.4376189112663269,\n",
       "   u'CMLL Super Viernes (May 2014)': 0,\n",
       "   u'Darrell May': 0.3132786154747009,\n",
       "   u'David May (footballer)': 0.5833011865615845,\n",
       "   u'Jonny May': 0.37462639808654785,\n",
       "   u'Kathy May': 0,\n",
       "   u'List of Pok\\xe9mon anime characters': 0.27335697412490845,\n",
       "   u'May': 0.25247299671173096,\n",
       "   u'May (Pok\\xe9mon)': 0.32922011613845825,\n",
       "   u'May (film)': 0.449781596660614,\n",
       "   u'May 14': 0.44619303941726685,\n",
       "   u'May 14 (Eastern Orthodox liturgics)': 0.687739372253418,\n",
       "   u'May, Oklahoma': 0.4376189112663269,\n",
       "   u'May, Texas': 0.349295437335968,\n",
       "   u'Sean May': 0.46635740995407104,\n",
       "   u'Stevie May': 0.4628463387489319,\n",
       "   u'XXNILXX': 0}},\n",
       " u'The bald and usually bespectacled [character actor] often portrayed imperious authority figures .': {u'gold': u'Character actor',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14090222120285034,\n",
       "   u'Academy Award for Best Actor': 0.3590152859687805,\n",
       "   u'Acting': 0.30501115322113037,\n",
       "   u'Actor': 0.30501115322113037,\n",
       "   u'Actor (UML)': 0.5044218897819519,\n",
       "   u'Actor (album)': 0.38457435369491577,\n",
       "   u'Actor (mythology)': 0.3138546049594879,\n",
       "   u'Character actor': 0.35113242268562317,\n",
       "   u'Film actor': 0.32630783319473267,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Miniseries or Television Film': 0.29526281356811523,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Motion Picture Drama': 0.393133282661438,\n",
       "   u'Golden Globe Award for Best Actor \\u2013 Motion Picture Musical or Comedy': 0.4189717769622803,\n",
       "   u'Pornographic film actor': 0.2744152545928955,\n",
       "   u'XXNILXX': 0,\n",
       "   u'acting': 0.30501115322113037,\n",
       "   u'actor': 0.30501115322113037,\n",
       "   u'character actor': 0.35113242268562317,\n",
       "   u'film actor': 0.32630783319473267,\n",
       "   u'pornographic actor': 0.30501115322113037,\n",
       "   u'television actor': 0.5078597664833069}},\n",
       " u\"had a brief role in Alfred Hitchcock 's film '' [The Birds] '' and a larger role in the original '' Invasion\": {u'gold': u'The Birds (film)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14511367678642273,\n",
       "   u'Bird': 0.4095912575721741,\n",
       "   u'Bird (disambiguation)': 0.21736294031143188,\n",
       "   u'Bird (film)': 0.2663971185684204,\n",
       "   u'Birds': 0.34116336703300476,\n",
       "   u'Birds (Anouk song)': 0.2929503321647644,\n",
       "   u'Birds (Bic Runga album)': 0,\n",
       "   u'Birds (Kate Nash song)': 0,\n",
       "   u'Birds, Illinois': 0.2663971185684204,\n",
       "   u'Greg Bird': 0.34447169303894043,\n",
       "   u'Larry Bird': 0.351001501083374,\n",
       "   u'Ryan Bird': 0,\n",
       "   u'Sue Bird': 0.27784544229507446,\n",
       "   u'The Birds': 0.29759448766708374,\n",
       "   u'The Birds (Respighi)': 0.377322793006897,\n",
       "   u'The Birds (band)': 0.2663971185684204,\n",
       "   u'The Birds (film)': 0.33358868956565857,\n",
       "   u'The Birds (play)': 0.2663971185684204,\n",
       "   u'The Birds (story)': 0.31748831272125244,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"he had a bit role as the policeman who admonishes [Shirley Jones] -LRB- '' Julie '' -RRB- and John Dehner -LRB- ''\": {u'gold': u'Shirley Jones',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1390082836151123,\n",
       "   u'Asjha Jones': 0.49128568172454834,\n",
       "   u'Jermaine Jones': 0.5707471370697021,\n",
       "   u'Jone': 0.3635709583759308,\n",
       "   u'Jone (band)': 0,\n",
       "   u'Jone (opera)': 0.33372706174850464,\n",
       "   u'Jone Pedro': 0,\n",
       "   u'Jone da Silva Pinto': 0,\n",
       "   u'Jones': 0.37280410528182983,\n",
       "   u'Jones County, Texas': 0.361398845911026,\n",
       "   u'Kenwyne Jones': 0.4325481355190277,\n",
       "   u'Nathan Jones (Australian rules footballer)': 0.36154335737228394,\n",
       "   u'Shirley Jones': 0.28522711992263794,\n",
       "   u'Shirley Jones (R&amp;B singer)': 0,\n",
       "   u'Shirley Jones (horse)': 0,\n",
       "   u'Shirley Jones (politician)': 0,\n",
       "   u'Stacey Jones': 0.33873558044433594,\n",
       "   u'The Jones Girls': 0.41596782207489014,\n",
       "   u'Todd Jones': 0.45294487476348877,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"revealed to have been gay , and his interview with [Boze Hadleigh] was published in Hadleigh 's '' Hollywood Gays '' ,\": {u'gold': u'Boze Hadleigh',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14224490523338318,\n",
       "   u'Boze Hadleigh': 0.3828426003456116,\n",
       "   u'Hadleigh': 0.34945929050445557,\n",
       "   u'Hadleigh (TV series)': 0.3883516490459442,\n",
       "   u'Hadleigh (disambiguation)': 0.3531455397605896,\n",
       "   u'Hadleigh Airfield': 0,\n",
       "   u'Hadleigh Castle': 0.547380805015564,\n",
       "   u'Hadleigh railway station': 0,\n",
       "   u'Hadleigh, Essex': 0.33463096618652344,\n",
       "   u'Hadleigh, Suffolk': 0.3883516490459442,\n",
       "   u'RAF Hadleigh': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'series of cookbook s and hosted a television series on [microwave] cooking .': {u'gold': u'Microwave oven',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14058175683021545,\n",
       "   u'British Telecom microwave network': 0.5859387516975403,\n",
       "   u'Cavalier Computer': 0,\n",
       "   u'EM spectrum': 0.41223201155662537,\n",
       "   u'Electromagnetic spectrum': 0.47080156207084656,\n",
       "   u'Microwave': 0.3478620648384094,\n",
       "   u'Microwave (game)': 0,\n",
       "   u'Microwave chemistry': 0.42084255814552307,\n",
       "   u'Microwave oven': 0.39732635021209717,\n",
       "   u'Microwave radio relay': 0.5208477973937988,\n",
       "   u'Microwave radiometer': 0.2457391321659088,\n",
       "   u'Microwave spectroscopy': 0.3478620648384094,\n",
       "   u'Microwave transmission': 0.39732635021209717,\n",
       "   u'Microwaves': 0.5208477973937988,\n",
       "   u'Rotational spectroscopy': 0.43886637687683105,\n",
       "   u'Waldorf Microwave': 0,\n",
       "   u'XXNILXX': 0,\n",
       "   u'microwave': 0.3478620648384094,\n",
       "   u'microwave oven': 0.39732635021209717,\n",
       "   u'microwave transmission': 0.39732635021209717}},\n",
       " u'the 1970s and 1980s , he wrote a series of [cookbook] s and hosted a television series on microwave cooking .': {u'gold': u'cookbook',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.14588704705238342,\n",
       "   u'Cookbook': 0.48305511474609375,\n",
       "   u\"Nanny Ogg's Cookbook\": 0.5628048181533813,\n",
       "   u'XXNILXX': 0,\n",
       "   u'cookbook': 0.48305511474609375}},\n",
       " u\"who admonishes Shirley Jones -LRB- '' Julie '' -RRB- and [John Dehner] -LRB- '' Mr.\": {u'gold': u'John Dehner',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.1386585235595703,\n",
       "   u'Dehner': 0.49209365248680115,\n",
       "   u'John Dehner': 0.4270651638507843,\n",
       "   u'XXNILXX': 0}}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
