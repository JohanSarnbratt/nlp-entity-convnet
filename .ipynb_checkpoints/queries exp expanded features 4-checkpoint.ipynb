{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Working on adding a lot of features just to see if it can get the score up regardless of how complicated or where the data is coming from\n",
    "\n",
    "Features that I will be adding\n",
    "\n",
    "* Taget given surface counts\n",
    "* words from the target and source document\n",
    "  * possible back prop into these vectors, idea is to replace tf-idf with some nn and back prop here\n",
    "* using a linear layer near the output to combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer, WordTokenizer\n",
    "import json\n",
    "import re\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links7-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirect7.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/enwiki-20141208-pages-articles-multistream-surface-counts7.json') as f:\n",
    "    surface_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and make the surfaces items match what we are looking for\n",
    "surface_counts_re = re.compile('([\\.,!\\?])')\n",
    "for sk in surface_counts.keys():\n",
    "    nsk = sk.replace('(', '-lrb-').replace(')', '-rrb-')\n",
    "    nsk = surface_counts_re.sub(' \\\\1', nsk)\n",
    "    if nsk != sk:\n",
    "        surface_counts[nsk] = surface_counts[sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, vectors=wordvectors, queries=queries, redirects=page_redirects, surface=surface_counts):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    wordvec = WordTokenizer(vectors, sentence_length=200)\n",
    "    documentvec = WordTokenizer(vectors, sentence_length=1)\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                documentvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            documentvec.get_location(redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content):\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "    rs = surface\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        documentvecs = documentvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        surface_counts = rs\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "\n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        self.num_words_to_use_conv = 5\n",
    "\n",
    "        self._setup()\n",
    "\n",
    "\n",
    "    def _setup(self):\n",
    "        #self.x_document_input = T.imatrix('x_doc')\n",
    "\n",
    "        #self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "\n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_matches_counts = T.imatrix('x_matches_counts')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "\n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        self.y_grouping = T.imatrix('y_grouping')\n",
    "\n",
    "\n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        self.embedding_W_docs = theano.shared(self.documentvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        \n",
    "#         self.document_l = lasagne.layers.InputLayer(\n",
    "#             (None,self.document_length),\n",
    "#             input_var=self.x_document_input\n",
    "#         )\n",
    "\n",
    "#         self.document_embedding_l = EmbeddingLayer(\n",
    "#             self.document_l,\n",
    "#             W=self.embedding_W,\n",
    "#             add_word_params=False,\n",
    "#         )\n",
    "\n",
    "#         self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.document_embedding_l,\n",
    "#             num_filters=500,\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             name='document_conv1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "#             self.document_conv1_l,\n",
    "#             name='document_pool1',\n",
    "#             pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "#             mode='sum',\n",
    "#         )\n",
    "\n",
    "#         self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.document_max_l,\n",
    "#             num_units=250,\n",
    "#             name='doucment_dens1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         document_output_length = 200\n",
    "\n",
    "#         self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop1,\n",
    "#             num_units=225,\n",
    "#             name='document_dens2',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens2,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop2,\n",
    "#             num_units=document_output_length,\n",
    "#             name='document_dens3',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_output = lasagne.layers.get_output(self.document_dens3)\n",
    "\n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "\n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "\n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "\n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_aligned_l = InputLayer(\n",
    "#             (None, document_output_length),\n",
    "#             input_var=self.document_output[self.x_document_id,:]\n",
    "#         )\n",
    "\n",
    "        ##############################################\n",
    "        ## changed to not use the documented\n",
    "\n",
    "#         self.source_l = lasagne.layers.ConcatLayer(\n",
    "#             [self.document_aligned_l, self.surface_dens1]\n",
    "#         )\n",
    "\n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_dens1,   # CHANGED\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "\n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "\n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape(\n",
    "            (self.x_matches_surface.shape[0], 1, 1, 1)).astype(theano.config.floatX)\n",
    "\n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "\n",
    "        self.target_matched_surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None,1,1,1),\n",
    "            input_var=matched_surface_reshaped,\n",
    "        )\n",
    "        \n",
    "        self.target_matched_counts_input_l = lasagne.layers.InputLayer(\n",
    "            (None,5),\n",
    "            input_var=self.x_matches_counts.astype(theano.config.floatX),\n",
    "        )\n",
    "\n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W_docs,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_combined_feats_l = lasagne.layers.ConcatLayer(\n",
    "            [self.target_embedding_l, self.target_matched_surface_input_l,\n",
    "            lasagne.layers.reshape(self.target_matched_counts_input_l, ([0],1,1,[1]))],\n",
    "            axis=3\n",
    "        )\n",
    "\n",
    "        self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length),\n",
    "            input_var=self.x_target_words,\n",
    "        )\n",
    "\n",
    "        self.target_words_embedding_l = EmbeddingLayer(\n",
    "            self.target_words_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_words_embedding_l,\n",
    "            name='target_wrds_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=350,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.target_words_conv1_l,\n",
    "            name='target_wrds_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "            [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "             lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "        )\n",
    "\n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_merge_l,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_embedding_l, #self.target_combined_feats_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "\n",
    "\n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input],\n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "\n",
    "\n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "\n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "\n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "\n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "\n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "\n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "        \n",
    "        #############################\n",
    "        ## Linear features combined\n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        self.linear_features_combined = lasagne.layers.concat(\n",
    "            [lasagne.layers.InputLayer(\n",
    "                    (None, 1), \n",
    "                    input_var=self.res_l.reshape((self.res_l.shape[0], 1)),\n",
    "                ),\n",
    "            lasagne.layers.reshape(self.target_matched_surface_input_l, ([0],1)),\n",
    "            self.target_matched_counts_input_l],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.linear_features_dens_l = lasagne.layers.DenseLayer(\n",
    "            self.linear_features_combined,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "            num_units=1,\n",
    "            name='linear_final_l',\n",
    "            W=lasagne.init.Normal(mean=1.0),\n",
    "        )\n",
    "        \n",
    "        self.linear_output = lasagne.layers.get_output(\n",
    "            lasagne.layers.reshape(self.linear_features_dens_l, ([0],))\n",
    "        )\n",
    "        \n",
    "        ########################################\n",
    "        ## true output values\n",
    "        ########################################\n",
    "        \n",
    "        self.true_output = self.linear_output\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) *\n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "\n",
    "\n",
    "        #self.golds = self.res_cap[self.y_answer]\n",
    "\n",
    "#         def maxOverRange(indx):\n",
    "#             #return T.max(self.res_cap[T.arange(indx[0],indx[1])]) - self.res_cap[indx[2]]\n",
    "#             #return -( self.res_l[indx[2]] - T.log(T.exp(self.res_l[T.arange(indx[0],indx[1])]).sum()) )\n",
    "#             return -( self.res_l[indx[2]] - self.res_l[indx[0]])\n",
    "\n",
    "#         # build a tensor to make a matrix with one set on each dimention\n",
    "#         self.grouped, grouped_update = theano.scan(maxOverRange, sequences=self.y_grouping)\n",
    "\n",
    "        def setSubSelector(indx, outputs):\n",
    "            return T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "\n",
    "        num_target_samples = self.linear_output.shape[0]\n",
    "\n",
    "        select_seq = T.concatenate([\n",
    "            self.y_grouping,\n",
    "            T.arange(self.y_grouping.shape[0]).reshape((self.y_grouping.shape[0], 1))\n",
    "        ], axis=1)\n",
    "\n",
    "        self.selecting_matrix, _ = theano.scan(\n",
    "            setSubSelector,\n",
    "            outputs_info=T.zeros((num_target_samples, self.y_grouping.shape[0])), #num_target_samples)),\n",
    "            #n_steps=self.y_grouping.shape[0]\n",
    "            sequences=select_seq,\n",
    "        )\n",
    "\n",
    "#         self.groupped_elems = T.dot(self.selecting_matrix[-1], \n",
    "#                                     T.diag(T.exp(self.true_output)))\n",
    "#         self.groupped_res = T.log(self.groupped_elems.sum(axis=0)[T.arange(self.y_grouping.shape[0])])\n",
    "        self.groupped_elems = T.dot(self.selecting_matrix[-1].T, \n",
    "                                   T.exp(self.true_output))\n",
    "        self.groupped_res = T.log(self.groupped_elems)\n",
    "        \n",
    "        self.loss_vec = self.groupped_res - self.true_output[self.y_grouping[:,2]]\n",
    "\n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff,\n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.linear_features_dens_l)\n",
    "            #lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "\n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "\n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "\n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "\n",
    "        # this one works reasonably well\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "\n",
    "        #self.loss_vec = self.grouped\n",
    "\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "\n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "\n",
    "        self.func_inputs = [\n",
    "            #self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, #self.x_document_id,\n",
    "            self.x_target_input, self.x_matches_surface, self.x_matches_counts, self.x_link_id,\n",
    "            self.y_answer, self.y_grouping\n",
    "        ]  # self.x_target_words,\n",
    "\n",
    "        ################################################################3\n",
    "        ## TODO: need to return the actual output layer instead of the res_cap, since that is something else now\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.current_learning_groups = []\n",
    "        self.learning_targets = []\n",
    "        self.current_surface_target_counts = []\n",
    "        \n",
    "        self.failed_match = []\n",
    "            \n",
    "    def compute_batch(self, isTraining=True, useTrainingFunc=True):\n",
    "        if isTraining and useTrainingFunc:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "\n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "\n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] != isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                surcounts = self.surface_counts.get(surmatch)\n",
    "                if not surcounts:\n",
    "                    self.failed_match.append(surmatch)\n",
    "                    surcounts = {}\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_match_counts = []\n",
    "                target_gold_loc = -1\n",
    "                target_group_start = len(self.current_target_input)\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    wiki_title = WikiRegexes.convertToTitle(target)\n",
    "                    cnt = self.documentvecs.get_location(wiki_title)\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                    target_match_counts.append(surcounts.get(wiki_title, 0))\n",
    "                    #if wiki_title not in surcounts:\n",
    "                    #    print surcounts, wiki_title\n",
    "                if target_gold_loc is not None or not isTraining:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    gold_loc = (len(self.current_target_goal) + target_gold_loc)\n",
    "                    sorted_match_counts = [-4,-3,-2,-1] + sorted(set(target_match_counts))\n",
    "                    #print sorted_match_counts\n",
    "                    target_match_counts_indicators = [\n",
    "                        [\n",
    "                            int(s == sorted_match_counts[-1]),\n",
    "                            int(s == sorted_match_counts[-2]),\n",
    "                            int(s == sorted_match_counts[-3]),\n",
    "                            int(s <= sorted_match_counts[-4]),\n",
    "                            int(s != 0),  # if there is at least one occurance of this link with the title\n",
    "                        ]\n",
    "                        for s in target_match_counts\n",
    "                    ]\n",
    "                    self.current_target_goal += [gold_loc] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                    self.current_surface_target_counts += target_match_counts_indicators\n",
    "                    target_group_end = len(self.current_target_input)\n",
    "                    self.current_learning_groups.append(\n",
    "                        [target_group_start, target_group_end,\n",
    "                         gold_loc])\n",
    "\n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links\n",
    "\n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "\n",
    "        return self.total_loss / self.total_links\n",
    "\n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            #self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, #self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_surface_target_counts, self.current_target_id,\n",
    "            self.current_target_goal, self.current_learning_groups,\n",
    "            # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = float(res_vec[i])\n",
    "        self.reset_accums()\n",
    "\n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "\n",
    "\n",
    "\n",
    "queries_exp = EntityVectorLinkExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentState(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct = 0\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if all_trained > numSamples:\n",
    "                        break\n",
    "                    all_measured += 1\n",
    "                    all_trained += len(en['vals'].values())\n",
    "                    m = max(en['vals'].values())\n",
    "                    if en['vals'][en['gold']] == m and m != 0:\n",
    "                        all_correct += 1\n",
    "           \n",
    "    r = all_measured, float(all_correct) / all_measured\n",
    "    print r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def augmentTrainingData():\n",
    "    for quk in queries.keys():\n",
    "        qu = queries[quk]\n",
    "        for enk in qu.keys():\n",
    "            en = qu[enk]\n",
    "            if not en['gold']:\n",
    "                del qu[enk]\n",
    "        if not qu:\n",
    "            del queries[quk]\n",
    "    for qu in queries.values():\n",
    "        training = random.random() > .15\n",
    "        for en in qu.values():\n",
    "            en['training'] = training\n",
    "augmentTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWrongItems(trainingData=True, numSamples=50):\n",
    "    ret = {}\n",
    "    for qu in queries.values():\n",
    "        for ek, en in qu.items():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if len(ret) > numSamples:\n",
    "                        return ret\n",
    "                    m = max(en['vals'].values())\n",
    "                    g = en['vals'][en['gold']]\n",
    "                    if g != m and g != 0:\n",
    "                        ret[ek] = en\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073664766775\n",
      "0.0684425504571\n",
      "0.0650635290629\n",
      "0.06233600284\n",
      "0.0600716107196\n",
      "CPU times: user 1h 51min, sys: 33.7 s, total: 1h 51min 34s\n",
      "Wall time: 14min 44s\n"
     ]
    }
   ],
   "source": [
    "%time for i in range(5): print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068227344835976853"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.compute_batch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "    queries_exp.func_inputs,\n",
    "    [queries_exp.true_output, queries_exp.loss_vec.sum(), queries_exp.loss_vec,\n",
    "     queries_exp.groupped_elems, queries_exp.selecting_matrix[-1],\n",
    "     queries_exp.y_grouping, queries_exp.true_output,\n",
    "    queries_exp.groupped_res, queries_exp.true_output[queries_exp.y_grouping[:,2]]],\n",
    "    #updates=self.updates,\n",
    "    on_unused_input='ignore',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.99606383,  3.03962326,  2.04000115, ...,  2.01388669,\n",
       "         1.95188332,  1.95188332], dtype=float32),\n",
       " array(145.324462890625, dtype=float32),\n",
       " array([ 1.8338666 ,  1.17015028,  2.41720009,  1.04355526,  0.86929059,\n",
       "         2.06050396,  0.70096517,  1.57975674,  0.29270625,  1.89613867,\n",
       "         0.81760788,  0.23383307,  0.99139833,  1.89617682,  1.9674263 ,\n",
       "         2.08845329,  2.56513214,  2.70211649,  0.34872532,  1.05627298,\n",
       "         1.42656136,  0.22090387,  1.18886423,  1.24751329,  1.42778349,\n",
       "         1.53347254,  1.54588795,  1.37518692,  2.08785963,  2.76909685,\n",
       "         2.01372981,  2.73249102,  3.05195141,  0.76029658,  2.43372822,\n",
       "         3.68318796,  2.75000334,  1.53864098,  1.1992979 ,  2.32328844,\n",
       "         1.77645326,  2.90260577,  1.54462194,  1.6602416 ,  1.59336758,\n",
       "         2.06587172,  1.89035082,  1.86834908,  2.61736822,  1.97729802,\n",
       "         1.85287118,  1.03629279,  1.60164022,  2.85347557,  1.94306517,\n",
       "         1.77176523,  0.81754827,  1.6723547 ,  1.95836639,  2.037148  ,\n",
       "         1.53427219,  0.75274873,  0.12051058,  1.89456892,  1.99478054,\n",
       "         1.91349506,  2.73708868,  1.11680794,  1.98418379,  2.11609745,\n",
       "         1.90886927,  1.2775507 ,  1.40219021,  2.65423393,  2.0431366 ,\n",
       "         2.82525969,  1.19150305,  2.83177328,  0.85115957,  1.41004276,\n",
       "         2.72625852,  1.88119507,  2.87465358], dtype=float32),\n",
       " array([ 130.77653503,   68.3102951 ,   78.98326111,   63.17730713,\n",
       "          50.63472748,   62.87594604,   39.76198196,  103.29334259,\n",
       "          30.41285324,  146.12678528,   47.02298737,   26.98877144,\n",
       "          61.58735657,  133.53179932,  138.66409302,  155.44844055,\n",
       "         102.30792236,  114.91597748,   28.11203575,   62.17655945,\n",
       "          92.76219177,   27.8880024 ,   68.1267395 ,   67.87471771,\n",
       "          94.41375732,  100.34648132,  101.10882568,   81.74557495,\n",
       "          63.85404968,  117.53059387,  143.6159668 ,  119.69061279,\n",
       "         142.04333496,   46.2234993 ,   91.57016754,  116.8034668 ,\n",
       "         110.13952637,  102.67671204,   71.44850922,   79.38121033,\n",
       "          43.75356674,  132.84216309,  103.07411957,  112.20596313,\n",
       "         101.62519073,  142.72576904,  140.37480164,  135.11697388,\n",
       "         105.103302  ,  147.18206787,  117.58004761,   53.26086807,\n",
       "         111.55104828,  127.21865082,  129.03233337,  127.76239014,\n",
       "          16.75528908,  106.80071259,  154.57624817,  154.42497253,\n",
       "          94.95779419,   45.79096985,   23.64990807,  132.32698059,\n",
       "         149.88825989,  144.769104  ,  126.77859497,   66.18253326,\n",
       "         154.19033813,   69.27772522,  139.50401306,   76.41662598,\n",
       "          82.29498291,  102.99996948,  140.21884155,  125.36898804,\n",
       "          72.8309021 ,  133.67793274,   51.8575058 ,   88.3680954 ,\n",
       "         127.35874939,  141.07484436,  133.71560669], dtype=float32),\n",
       " array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.]], dtype=float32),\n",
       " array([[   0,   17,    1],\n",
       "        [  17,   35,   23],\n",
       "        [  35,   51,   43],\n",
       "        [  51,   65,   56],\n",
       "        [  65,   77,   71],\n",
       "        [  77,   91,   80],\n",
       "        [  91,   99,   98],\n",
       "        [  99,  117,  103],\n",
       "        [ 117,  121,  120],\n",
       "        [ 121,  140,  129],\n",
       "        [ 140,  144,  141],\n",
       "        [ 144,  147,  144],\n",
       "        [ 147,  160,  156],\n",
       "        [ 160,  178,  165],\n",
       "        [ 178,  196,  182],\n",
       "        [ 196,  215,  198],\n",
       "        [ 215,  230,  228],\n",
       "        [ 230,  246,  245],\n",
       "        [ 246,  250,  249],\n",
       "        [ 250,  266,  254],\n",
       "        [ 266,  285,  281],\n",
       "        [ 285,  288,  285],\n",
       "        [ 288,  305,  288],\n",
       "        [ 305,  322,  307],\n",
       "        [ 322,  341,  338],\n",
       "        [ 341,  357,  351],\n",
       "        [ 357,  373,  370],\n",
       "        [ 373,  389,  374],\n",
       "        [ 389,  404,  392],\n",
       "        [ 404,  421,  411],\n",
       "        [ 421,  440,  426],\n",
       "        [ 440,  459,  455],\n",
       "        [ 459,  477,  463],\n",
       "        [ 477,  487,  484],\n",
       "        [ 487,  505,  502],\n",
       "        [ 505,  524,  505],\n",
       "        [ 524,  543,  533],\n",
       "        [ 543,  559,  543],\n",
       "        [ 559,  578,  571],\n",
       "        [ 578,  596,  595],\n",
       "        [ 596,  610,  597],\n",
       "        [ 610,  629,  616],\n",
       "        [ 629,  647,  644],\n",
       "        [ 647,  664,  655],\n",
       "        [ 664,  681,  670],\n",
       "        [ 681,  700,  682],\n",
       "        [ 700,  718,  715],\n",
       "        [ 718,  736,  723],\n",
       "        [ 736,  755,  751],\n",
       "        [ 755,  774,  755],\n",
       "        [ 774,  793,  779],\n",
       "        [ 793,  807,  798],\n",
       "        [ 807,  825,  814],\n",
       "        [ 825,  841,  831],\n",
       "        [ 841,  858,  846],\n",
       "        [ 858,  877,  870],\n",
       "        [ 877,  883,  876],\n",
       "        [ 883,  902,  891],\n",
       "        [ 902,  921,  906],\n",
       "        [ 921,  940,  938],\n",
       "        [ 940,  954,  952],\n",
       "        [ 954,  957,  954],\n",
       "        [ 957,  959,  957],\n",
       "        [ 959,  977,  972],\n",
       "        [ 977,  995,  977],\n",
       "        [ 995, 1014, 1008],\n",
       "        [1014, 1033, 1019],\n",
       "        [1033, 1050, 1043],\n",
       "        [1050, 1069, 1062],\n",
       "        [1069, 1085, 1080],\n",
       "        [1085, 1103, 1102],\n",
       "        [1103, 1121, 1120],\n",
       "        [1121, 1138, 1137],\n",
       "        [1138, 1156, 1142],\n",
       "        [1156, 1175, 1173],\n",
       "        [1175, 1194, 1178],\n",
       "        [1194, 1202, 1200],\n",
       "        [1202, 1221, 1203],\n",
       "        [1221, 1233, 1222],\n",
       "        [1233, 1250, 1237],\n",
       "        [1250, 1268, 1250],\n",
       "        [1268, 1286, 1285],\n",
       "        [1286, 1305, 1288]], dtype=int32),\n",
       " array([ 1.99606383,  3.03962326,  2.04000115, ...,  2.01388669,\n",
       "         1.95188332,  1.95188332], dtype=float32),\n",
       " array([ 4.87348986,  4.22406054,  4.36923599,  4.14594507,  3.92463756,\n",
       "         4.14116383,  3.68291116,  4.63757277,  3.41486526,  4.98447466,\n",
       "         3.85063648,  3.29542089,  4.1204567 ,  4.89433956,  4.93205452,\n",
       "         5.04631424,  4.62798691,  4.74420118,  3.33619785,  4.12997818,\n",
       "         4.53003931,  3.32819653,  4.22136974,  4.21766376,  4.54768658,\n",
       "         4.60862923,  4.61619759,  4.40361166,  4.1566    ,  4.76669884,\n",
       "         4.96714306,  4.7849102 ,  4.95613241,  3.83348823,  4.51710558,\n",
       "         4.7604928 ,  4.70174789,  4.63158512,  4.26897717,  4.37426186,\n",
       "         3.77857304,  4.88916159,  4.63544846,  4.72033596,  4.62129164,\n",
       "         4.9609251 ,  4.94431591,  4.9061408 ,  4.65494347,  4.99167061,\n",
       "         4.76711941,  3.97520185,  4.71448231,  4.84590721,  4.86006308,\n",
       "         4.85017204,  2.8187139 ,  4.67096472,  5.04068756,  5.03970814,\n",
       "         4.55343246,  3.8240869 ,  3.16335917,  4.88527584,  5.00989008,\n",
       "         4.97514009,  4.84244204,  4.19241667,  5.03818798,  4.23812342,\n",
       "         4.93809319,  4.33620024,  4.41031027,  4.63472891,  4.9432044 ,\n",
       "         4.83126116,  4.2881403 ,  4.89543343,  3.94849968,  4.48151112,\n",
       "         4.84700775,  4.94929075,  4.89571524], dtype=float32),\n",
       " array([ 3.03962326,  3.05391026,  1.9520359 ,  3.10238981,  3.05534697,\n",
       "         2.08065987,  2.98194599,  3.05781603,  3.122159  ,  3.08833599,\n",
       "         3.0330286 ,  3.06158781,  3.12905836,  2.99816275,  2.96462822,\n",
       "         2.95786095,  2.06285477,  2.04208469,  2.98747253,  3.0737052 ,\n",
       "         3.10347795,  3.10729265,  3.03250551,  2.97015047,  3.11990309,\n",
       "         3.07515669,  3.07030964,  3.02842474,  2.06874037,  1.99760211,\n",
       "         2.95341325,  2.05241919,  1.90418088,  3.07319164,  2.08337736,\n",
       "         1.07730496,  1.95174456,  3.09294415,  3.06967926,  2.05097342,\n",
       "         2.00211978,  1.98655581,  3.09082651,  3.06009436,  3.02792406,\n",
       "         2.89505339,  3.05396509,  3.03779173,  2.03757524,  3.01437259,\n",
       "         2.91424823,  2.93890905,  3.11284208,  1.99243152,  2.91699791,\n",
       "         3.07840681,  2.00116563,  2.99861002,  3.08232117,  3.00256014,\n",
       "         3.01916027,  3.07133818,  3.04284859,  2.99070692,  3.01510954,\n",
       "         3.06164503,  2.10535336,  3.07560873,  3.05400419,  2.12202597,\n",
       "         3.02922392,  3.05864954,  3.00812006,  1.98049498,  2.90006781,\n",
       "         2.00600147,  3.09663725,  2.06366014,  3.09734011,  3.07146835,\n",
       "         2.12074924,  3.06809568,  2.02106166], dtype=float32)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_gg(self):\n",
    "    return gg_func(\n",
    "        #self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, #self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_surface_target_counts, self.current_target_id,\n",
    "            self.current_target_goal, self.current_learning_groups,\n",
    "            # self.current_target_words,\n",
    "    )\n",
    "\n",
    "gg_res = run_gg(queries_exp)\n",
    "\n",
    "gg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(gg_res[-2:][0] - gg_res[-2:][1] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([140, 144, 141], dtype=int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_res[-4][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9241138"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_res[-2:][1][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10]),)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.exp(gg_res[-3]) == 2.97148943)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_res[-5][:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.97148943,  2.97148943,  2.97148943,  2.97148943], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_res[-6][:,10][140:144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([140, 141, 142, 143]),)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(gg_res[-5][:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.08906329,  1.08906329,  1.08906329,  1.08906329], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(gg_res[-5], np.diag(gg_res[-3]))[:,10][140:144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.dot(gg_res[-5].T, gg_res[-3]) != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.189476"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_res[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, -0.08257215597738024)\n",
      "(1, -0.11069699509292312)\n",
      "(2, -0.12883317005099618)\n",
      "(271, 0.7564575645756457)\n",
      "(268, 0.7761194029850746)\n"
     ]
    }
   ],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(3):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.078538599043427942),\n",
       " (1, 0.013858774709251018),\n",
       " (2, -0.07343112753042555),\n",
       " (3, -0.18453017205599387),\n",
       " (4, -0.314641321710036),\n",
       " (5, -0.46106892619986317),\n",
       " (6, -0.62041261969878891),\n",
       " (7, -0.78368106992379538),\n",
       " ('testing run', -0.53842870805970999),\n",
       " ('training state', (2914, 0.5748112560054908)),\n",
       " ('testing state', (2765, 0.49330922242314645))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(4):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[linear_final_l.W, linear_final_l.b]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.81728172],\n",
       "       [  2.58984065],\n",
       "       [  2.68953562],\n",
       "       [  0.09458681],\n",
       "       [  0.50670612],\n",
       "       [ -0.62219417],\n",
       "       [  2.4768362 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for i in range(5): print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
