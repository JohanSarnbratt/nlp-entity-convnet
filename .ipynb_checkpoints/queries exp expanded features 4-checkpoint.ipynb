{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Working on adding a lot of features just to see if it can get the score up regardless of how complicated or where the data is coming from\n",
    "\n",
    "Features that I will be adding\n",
    "\n",
    "* Taget given surface counts\n",
    "* words from the target and source document\n",
    "  * possible back prop into these vectors, idea is to replace tf-idf with some nn and back prop here\n",
    "* using a linear layer near the output to combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer, WordTokenizer\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links7-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirect7.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/enwiki-20141208-pages-articles-multistream-surface-counts7.json') as f:\n",
    "    surface_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and make the surfaces items match what we are looking for\n",
    "surface_counts_re = re.compile('([\\.,!\\?])')\n",
    "for sk in surface_counts.keys():\n",
    "    nsk = sk.replace('(', '-lrb-').replace(')', '-rrb-')\n",
    "    nsk = surface_counts_re.sub(' \\\\1', nsk)\n",
    "    if nsk != sk:\n",
    "        surface_counts[nsk] = surface_counts[sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, vectors=wordvectors, queries=queries, redirects=page_redirects, surface=surface_counts):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    wordvec = WordTokenizer(vectors, sentence_length=200)\n",
    "    documentvec = WordTokenizer(vectors, sentence_length=1)\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                documentvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            documentvec.get_location(redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content, namespace):\n",
    "#             if namespace != 0:\n",
    "#                 return\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "    rs = surface\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        documentvecs = documentvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        surface_counts = rs\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 91.7 ms, total: 1min 46s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%time basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for qu in queries.values():\n",
    "    for en in qu.values():\n",
    "        en['boosted'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanUpMultipleLinks():\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            gold_page = en['gold']\n",
    "            gold_title = WikiRegexes.convertToTitle(gold_page)\n",
    "            gold_title = page_redirects.get(gold_title, gold_title)\n",
    "            pages = set()\n",
    "            for p in en['vals'].keys():\n",
    "                wiki_title = WikiRegexes.convertToTitle(p)\n",
    "                wiki_title = page_redirects.get(wiki_title, wiki_title)\n",
    "                if wiki_title == gold_title and p != en['gold']:\n",
    "                    del en['vals'][p]\n",
    "                elif wiki_title not in pages:\n",
    "                    pages.add(wiki_title)\n",
    "                else:\n",
    "                    del en['vals'][p]\n",
    "cleanUpMultipleLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34863863"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeSingleLinkTargets():\n",
    "    \"items that the surface link set only has a single item are trival, so remove\"\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    for quk, qu in queries.items():\n",
    "        for sur in qu.keys():\n",
    "            surlink = get_link.match(sur).group(1)\n",
    "            surmatch = surlink.lower()\n",
    "            surcounts = surface_counts.get(surmatch)\n",
    "            if surcounts and len(surcounts) == 1:\n",
    "                # this is trival since there is only one item\n",
    "                del qu[sur]\n",
    "        if not qu:\n",
    "            # we removed all the links on this page, remove it otherwise the program crashes\n",
    "            del queries[quk]\n",
    "removeSingleLinkTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34793912"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "\n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        \n",
    "        self.num_words_to_use_conv = 5\n",
    "        self.enable_boosting = False\n",
    "        self.num_negative_target_samples = 1\n",
    "        #self.enable_match_surface = False\n",
    "        #self.enable_link_counts = True\n",
    "        self.enable_train_wordvecs = False\n",
    "        self.enable_cap_boosting = True\n",
    "\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_doc')\n",
    "\n",
    "        self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "\n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_matches_counts = T.imatrix('x_matches_counts')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "\n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        self.y_grouping = T.imatrix('y_grouping')\n",
    "        self.y_boosted = T.vector('y_boosted')\n",
    "\n",
    "\n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX),name='embedding_W')\n",
    "        self.embedding_W_docs = theano.shared(self.documentvecs.get_numpy_matrix().astype(theano.config.floatX),name='embedding_W_docs')\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer(\n",
    "            (None,self.document_length),\n",
    "            input_var=self.x_document_input\n",
    "        )\n",
    "\n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=500,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        document_output_length = 200\n",
    "        \n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=document_output_length,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop1,\n",
    "#             num_units=225,\n",
    "#             name='document_dens2',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens2,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop2,\n",
    "#             num_units=document_output_length,\n",
    "#             name='document_dens3',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens1)\n",
    "    \n",
    "    \n",
    "        ##########################################\n",
    "        ## surface text\n",
    "\n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "\n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "\n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "\n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "        ##################################################\n",
    "        ## merge the documents with the surface info\n",
    "\n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "\n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens1]\n",
    "        )\n",
    "\n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "\n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "\n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape(\n",
    "            (self.x_matches_surface.shape[0], 1, 1, 1)).astype(theano.config.floatX)\n",
    "\n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "\n",
    "        self.target_matched_surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None,1,1,1),\n",
    "            input_var=matched_surface_reshaped,\n",
    "        )\n",
    "        \n",
    "        self.target_matched_counts_input_l = lasagne.layers.InputLayer(\n",
    "            (None,5),\n",
    "            input_var=self.x_matches_counts.astype(theano.config.floatX),\n",
    "        )\n",
    "\n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W_docs,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_combined_feats_l = lasagne.layers.ConcatLayer(\n",
    "            [self.target_embedding_l, self.target_matched_surface_input_l,\n",
    "            lasagne.layers.reshape(self.target_matched_counts_input_l, ([0],1,1,[1]))],\n",
    "            axis=3\n",
    "        )\n",
    "\n",
    "#         self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "#             (None,self.sentence_length),\n",
    "#             input_var=self.x_target_words,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_embedding_l = EmbeddingLayer(\n",
    "#             self.target_words_input_l,\n",
    "#             W=self.embedding_W,\n",
    "#             add_word_params=False,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.target_words_embedding_l,\n",
    "#             name='target_wrds_conv1',\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             num_filters=350,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "#             self.target_words_conv1_l,\n",
    "#             name='target_wrds_pool1',\n",
    "#             pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "#             mode='sum',\n",
    "#         )\n",
    "\n",
    "#         self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "#             [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "#              lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "#         )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_merge_l,\n",
    "#             name='target_wrds_dens1',\n",
    "#             num_units=400,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_wrds_dens1',\n",
    "#             num_units=compared_vector_size,\n",
    "#             nonlinearity=lasagne.nonlinearities.linear,\n",
    "#         )\n",
    "\n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_embedding_l, #self.target_combined_feats_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "\n",
    "\n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input],\n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "\n",
    "\n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "\n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "\n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "\n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "\n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "\n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "        \n",
    "        #############################\n",
    "        ## Linear features combined\n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        self.linear_features_combined = lasagne.layers.concat(\n",
    "            [lasagne.layers.InputLayer(\n",
    "                    (None, 1), \n",
    "                    input_var=self.res_l.reshape((self.res_l.shape[0], 1)),\n",
    "                ),\n",
    "            lasagne.layers.reshape(self.target_matched_surface_input_l, ([0],1)),\n",
    "            self.target_matched_counts_input_l],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.linear_features_dens_l = lasagne.layers.DenseLayer(\n",
    "            self.linear_features_combined,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "            num_units=1,\n",
    "            name='linear_final_l',\n",
    "            W=lasagne.init.Normal(mean=1.0),\n",
    "        )\n",
    "        \n",
    "        self.linear_output = lasagne.layers.get_output(\n",
    "            lasagne.layers.reshape(self.linear_features_dens_l, ([0],))\n",
    "        )\n",
    "        \n",
    "        ########################################\n",
    "        ## true output values\n",
    "        ########################################\n",
    "        \n",
    "        self.true_output = self.linear_output\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) *\n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "\n",
    "\n",
    "        #self.golds = self.res_cap[self.y_answer]\n",
    "\n",
    "#         def maxOverRange(indx):\n",
    "#             #return T.max(self.res_cap[T.arange(indx[0],indx[1])]) - self.res_cap[indx[2]]\n",
    "#             #return -( self.res_l[indx[2]] - T.log(T.exp(self.res_l[T.arange(indx[0],indx[1])]).sum()) )\n",
    "#             return -( self.res_l[indx[2]] - self.res_l[indx[0]])\n",
    "\n",
    "#         # build a tensor to make a matrix with one set on each dimention\n",
    "#         self.grouped, grouped_update = theano.scan(maxOverRange, sequences=self.y_grouping)\n",
    "\n",
    "        def setSubSelector(indx, outputs):\n",
    "            return T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "\n",
    "        num_target_samples = self.linear_output.shape[0]\n",
    "\n",
    "        select_seq = T.concatenate([\n",
    "            self.y_grouping,\n",
    "            T.arange(self.y_grouping.shape[0]).reshape((self.y_grouping.shape[0], 1))\n",
    "        ], axis=1)\n",
    "\n",
    "        self.selecting_matrix, _ = theano.scan(\n",
    "            setSubSelector,\n",
    "            outputs_info=T.zeros((num_target_samples, self.y_grouping.shape[0])), #num_target_samples)),\n",
    "            #n_steps=self.y_grouping.shape[0]\n",
    "            sequences=select_seq,\n",
    "        )\n",
    "\n",
    "#         self.groupped_elems = T.dot(self.selecting_matrix[-1], \n",
    "#                                     T.diag(T.exp(self.true_output)))\n",
    "#         self.groupped_res = T.log(self.groupped_elems.sum(axis=0)[T.arange(self.y_grouping.shape[0])])\n",
    "        self.groupped_elems = T.dot(self.selecting_matrix[-1].T, \n",
    "                                   T.exp(self.true_output))\n",
    "        self.groupped_res = T.log(self.groupped_elems)\n",
    "        \n",
    "        self.loss_vec = self.groupped_res - self.true_output[self.y_grouping[:,2]]\n",
    "        \n",
    "        if self.enable_boosting:\n",
    "            self.loss_scalar = T.dot(self.y_boosted, self.loss_vec)\n",
    "        else:\n",
    "            self.loss_scalar = self.loss_vec.sum()\n",
    "\n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff,\n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.linear_features_dens_l)\n",
    "            #lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "\n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "\n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "\n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "\n",
    "        # this one works reasonably well\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "\n",
    "        #self.loss_vec = self.grouped\n",
    "\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "\n",
    "        self.updates = lasagne.updates.adadelta(self.loss_scalar / self.loss_vec.shape[0], self.all_params)\n",
    "\n",
    "        self.func_inputs = [\n",
    "            self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, self.x_document_id,\n",
    "            self.x_target_input, self.x_matches_surface, self.x_matches_counts, self.x_link_id,\n",
    "            self.y_answer, self.y_grouping, self.y_boosted\n",
    "        ]  # self.x_target_words,\n",
    "\n",
    "        ################################################################3\n",
    "        ## TODO: need to return the actual output layer instead of the res_cap, since that is something else now\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_scalar, self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_scalar, self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.current_learning_groups = []\n",
    "        self.learning_targets = []\n",
    "        self.current_surface_target_counts = []\n",
    "        self.current_boosted_groups = []\n",
    "        \n",
    "        self.failed_match = []\n",
    "            \n",
    "    def compute_batch(self, isTraining=True, useTrainingFunc=True):\n",
    "        if isTraining and useTrainingFunc:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        self.total_boosted_loss = 0.0\n",
    "\n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "\n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] != isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                surcounts = self.surface_counts.get(surmatch)\n",
    "                if not surcounts:\n",
    "                    self.failed_match.append(surmatch)\n",
    "                    surcounts = {}\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_match_counts = []\n",
    "                target_gold_loc = -1\n",
    "                target_group_start = len(self.current_target_input)\n",
    "                \n",
    "#                 pages = []\n",
    "#                 pages_info_link = [] \n",
    "#                 gold = None\n",
    "#                 # skip the items that we don't know the gold for\n",
    "#                 if not targets['gold'] and isTraining:\n",
    "#                     continue\n",
    "#                 for target in set(targets['vals'].keys() +\n",
    "#                                  random.sample(self.documentvecs.reverse_word_location, self.num_negative_target_samples)\n",
    "#                                   ) - {None,}:\n",
    "#                     isGold = target == targets['gold']\n",
    "#                     wiki_title = WikiRegexes.convertToTitle(target)\n",
    "#                     cnt = self.documentvecs.get_location(wiki_title)\n",
    "#                     if wiki_title == 'nil':\n",
    "#                         cnt = 0  # this is the stop symbol location\n",
    "#                     if cnt is None:\n",
    "#                         # were not able to find this wikipedia document\n",
    "#                         # so just ignore tihs result since trying to train on it will cause\n",
    "#                         # issues\n",
    "#                         continue\n",
    "#                     if isGold:\n",
    "#                         gold = cnt\n",
    "#                     if cnt not in pages:\n",
    "#                         pages.append(cnt)\n",
    "#                         pages_info_link.append((targets, target))\n",
    "#                     elif isGold:\n",
    "#                         # there are two links to the same page somehow\n",
    "#                         # and we already have something that isn't linking to the gold\n",
    "#                         for i in xrange(len(pages)):\n",
    "#                             if pages[i] == cnt:\n",
    "#                                 # this links to the same page as the gold\n",
    "#                                 pages_info_link[i] = (targets, target)\n",
    "#                 for i in xrange(len(pages)):\n",
    "                    \n",
    "                    \n",
    "                for target in set(targets['vals'].keys() +\n",
    "                                 random.sample(self.documentvecs.reverse_word_location, self.num_negative_target_samples)\n",
    "                                  ) - {None,}:\n",
    "                    \n",
    "                    \n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    wiki_title = WikiRegexes.convertToTitle(target)\n",
    "                    cnt = self.documentvecs.get_location(wiki_title)\n",
    "                    if wiki_title == 'nil':\n",
    "                        cnt = 0  # this is the stop symbol location\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                    target_match_counts.append(surcounts.get(wiki_title, 0))\n",
    "                    #if wiki_title not in surcounts:\n",
    "                    #    print surcounts, wiki_title\n",
    "                if target_gold_loc is not None or not isTraining:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    gold_loc = (len(self.current_target_goal) + target_gold_loc)\n",
    "                    sorted_match_counts = [-4,-3,-2,-1] + sorted(set(target_match_counts))\n",
    "                    #print sorted_match_counts\n",
    "                    target_match_counts_indicators = [\n",
    "                        [\n",
    "                            int(s == sorted_match_counts[-1]),\n",
    "                            int(s == sorted_match_counts[-2]),\n",
    "                            int(s == sorted_match_counts[-3]),\n",
    "                            int(0 < s <= sorted_match_counts[-4]),\n",
    "                            int(s == 0),\n",
    "                        ]\n",
    "                        for s in target_match_counts\n",
    "                    ]\n",
    "                    self.current_target_goal += [gold_loc] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                    self.current_surface_target_counts += target_match_counts_indicators\n",
    "                    target_group_end = len(self.current_target_input)\n",
    "                    self.current_learning_groups.append(\n",
    "                        [target_group_start, target_group_end,\n",
    "                         gold_loc])\n",
    "                    self.current_boosted_groups.append(targets['boosted'])\n",
    "\n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links, self.total_boosted_loss / self.total_links\n",
    "\n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "\n",
    "        return self.total_loss / self.total_links, self.total_boosted_loss / self.total_links\n",
    "\n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_boosted, loss_vec = func(\n",
    "            self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_surface_target_counts, self.current_target_id,\n",
    "            self.current_target_goal, self.current_learning_groups, self.current_boosted_groups,\n",
    "            # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        self.total_boosted_loss += loss_boosted\n",
    "        learned_groups = []  # right...dict not hashable....\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            if l[1] in l[0]['vals']:\n",
    "                l[0]['vals'][ l[1] ] = float(res_vec[i])\n",
    "            if l[0] not in learned_groups:\n",
    "                learned_groups.append(l[0])\n",
    "        for group in learned_groups:\n",
    "            if group['gold']:\n",
    "                correct = max(group['vals']) == group['vals'][group['gold']]\n",
    "                group['boosted'] *= .4 if correct else 2.0\n",
    "                if self.enable_cap_boosting:\n",
    "                    if group['boosted'] > 10:\n",
    "                        group['boosted'] = 10.0\n",
    "                    elif group['boosted'] < 0.1:\n",
    "                        group['boosted'] = 0.1\n",
    "        self.reset_accums()\n",
    "\n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "\n",
    "\n",
    "\n",
    "queries_exp = EntityVectorLinkExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentState(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct = 0\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            if en['gold']:\n",
    "                if all_trained > numSamples:\n",
    "                    break\n",
    "                all_measured += 1\n",
    "                all_trained += len(en['vals'].values())\n",
    "                m = max(en['vals'].values())\n",
    "                if en['vals'][en['gold']] == m and m != 0:\n",
    "                    all_correct += 1\n",
    "           \n",
    "    r = all_measured, float(all_correct) / all_measured\n",
    "    print r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentStateRank(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct_place = 0\n",
    "    p_counts = dict((k,0) for k in range(0,10))\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            if en['gold']:\n",
    "                if all_trained > numSamples:\n",
    "                    break\n",
    "                svals = sorted(en['vals'].values(), key=lambda x: -x)\n",
    "                gv = en['vals'][en['gold']]\n",
    "                if gv == 0:\n",
    "                    continue\n",
    "                all_measured += 1\n",
    "                for i in xrange(len(svals)):\n",
    "                    if svals[i] == gv:\n",
    "                        if i < 10:\n",
    "                            p_counts[i] += 1\n",
    "                        all_correct_place += i + 1\n",
    "                        break\n",
    "\n",
    "    r = all_measured, float(all_correct_place) / all_measured, p_counts\n",
    "    print r\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def augmentTrainingData():\n",
    "    for quk in queries.keys():\n",
    "        qu = queries[quk]\n",
    "        for enk in qu.keys():\n",
    "            en = qu[enk]\n",
    "            if not en['gold']:\n",
    "                del qu[enk]\n",
    "        if not qu:\n",
    "            del queries[quk]\n",
    "    for qu in queries.values():\n",
    "        training = random.random() > .15\n",
    "        for en in qu.values():\n",
    "            en['training'] = training\n",
    "augmentTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWrongItems(trainingData=True, numSamples=50):\n",
    "    ret = {}\n",
    "    for qu in queries.values():\n",
    "        for ek, en in qu.items():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if len(ret) > numSamples:\n",
    "                        return ret\n",
    "                    m = max(en['vals'].values())\n",
    "                    g = en['vals'][en['gold']]\n",
    "                    if g != m and g != 0:\n",
    "                        ret[ek] = en\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (0.13592339395444425, 0.13592339395444425))\n",
      "(1, (0.10256952598076567, 0.10256952598076567))\n",
      "(2, (0.084489537202176362, 0.084489537202176362))\n",
      "(3, (0.073435652840361937, 0.073435652840361937))\n",
      "(4, (0.066023715297971436, 0.066023715297971436))\n",
      "(3162, 0.7795698924731183)\n",
      "(3207, 0.7605238540692236)\n",
      "(5, (0.059168833872545908, 0.059168833872545908))\n",
      "(6, (0.054642100344395976, 0.054642100344395976))\n",
      "(7, (0.050485458860445073, 0.050485458860445073))\n",
      "(8, (0.04787478579930849, 0.04787478579930849))\n",
      "(9, (0.044848431546077969, 0.044848431546077969))\n",
      "(3162, 0.8260594560404807)\n",
      "(3207, 0.7789211100717182)\n"
     ]
    }
   ],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(10):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "    if i % 5 == 4:\n",
    "        exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "        exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "        exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 50000\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0.13592339395444425, 0.13592339395444425)),\n",
       " (1, (0.10256952598076567, 0.10256952598076567)),\n",
       " (2, (0.084489537202176362, 0.084489537202176362)),\n",
       " (3, (0.073435652840361937, 0.073435652840361937)),\n",
       " (4, (0.066023715297971436, 0.066023715297971436)),\n",
       " ('testing run', (0.070020115041002931, 0.070020115041002931)),\n",
       " ('training state', (3162, 0.7795698924731183)),\n",
       " ('testing state', (3207, 0.7605238540692236)),\n",
       " (5, (0.059168833872545908, 0.059168833872545908)),\n",
       " (6, (0.054642100344395976, 0.054642100344395976)),\n",
       " (7, (0.050485458860445073, 0.050485458860445073)),\n",
       " (8, (0.04787478579930849, 0.04787478579930849)),\n",
       " (9, (0.044848431546077969, 0.044848431546077969)),\n",
       " ('testing run', (0.058955029261355499, 0.058955029261355499)),\n",
       " ('training state', (3162, 0.8260594560404807)),\n",
       " ('testing state', (3207, 0.7789211100717182))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surface_counts['The Silent World of Nicholas Quinn'.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surface_counts['canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surface_counts['urban district']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u\"'' mdash ; for his heroism in April 1970 in [Vietnam] .\": {'boosted': 10.0,\n",
       "  u'gold': u'Vietnam War',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.5341694355010986,\n",
       "   u'Buddhism in Vietnam': 0.647974967956543,\n",
       "   u'Cinema of Vietnam': -0.2520301640033722,\n",
       "   u'French Indochina': 1.5234445333480835,\n",
       "   u'North Vietnam': 2.184170961380005,\n",
       "   u'South Vietnam': 2.576929807662964,\n",
       "   u'Vietnam': 5.990146160125732,\n",
       "   u'Vietnam (miniseries)': -1.3929743766784668,\n",
       "   u'Vietnam Football Federation': -1.2757927179336548,\n",
       "   u'Vietnam War': 2.986530303955078,\n",
       "   u'Vietnam at the 2005 Southeast Asian Games': -1.936440348625183,\n",
       "   u'Vietnam at the 2007 Southeast Asian Games': 0,\n",
       "   u'Vietnam at the 2009 Southeast Asian Games': 0,\n",
       "   u'Vietnam national football team': -1.3600518703460693,\n",
       "   u\"Vietnam women's national football team\": -1.9970697164535522,\n",
       "   u\"Vietnam women's national volleyball team\": -1.860672116279602,\n",
       "   u'Vietnamese cuisine': -1.2969914674758911,\n",
       "   u'Volleyball Federation of Vietnam': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"'' ndash ; April 6 , 1528 -RRB- was a [German] painter , printmaker and theorist from Nuremberg , Germany .\": {'boosted': 10.0,\n",
       "  u'gold': u'Germans',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.9268836975097656,\n",
       "   u'Cinema of Germany': 0.4210917055606842,\n",
       "   u'German': 2.1175951957702637,\n",
       "   u'German American': 1.418041467666626,\n",
       "   u'German Empire': 0.9330065846443176,\n",
       "   u'German language': 2.2741494178771973,\n",
       "   u'German poetry': 1.0119003057479858,\n",
       "   u'Germans': 1.9114128351211548,\n",
       "   u'Germany': 4.325403213500977,\n",
       "   u'GfK Entertainment': -0.7822624444961548,\n",
       "   u'Kriegsmarine': -0.6427406072616577,\n",
       "   u'Nazi Germany': 1.9116019010543823,\n",
       "   u'Wehrmacht': 0.052445389330387115,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"'' public house and restaurant is named after the seventeenth-century [fisherman] whose book '' The Compleat Angler '' is still in\": {'boosted': 10.0,\n",
       "  u'gold': u'Izaak Walton',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.078784704208374,\n",
       "   u'Bass fishing': -0.42926517128944397,\n",
       "   u'Casio_G-Shock_Fisherman': 0,\n",
       "   u'Commercial fishing': 1.3462672233581543,\n",
       "   u'Fisherman (English horse)': 0.0681215450167656,\n",
       "   u'Fisherman (Pre\\u0161eren)': 0,\n",
       "   u'Fisherman (comics)': -0.42010608315467834,\n",
       "   u'Fisherman (horse)': -1.5308661460876465,\n",
       "   u\"Fisherman's staysail\": 0,\n",
       "   u'Fishing': 1.9957301616668701,\n",
       "   u'Heart of the Congos': -0.8963301181793213,\n",
       "   u'Izaak Walton': 2.267693519592285,\n",
       "   u'Recreational fishing': 0.9085744023323059,\n",
       "   u'XXNILXX': 0,\n",
       "   u'fisherman': 5.136682510375977,\n",
       "   u'fishing industry': 1.0522915124893188}},\n",
       " u'1455 had moved to Nuremberg from Ajt s , near [Gyula] in Hungary .': {'boosted': 10.0,\n",
       "  u'gold': u'Gyula, Hungary',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.724822759628296,\n",
       "   u'Diocese of Gyula': 0,\n",
       "   u'Gyula': 2.804126024246216,\n",
       "   u'Gyula (name)': 0,\n",
       "   u'Gyula (telep\\xfcl\\xe9s)': 0,\n",
       "   u'Gyula (title)': 1.5729230642318726,\n",
       "   u'Gyula (v\\xe1ros)': 0,\n",
       "   u'Gyula Andr\\xe1ssy': 1.0717618465423584,\n",
       "   u'Gyula Andr\\xe1ssy the Younger': -0.5940715074539185,\n",
       "   u'Gyula Erkel': 0,\n",
       "   u'Gyula II': 1.1720919609069824,\n",
       "   u'Gyula III': -1.2457196712493896,\n",
       "   u'Gyula Kert\\xe9sz': 0,\n",
       "   u'Gyula and Hungary': 0,\n",
       "   u'Gyula(town)': 0,\n",
       "   u'Gyula, Hungary': 1.4203840494155884,\n",
       "   u'Julius I K\\xe1n': -1.0096372365951538,\n",
       "   u'Siege of Gyula': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u': H7 illness in United States history occurred at the [Washington County] , New York fair .': {'boosted': 10.0,\n",
       "  u'gold': u'Washington County, New York',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.5511035919189453,\n",
       "   u'Counties of Estonia': -1.5726171731948853,\n",
       "   u'Counties of Hungary': -1.635637640953064,\n",
       "   u'Counties of Iran': -1.3120254278182983,\n",
       "   u'Counties of Ireland': -2.4994795322418213,\n",
       "   u'Counties of Norway': -0.5215862989425659,\n",
       "   u'Counties of Romania': -1.5588473081588745,\n",
       "   u'Counties of Sweden': -1.4743932485580444,\n",
       "   u\"Counties of the People's Republic of China\": -1.6462067365646362,\n",
       "   u'Powiat': -1.5784600973129272,\n",
       "   u'Washington County, Alabama': 0.2874391973018646,\n",
       "   u'Washington County, Arkansas': 0.8089756369590759,\n",
       "   u'Washington County, Maine': 0.592526912689209,\n",
       "   u'Washington County, Maryland': 0.20426420867443085,\n",
       "   u'Washington County, New York': 1.86818265914917,\n",
       "   u'Washington County, Ohio': 0.45760422945022583,\n",
       "   u'Washington County, Oregon': 1.5949000120162964,\n",
       "   u'Washington County, Pennsylvania': 3.276883840560913,\n",
       "   u'Washington County, Vermont': 0.659368634223938,\n",
       "   u'XXNILXX': 0}},\n",
       " u': Most of the emigrates to America belonged to the [Greek Catholic Church] .': {'boosted': 10.0,\n",
       "  u'gold': u'Greek Catholic Church',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.3509066104888916,\n",
       "   u'Catholic Church in the United States': -0.5683975219726562,\n",
       "   u'Christian Church': 1.1244653463363647,\n",
       "   u'Church': 2.4276273250579834,\n",
       "   u'Church (building)': 1.7350221872329712,\n",
       "   u'Greek Catholic Church': 3.284653425216675,\n",
       "   u'Melkite Christianity in Lebanon': -0.13574637472629547,\n",
       "   u'Melkite Greek Catholic Church': -0.1987207531929016,\n",
       "   u'Roman Catholic Church': 2.526700258255005,\n",
       "   u'Roman Catholicism in the Philippines': -1.114059329032898,\n",
       "   u'Simon Church': -1.8625925779342651,\n",
       "   u'Slovak Greek Catholic Church': 0.21489112079143524,\n",
       "   u'Ukrainian Greek Catholic Church': 3.751767873764038,\n",
       "   u'Union of Brest': -0.0643494576215744,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Composed in [dactylic hexameters] , Horace 's '' Satires '' explore the secrets of\": {'boosted': 10.0,\n",
       "  u'gold': u'Dactylic hexameter',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.2607097327709198,\n",
       "   u'Dactylic hexameter': 2.463752269744873,\n",
       "   u'XXNILXX': 0,\n",
       "   u'dactylic hexameters': 5.021961212158203,\n",
       "   u'hexameters': 2.684670925140381}},\n",
       " u'Directly underneath the burner is the [mantle] , a fabric bag coated with chemicals which incandesce -LRB-': {'boosted': 10.0,\n",
       "  u'gold': u'Gas mantle',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.006397500168532133,\n",
       "   u'Gas mantle': 2.1907334327697754,\n",
       "   u'Lee Mantle': -0.23616373538970947,\n",
       "   u'Mantle': 2.855233907699585,\n",
       "   u'Mantle (API)': 0.7743301391601562,\n",
       "   u'Mantle (climbing)': 0.9129906296730042,\n",
       "   u'Mantle (geology)': 2.52517032623291,\n",
       "   u'Mantle (mollusc)': 0.4951259195804596,\n",
       "   u'Mantle (vesture)': 0.2639189064502716,\n",
       "   u'Mantling': 0.6219220161437988,\n",
       "   u'Mickey Mantle': -0.8159354329109192,\n",
       "   u'Robe': 0.4835488796234131,\n",
       "   u'XXNILXX': 0,\n",
       "   u'mantle (clothing)': 0.7365752458572388}},\n",
       " u\"Especially in the [German] speaking world , geodesy is divided into geomensuration -LRB- ''\": {'boosted': 10.0,\n",
       "  u'gold': u'German language',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.687074899673462,\n",
       "   u'Cinema of Germany': 0.4117586016654968,\n",
       "   u'German': 1.9242503643035889,\n",
       "   u'German American': 1.1708024740219116,\n",
       "   u'German Empire': 0.8655787110328674,\n",
       "   u'German language': 2.5831549167633057,\n",
       "   u'German poetry': 1.115097999572754,\n",
       "   u'Germany': 3.861398696899414,\n",
       "   u'GfK Entertainment': -1.073133945465088,\n",
       "   u'Kriegsmarine': -0.601036787033081,\n",
       "   u'Nazi Germany': 1.3936834335327148,\n",
       "   u'Wehrmacht': 0.09369497001171112,\n",
       "   u'XXNILXX': 0,\n",
       "   u'german people': 1.430077075958252}},\n",
       " u'French , African , India n , Chinese , and [Arab] Roman Catholic 82.2 % , Anglican 6.4 % , Seventh-day': {'boosted': 10.0,\n",
       "  u'gold': u'Arab people',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.7984426021575928,\n",
       "   u'Arab American': 0.7869606018066406,\n",
       "   u'Arab Canadian': -1.2342058420181274,\n",
       "   u'Arab League': 0.9255086779594421,\n",
       "   u'Arab citizens of Israel': 2.313779354095459,\n",
       "   u'Arab cuisine': -1.364951252937317,\n",
       "   u'Arab culture': 0.3063618242740631,\n",
       "   u'Arab people': 2.2306125164031982,\n",
       "   u'Arab world': 1.9139844179153442,\n",
       "   u'Arab, Alabama': 0.1795804500579834,\n",
       "   u'Arabian horse': 1.4023613929748535,\n",
       "   u'Arabic language': 2.472878932952881,\n",
       "   u'Arabic music': -0.16428758203983307,\n",
       "   u'British Arabs': -1.1195106506347656,\n",
       "   u'History of Arabs in Afghanistan': -0.9587673544883728,\n",
       "   u'Palestinian people': 2.0934667587280273,\n",
       "   u'XXNILXX': 0}},\n",
       " u'He is associated with the region surrounding [Cartagena] , of which he is co - patron .': {'boosted': 10.0,\n",
       "  u'gold': u'Cartagena, Spain',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.8263614177703857,\n",
       "   u'Battle of Cartagena de Indias': 0.17975126206874847,\n",
       "   u'Cartagena': 2.5977537631988525,\n",
       "   u'Cartagena (Espa\\xf1a)': 0,\n",
       "   u'Cartagena (board game)': 0.32396399974823,\n",
       "   u'Cartagena CF': -0.6750214695930481,\n",
       "   u'Cartagena FC': -0.5363031625747681,\n",
       "   u'Cartagena Film Festival': 0.0007888107793405652,\n",
       "   u'Cartagena Province': -0.7422205805778503,\n",
       "   u'Cartagena, Chile': -0.5836858153343201,\n",
       "   u'Cartagena, Colombia': 2.542501449584961,\n",
       "   u'Cartagena, Spain': 2.13157320022583,\n",
       "   u'Estadio Jaime Mor\\xf3n Le\\xf3n': -0.791400671005249,\n",
       "   u'FC Cartagena': -0.4415714740753174,\n",
       "   u'FS Cartagena': -0.6684408187866211,\n",
       "   u'Rafael N\\xfa\\xf1ez International Airport': -0.8995654582977295,\n",
       "   u'Roman Catholic Archdiocese of Cartagena': -0.7002612352371216,\n",
       "   u'XXNILXX': 0}},\n",
       " u'He is currently playing for German [2nd Bundesliga] team 1 .': {'boosted': 10.0,\n",
       "  u'gold': u'2. Fu\\xdfball-Bundesliga',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.5282838344573975,\n",
       "   u'2. Fu\\xdfball-Bundesliga': 2.347094774246216,\n",
       "   u'2. Fu\\xdfball-Bundesliga (women)': 0.701959490776062,\n",
       "   u'2012\\u201313 Bundesliga': 0.07826611399650574,\n",
       "   u'2013\\u201314 Bundesliga': 0.9443637132644653,\n",
       "   u'2nd Bundesliga': 0,\n",
       "   u'2nd Bundesliga (ice hockey)': 3.5741851329803467,\n",
       "   u'Austrian Football Bundesliga': 1.842813491821289,\n",
       "   u'Basketball Bundesliga': 1.4441237449645996,\n",
       "   u'Bundesliga': 1.9776952266693115,\n",
       "   u'Fu\\xdfball-Bundesliga (women)': 1.4028915166854858,\n",
       "   u'German Football League 2': 1.3498750925064087,\n",
       "   u'XXNILXX': 0}},\n",
       " u'He served in the Second Australian Imperial Force in [New Guinea] with the 39th Transport Platoon in 1944 and 1945 and': {'boosted': 10.0,\n",
       "  u'gold': u'New Guinea campaign',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.0943894386291504,\n",
       "   u'Anglican Church of Papua New Guinea': -0.1121833547949791,\n",
       "   u'Elections in Guinea': -0.3637927174568176,\n",
       "   u'French Guinea': 0.4228859543800354,\n",
       "   u'F\\xe9d\\xe9ration Guin\\xe9enne de Football': -0.9379677176475525,\n",
       "   u'German New Guinea': 1.4006586074829102,\n",
       "   u'Guinea': 1.1496946811676025,\n",
       "   u'Guinea (British coin)': -0.3204309940338135,\n",
       "   u'Guinea (region)': 0.22436369955539703,\n",
       "   u'Guinea national football team': 0.3228384554386139,\n",
       "   u\"Guinea women's national football team\": 0,\n",
       "   u'Netherlands New Guinea': 1.3860048055648804,\n",
       "   u'New Guinea': 4.6812615394592285,\n",
       "   u'New Guinea campaign': 2.45607590675354,\n",
       "   u'Papua New Guinea': 1.3437352180480957,\n",
       "   u'Papuan languages': -0.47067663073539734,\n",
       "   u'Portuguese Guinea': 0.8590911626815796,\n",
       "   u'Territory of New Guinea': 1.691986083984375,\n",
       "   u'XXNILXX': 0}},\n",
       " u'If the [mantle] is visibly damaged , heat may become focused and damage': {'boosted': 10.0,\n",
       "  u'gold': u'Gas mantle',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.0998364686965942,\n",
       "   u'Gas mantle': 1.6869642734527588,\n",
       "   u'Lee Mantle': -0.9023422002792358,\n",
       "   u'Mantle': 2.4360132217407227,\n",
       "   u'Mantle (API)': 0.14998245239257812,\n",
       "   u'Mantle (climbing)': -0.1505756974220276,\n",
       "   u'Mantle (geology)': 2.2680704593658447,\n",
       "   u'Mantle (mollusc)': 0.8229799866676331,\n",
       "   u'Mantle (vesture)': 0.2948889434337616,\n",
       "   u'Mantling': 0.41740623116493225,\n",
       "   u'Mickey Mantle': -0.8034210801124573,\n",
       "   u'Robe': 0.773875892162323,\n",
       "   u'XXNILXX': 0,\n",
       "   u'mantle (clothing)': 0.27274176478385925}},\n",
       " u'Italy , alone , perhaps stimulated by an outbreak of [plague] in Nuremberg .': {'boosted': 10.0,\n",
       "  u'gold': u'Black Death',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.6609325408935547,\n",
       "   u'Black Death': 1.7593836784362793,\n",
       "   u'Bubonic plague': 2.778386354446411,\n",
       "   u'Epidemic': 1.1041265726089478,\n",
       "   u'Great Plague of London': 1.5860034227371216,\n",
       "   u'Klinik': -1.2324118614196777,\n",
       "   u'Pandemic': -0.8638682961463928,\n",
       "   u'Plague (1978 film)': -0.9119076728820801,\n",
       "   u'Plague (band)': 0,\n",
       "   u'Plague (comics)': -1.6672035455703735,\n",
       "   u'Plague (disease)': 3.610664129257202,\n",
       "   u'Plague (song)': 0,\n",
       "   u'Plague of Athens': 1.7886695861816406,\n",
       "   u'Plagues of Egypt': 1.0544980764389038,\n",
       "   u'XXNILXX': 0,\n",
       "   u'plague': 3.5013601779937744}},\n",
       " u\"Latin for '' miscellaneous poems '' -RRB- , Horace combines [Epicurean] , that is , originally Greek philosophy with Roman good\": {'boosted': 10.0,\n",
       "  u'gold': u'Epicurus',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.6733388900756836,\n",
       "   u'Epicurean': 4.060953140258789,\n",
       "   u'Epicurean (album)': 0,\n",
       "   u'Epicurean (band)': 0,\n",
       "   u'Epicurus': 3.7209479808807373,\n",
       "   u'Problem of evil': 0.962831437587738,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Located approximately 18 minutes from [Murwillumbah] , 45 minutes from the Gold Coast , Queensland and': {'boosted': 10.0,\n",
       "  u'gold': u'Murwillumbah, New South Wales',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.2260217666625977,\n",
       "   u'Electoral district of Murwillumbah': 0,\n",
       "   u'Murwillumbah FC': 0,\n",
       "   u'Murwillumbah Parish, Rous': 0,\n",
       "   u'Murwillumbah railway line': 2.1964447498321533,\n",
       "   u'Murwillumbah railway station, New South Wales': 0,\n",
       "   u'Murwillumbah, New South Wales': 2.1214680671691895,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Panofsky argues that this print combined the ' [Ulm] ian style ' of Koberger 's ' Lives of the\": {'boosted': 10.0,\n",
       "  u'gold': u'Ulm',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.7479660511016846,\n",
       "   u'Battle of Ulm': 2.915086507797241,\n",
       "   u'David Ulm': 0,\n",
       "   u'Donaustadion': -0.9351232051849365,\n",
       "   u'Free Imperial City of Ulm': 0,\n",
       "   u'Ratiopharm Ulm': -1.3115098476409912,\n",
       "   u'Robert Ulm': 0,\n",
       "   u'SSV Ulm 1846': -1.6677896976470947,\n",
       "   u'SSV Ulm 1846 Fu\\xdfball': -0.6241095066070557,\n",
       "   u'Trams in Ulm': 0,\n",
       "   u'Ulm': 2.45888352394104,\n",
       "   u'Ulm Campaign': 0.5621033310890198,\n",
       "   u'Ulm Central Station': -0.11509960889816284,\n",
       "   u'Ulm Minster': 1.732229471206665,\n",
       "   u'Ulm, Arkansas': -1.465696930885315,\n",
       "   u'Ulm, Montana': 0.15888987481594086,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Published probably in 35 BCE and at the latest by [33 BCE] , the first book of '' Satires '' represents Horace\": {'boosted': 10.0,\n",
       "  u'gold': u'33 BC',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.3985595405101776,\n",
       "   u'33 BC': 1.7715274095535278,\n",
       "   u'Anno Domini': 3.5122199058532715,\n",
       "   u'Associa\\xe7\\xe3o de Basquete Cearense': 0.6623108983039856,\n",
       "   u'BCE (disambiguation)': 0,\n",
       "   u'Bell Canada Enterprises': 1.7303959131240845,\n",
       "   u'Common Era': 3.8205342292785645,\n",
       "   u'Southern Africa BC': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'South Asian and [Chinese] -LRB- 1.1 % of the population -RRB- account for the': {'boosted': 10.0,\n",
       "  u'gold': u'Overseas Chinese',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.644362211227417,\n",
       "   u'Chinese': 3.279963493347168,\n",
       "   u'Chinese American': 1.319133996963501,\n",
       "   u'Chinese Canadian': 0.08261226117610931,\n",
       "   u'Chinese characters': 0.5431445837020874,\n",
       "   u'Chinese cuisine': 1.595970869064331,\n",
       "   u'Chinese language': 3.985731363296509,\n",
       "   u'Chinese people': 2.51590633392334,\n",
       "   u'Han Chinese': 1.5417559146881104,\n",
       "   u'History of China': 0.9682263135910034,\n",
       "   u'Overseas Chinese': 1.889473557472229,\n",
       "   u\"People's Republic of China\": 2.284214496612549,\n",
       "   u'Simplified Chinese characters': 0.7496347427368164,\n",
       "   u'Varieties of Chinese': 0.2776169180870056,\n",
       "   u'XXNILXX': 0}},\n",
       " u'The Schooner crater itself was host to a series of [Apollo] astronauts training in its artificial lunar landscape , including Neil': {'boosted': 10.0,\n",
       "  u'gold': u'Apollo program',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.7144360542297363,\n",
       "   u'Apollo': 4.455021858215332,\n",
       "   u'Apollo (ballet)': -1.6292049884796143,\n",
       "   u'Apollo (comics)': -1.2750828266143799,\n",
       "   u'Apollo (crater)': -1.030488133430481,\n",
       "   u'Apollo (magazine)': -1.2985478639602661,\n",
       "   u'Apollo (quintet)': -0.920185387134552,\n",
       "   u'Apollo (spacecraft)': 0.6566142439842224,\n",
       "   u'Apollo Records (1944)': -0.41553276777267456,\n",
       "   u'Apollo Records (Belgium)': -1.5830131769180298,\n",
       "   u'Apollo Theater': 0.8217607736587524,\n",
       "   u'Apollo asteroid': -0.21956878900527954,\n",
       "   u'Apollo program': 2.4152486324310303,\n",
       "   u'Apollo, Pennsylvania': -0.4321116805076599,\n",
       "   u'Games by Apollo': -0.20424483716487885,\n",
       "   u'Lee Adama': -0.562773585319519,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"The local [Seychellois Creole -LRB- '' Kreol] '' -RRB- , a creole language derived from French and\": {'boosted': 10.0,\n",
       "  u'gold': u'Seychellois Creole',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.3500942885875702,\n",
       "   u'Creole': 2.521867513656616,\n",
       "   u'Creole (markup)': 0.6616966128349304,\n",
       "   u'Creole -LRB-': 0,\n",
       "   u\"Creole -LRB- ''\": 0,\n",
       "   u'Creole language': 2.180790662765503,\n",
       "   u'Creole peoples': 2.995063066482544,\n",
       "   u'Criollo people': 2.3459229469299316,\n",
       "   u'Haitian Creole': 2.5543410778045654,\n",
       "   u'Louisiana Creole cuisine': 1.0560718774795532,\n",
       "   u'Louisiana Creole people': 2.9890334606170654,\n",
       "   u'Mauritian creole': 1.5082305669784546,\n",
       "   u'Seychellois Creole': 1.0705846548080444,\n",
       "   u\"Seychellois Creole -LRB- '' Kreol\": 0,\n",
       "   u'Seychellois Creole -LRB- Kreol': 0,\n",
       "   u'Seychellois Creole people': 0,\n",
       "   u'Sierra Leone Creole people': 2.353364944458008,\n",
       "   u'XXNILXX': 0}},\n",
       " u'These [dyes] find extensive use in various areas of textile dye ing': {'boosted': 10.0,\n",
       "  u'gold': u'Dye',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.1097378730773926,\n",
       "   u'Acid dye': -0.2419782131910324,\n",
       "   u'Dye': 2.2165164947509766,\n",
       "   u'Dye, Georgia': 0,\n",
       "   u'Dyeing': 3.855269193649292,\n",
       "   u'Dyess Air Force Base': -0.4189167320728302,\n",
       "   u'Dyewoods': 0.6829866170883179,\n",
       "   u'Jermaine Dye': -0.44538721442222595,\n",
       "   u'Natural dye': 3.245281457901001,\n",
       "   u'Ngangam language': 0,\n",
       "   u'Thomas R. Dye': 0,\n",
       "   u'Tippy Dye': -0.7819390892982483,\n",
       "   u'XXNILXX': 0,\n",
       "   u'mushroom dye': 0.7871966361999512}},\n",
       " u'They fly between Seward Park and [Maple Leaf] in northeast Seattle .': {'boosted': 10.0,\n",
       "  u'gold': u'Maple Leaf, Seattle',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.1689519882202148,\n",
       "   u'Flag of Canada': -0.12196142226457596,\n",
       "   u'Henry Leaf': 0,\n",
       "   u'Leaf': 0.7299705743789673,\n",
       "   u'Leaf (Israeli company)': -0.4198729693889618,\n",
       "   u'Leaf (Japanese company)': -0.8369763493537903,\n",
       "   u'Leaf International': -0.43291616439819336,\n",
       "   u'Maple Leaf (train)': 0.7704975605010986,\n",
       "   u'Maple Leaf, Seattle': 0.7359983921051025,\n",
       "   u'Maple Leaf, Toronto': -0.3278335928916931,\n",
       "   u'Maple leaf': 3.3898186683654785,\n",
       "   u'Nissan Leaf': 0.9383474588394165,\n",
       "   u'Ryan Leaf': -0.2806205749511719,\n",
       "   u'The Leaf Label': -1.8457903861999512,\n",
       "   u'XXNILXX': 0}},\n",
       " u'When Lang was abolished prior to the [1977 election] , he transferred to Grayndler .': {'boosted': 10.0,\n",
       "  u'gold': u'Australian federal election, 1977',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.7480233907699585,\n",
       "   u'Australian federal election, 1977': 0.923015832901001,\n",
       "   u'Canadian federal election, 2011': 0.6316069960594177,\n",
       "   u'Election (1999 film)': 0.6918074488639832,\n",
       "   u'Election (2005 film)': -0.08070088177919388,\n",
       "   u'Election (horse)': -1.4550310373306274,\n",
       "   u'Election (novel)': -0.5036104321479797,\n",
       "   u'Fiji election of 2001': -0.024782221764326096,\n",
       "   u'List of elections in 1977': 0,\n",
       "   u'Manitoba general election, 1977': 0.5035786032676697,\n",
       "   u'Ontario general election, 1977': 1.3110758066177368,\n",
       "   u'Philippine general election, 2007': -0.2824065089225769,\n",
       "   u'Royal elections in Poland': 0.0723412036895752,\n",
       "   u'Tamil Nadu Legislative Assembly election, 1977': 2.2329368591308594,\n",
       "   u'United States presidential election, 2008': 1.088081955909729,\n",
       "   u'XXNILXX': 0,\n",
       "   u'election': 0.9124931693077087}},\n",
       " u'a variety of dance and theatre classes and workshops including [salsa] , breakdancing , circus skills and youth theatre .': {'boosted': 10.0,\n",
       "  u'gold': u'Salsa',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.3055543899536133,\n",
       "   u'Latin American music': 0.7452428936958313,\n",
       "   u'Music of Puerto Rico': -0.8797909021377563,\n",
       "   u'Salsa': 0.02464701235294342,\n",
       "   u'Salsa (1988 film)': -0.9269459247589111,\n",
       "   u'Salsa (dance)': 1.32402765750885,\n",
       "   u'Salsa (sauce)': 0.27190494537353516,\n",
       "   u'Salsa music ': 0,\n",
       "   u'Victoria Grizzlies': -0.5855501294136047,\n",
       "   u'XXNILXX': 0,\n",
       "   u'salsa (music)': 0.47499072551727295}},\n",
       " u\"al-Ma ' mun commissioned a group of Muslim astronomers and [geographers] to measure the distance from Tadmur -LRB- Palmyra -RRB- to\": {'boosted': 10.0,\n",
       "  u'gold': u'Geography and cartography in medieval Islam',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.634253978729248,\n",
       "   u'Chinese geography': -0.44370436668395996,\n",
       "   u'Geographer (Band)': -2.005075216293335,\n",
       "   u'Geography and cartography in medieval Islam': 1.2585113048553467,\n",
       "   u'History of geography': 0.005381112918257713,\n",
       "   u'Human geography': 0.6066229939460754,\n",
       "   u'List of Graeco-Roman geographers': -0.36832261085510254,\n",
       "   u'Physical geography': -0.3457469344139099,\n",
       "   u'Ptolemy': 1.74225914478302,\n",
       "   u'XXNILXX': 0,\n",
       "   u'geographer': 3.9847729206085205,\n",
       "   u'geography': 2.506525754928589}},\n",
       " u'century there was also a short line from Cresswell to [Cheadle] -LRB- Cheadle Branch Line -RRB- .': {'boosted': 10.0,\n",
       "  u'gold': u'Cheadle, Staffordshire',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.8127580881118774,\n",
       "   u'Cheadle': 1.4456061124801636,\n",
       "   u'Cheadle (constituency)': -0.7868959307670593,\n",
       "   u'Cheadle CLC railway station': 0,\n",
       "   u'Cheadle Hulme': 0.7920551896095276,\n",
       "   u'Cheadle Hulme, Greater Manchester': 0,\n",
       "   u'Cheadle LNW railway station': 0,\n",
       "   u'Cheadle Rural District': 0,\n",
       "   u'Cheadle by-election, 2005': -0.9474254846572876,\n",
       "   u'Cheadle railway station': 0,\n",
       "   u'Cheadle, Alberta': -0.34988680481910706,\n",
       "   u'Cheadle, Greater Manchester': 1.7413750886917114,\n",
       "   u'Cheadle, Staffordshire': 1.5098868608474731,\n",
       "   u'Dave Cheadle': -1.4760698080062866,\n",
       "   u'Hazel Cheadle': 0,\n",
       "   u'Tommy Cheadle': -1.830241084098816,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"de Arl s '' , who was martyred in the [4th century] .\": {'boosted': 10.0,\n",
       "  u'gold': u'4th century',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.108738422393799,\n",
       "   u'4th century': 3.351227283477783,\n",
       "   u'4th century BC': 1.4417332410812378,\n",
       "   u'4th century in Ireland': 0,\n",
       "   u'4th century in Roman Britain': 0,\n",
       "   u'Buick Century': -0.23071925342082977,\n",
       "   u'Centuria': 0.6698364615440369,\n",
       "   u'Century (film)': -0.5044951438903809,\n",
       "   u'Century, Florida': -1.1698095798492432,\n",
       "   u'Christianity in the 4th century': 3.6474673748016357,\n",
       "   u'Chronological list of saints in the 4th century': -0.566197395324707,\n",
       "   u'Random House': 0.09858706593513489,\n",
       "   u'The Century Magazine': -0.07020217180252075,\n",
       "   u'XXNILXX': 0,\n",
       "   u'century': 1.8219599723815918,\n",
       "   u'century (cricket)': -0.11627169698476791}},\n",
       " u\"faction of the party and was widely considered a '' [grouper] '' .\": {'boosted': 10.0,\n",
       "  u'gold': u'Industrial Groups',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.21542669832706451,\n",
       "   u'Giant grouper': 1.1913533210754395,\n",
       "   u'Grouper (band)': 0.10638427734375,\n",
       "   u'Industrial Groups': 1.8174916505813599,\n",
       "   u'Nexus 7 (2012 version)': 0.5383498072624207,\n",
       "   u'USS Grouper (SS-214)': 0.35567331314086914,\n",
       "   u'XXNILXX': 0,\n",
       "   u'grouper': 2.972658395767212}},\n",
       " u\"for American singer Hilary Duff 's debut album , '' [Metamorphosis] '' (2003) .\": {'boosted': 10.0,\n",
       "  u'gold': u'Metamorphosis (Hilary Duff album)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.226172685623169,\n",
       "   u'Metamorphosi': -1.039946436882019,\n",
       "   u'Metamorphosis': 2.9456465244293213,\n",
       "   u'Metamorphosis (2012 film)': -1.9879573583602905,\n",
       "   u'Metamorphosis (Arthur Blythe album)': -0.5325279831886292,\n",
       "   u'Metamorphosis (Circle of Dust album)': -1.731492519378662,\n",
       "   u'Metamorphosis (Don Friedman album)': -0.5296127796173096,\n",
       "   u'Metamorphosis (EP)': 0.3406669497489929,\n",
       "   u'Metamorphosis (Hilary Duff album)': 1.5847047567367554,\n",
       "   u'Metamorphosis (Iron Butterfly album)': -0.022507483139634132,\n",
       "   u'Metamorphosis (Papa Roach album)': 1.1372833251953125,\n",
       "   u'Metamorphosis (Rolling Stones album)': 0.5918455123901367,\n",
       "   u'Metamorphosis (TV series)': -1.0378750562667847,\n",
       "   u'Metamorphosis (Wade Marcus album)': 0.48508110642433167,\n",
       "   u'Metamorphosis (World Saxophone Quartet album)': 0,\n",
       "   u'Metamorphosis (Yeng Constantino album)': 0,\n",
       "   u'Metamorphosis (illusion)': -1.1391037702560425,\n",
       "   u'The Metamorphosis': -0.4523847997188568,\n",
       "   u'XXNILXX': 0}},\n",
       " u'friend Maecenas when the latter negotiated one last truce between [Antony] and Octavian , the Peace of Brundisium -LRB- 36 BCE': {'boosted': 10.0,\n",
       "  u'gold': u'Antony',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.279149055480957,\n",
       "   u'Anthony the Great': 1.7368360757827759,\n",
       "   u'Antony': 2.8192267417907715,\n",
       "   u'Antony (Paris RER)': 0,\n",
       "   u'Antony (racing driver)': 0,\n",
       "   u'Antony Acland': -0.36960265040397644,\n",
       "   u'Antony Hegarty': -0.02160758152604103,\n",
       "   u'Antony House': 1.2487951517105103,\n",
       "   u'Antony Worrall Thompson': -0.41535860300064087,\n",
       "   u'Antony and the Johnsons': -0.4454231262207031,\n",
       "   u'Antony, Cornwall': 1.1265537738800049,\n",
       "   u'Antony, Hauts-de-Seine': 2.1499481201171875,\n",
       "   u'Arrondissement of Antony': 0,\n",
       "   u'Canton of Antony': 0,\n",
       "   u\"Gare d'Antony\": 0,\n",
       "   u'Mark Antony': 3.5866241455078125,\n",
       "   u'Mark Antony (Rome character)': 0.9305697083473206,\n",
       "   u'XXNILXX': 0}},\n",
       " u'identification with the cult of an Islamic jinn ; a [Roman] genius ; as well as with an ancient Carthaginian site': {'boosted': 10.0,\n",
       "  u'gold': u'Roman Empire',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.175008535385132,\n",
       "   u'Ancient Roman architecture': 0.8837987184524536,\n",
       "   u'Latin alphabet': 0.3281463384628296,\n",
       "   u'Roman': 1.920945167541504,\n",
       "   u'Roman Britain': 1.7645766735076904,\n",
       "   u'Roman Catholic (term)': 0.15539516508579254,\n",
       "   u'Roman Empire': 3.097529649734497,\n",
       "   u'Roman Republic': 1.7605115175247192,\n",
       "   u'Roman Rite': 0.6479516625404358,\n",
       "   u'Roman art': 1.8834885358810425,\n",
       "   u'Roman law': 1.4237816333770752,\n",
       "   u'Roman mythology': 1.4895930290222168,\n",
       "   u'Roman, Romania': 0.8528891801834106,\n",
       "   u'Rome': 2.166269302368164,\n",
       "   u'Switzerland in the Roman era': 1.1924142837524414,\n",
       "   u'XXNILXX': 0,\n",
       "   u'ancient Rome': 4.342885494232178}},\n",
       " u'impressive performances , notably a hat trick for Slovenia against [Trinidad and Tobago] ': {'boosted': 10.0,\n",
       "  u'gold': u'Trinidad and Tobago national football team',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.4866938591003418,\n",
       "   u'Music of Trinidad and Tobago': 0.3009393811225891,\n",
       "   u'Trinidad': -0.26173731684684753,\n",
       "   u'Trinidad and': 0,\n",
       "   u'Trinidad and Tobago': 3.849586248397827,\n",
       "   u'Trinidad and Tobago Football Association': 0.7708442807197571,\n",
       "   u'Trinidad and Tobago at the 2006 Commonwealth Games': 0,\n",
       "   u'Trinidad and Tobago national cricket team': 1.8077422380447388,\n",
       "   u'Trinidad and Tobago national football team': 2.8217711448669434,\n",
       "   u\"Trinidad and Tobago women's national football team\": 0.15711022913455963,\n",
       "   u'Trinidad, Bohol': -0.6965724229812622,\n",
       "   u'Trinidad, Bolivia': -0.6709165573120117,\n",
       "   u'Trinidad, California': -0.983791172504425,\n",
       "   u'Trinidad, Colorado': -0.668942928314209,\n",
       "   u'Trinidad, Cuba': -0.6292270421981812,\n",
       "   u'Trinidad, Uruguay': -0.3124801814556122,\n",
       "   u'XXNILXX': 0}},\n",
       " u'including Neil Armstrong , Dick Gordon , Buzz Aldrin , [Dave Scott] and Rusty Schweickart .': {'boosted': 10.0,\n",
       "  u'gold': u'David Scott',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.8214328289031982,\n",
       "   u'Dave Scott': 0,\n",
       "   u'Dave Scott (American football official)': 0,\n",
       "   u'Dave Scott (American football)': 0,\n",
       "   u'Dave Scott (choreographer)': -0.5164754986763,\n",
       "   u'Dave Scott (driver)': 0,\n",
       "   u'Dave Scott (racing driver)': 0,\n",
       "   u'Dave Scott (triathlete)': 0.08954225480556488,\n",
       "   u'David Scott': 1.2405402660369873,\n",
       "   u'Davy Scott (racing driver)': 0,\n",
       "   u'Robert Falcon Scott': 1.5100411176681519,\n",
       "   u'Scott': -1.5542734861373901,\n",
       "   u'Scott County, Indiana': -1.8678172826766968,\n",
       "   u'Scott County, Iowa': -1.655368447303772,\n",
       "   u'Scott County, Minnesota': -0.8472515344619751,\n",
       "   u'Scott County, Mississippi': -0.667178213596344,\n",
       "   u'Scott County, Missouri': -1.6948484182357788,\n",
       "   u'Scott Steiner': -1.3458093404769897,\n",
       "   u'XXNILXX': 0,\n",
       "   u'Zach Scott': -2.1708850860595703}},\n",
       " u'is the principal and newest theatre in the city of [Newport] , South Wales .': {'boosted': 10.0,\n",
       "  u'gold': u'Newport',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.7393009662628174,\n",
       "   u'Newport': 3.4213898181915283,\n",
       "   u'Newport (Cornwall) (UK Parliament constituency)': -1.2119592428207397,\n",
       "   u'Newport (Isle of Wight) (UK Parliament constituency)': -1.2804148197174072,\n",
       "   u'Newport (city), Vermont': 0.6722165942192078,\n",
       "   u'Newport County, Rhode Island': 1.1213266849517822,\n",
       "   u'Newport RFC': -1.1426042318344116,\n",
       "   u'Newport, Arkansas': 0.18056891858577728,\n",
       "   u'Newport, Delaware': 1.5438355207443237,\n",
       "   u'Newport, Isle of Wight': 1.2178363800048828,\n",
       "   u'Newport, Kentucky': 0.9887382984161377,\n",
       "   u'Newport, New Hampshire': -0.10749424248933792,\n",
       "   u'Newport, Oregon': 1.6501346826553345,\n",
       "   u'Newport, Pembrokeshire': 1.5530213117599487,\n",
       "   u'Newport, Rhode Island': 2.4478986263275146,\n",
       "   u'Newport, Shropshire': 0.36172303557395935,\n",
       "   u'Newport, Tennessee': 1.5048117637634277,\n",
       "   u'Newport, Victoria': 1.1388381719589233,\n",
       "   u'Newport, Wales': 3.6476171016693115,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"n al-B r n -LRB- 973-1048 -RRB- solved a complex [geodesic] equation in order to accurately compute the Earth 's circumference\": {'boosted': 10.0,\n",
       "  u'gold': u'Geodesy',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.7651054859161377,\n",
       "   u'As the crow flies': 0.2852645218372345,\n",
       "   u'Distance (graph theory)': 0.502475380897522,\n",
       "   u'Geodesic (general relativity)': -0.34578123688697815,\n",
       "   u'Geodesic airframe': 0.37730872631073,\n",
       "   u'Geodesic as Hamiltonian flow': 0,\n",
       "   u'Geodesic grid': -0.7028902173042297,\n",
       "   u'Geodesy': 1.3342511653900146,\n",
       "   u'XXNILXX': 0,\n",
       "   u'geodesic': 4.624479293823242,\n",
       "   u'geodesic dome': 0.5463342070579529}},\n",
       " u'n club LASK Linz , having began his career with [Olimpija Ljubljana] . sup / sup With Litex he reached the UEFA': {'boosted': 10.0,\n",
       "  u'gold': u'Olimpija Ljubljana',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -0.4036501944065094,\n",
       "   u'City Municipality of Ljubljana': 0.006056870333850384,\n",
       "   u'FC Ljubljana': -0.292174369096756,\n",
       "   u'HDD Olimpija Ljubljana': 1.7039800882339478,\n",
       "   u'KK Union Olimpija': 0.666414737701416,\n",
       "   u'Ljubljana': -0.7739601731300354,\n",
       "   u'Ljubljana Jo\\u017ee Pu\\u010dnik Airport': -1.3622064590454102,\n",
       "   u'Ljubljana railway station': -0.6571348309516907,\n",
       "   u'NK Ljubljana': 0.47392863035202026,\n",
       "   u'NK Olimpija Ljubljana (1911\\u20132004)': 1.4710227251052856,\n",
       "   u'NK Olimpija Ljubljana (2005)': 2.711937189102173,\n",
       "   u'Olimpija Ljubljana': 2.506939649581909,\n",
       "   u'RK Olimpija': 0.11451364308595657,\n",
       "   u'Roman Catholic Archdiocese of Ljubljana': -0.21594373881816864,\n",
       "   u'University of Ljubljana': -0.937613844871521,\n",
       "   u'XXNILXX': 0}},\n",
       " u'not to be a perfect sphere but to approximate an [oblate spheroid] , a specific type of ellipsoid .': {'boosted': 10.0,\n",
       "  u'gold': u'Spheroid',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.8026835918426514,\n",
       "   u'Bulge (astronomy)': -0.3484538495540619,\n",
       "   u'Oblate spheroid': 3.9795007705688477,\n",
       "   u'Spheroid': 1.2083011865615845,\n",
       "   u'XXNILXX': 0,\n",
       "   u'galactic bulge': 0.09857820719480515,\n",
       "   u'oblate': 0.32344600558280945,\n",
       "   u'sphere': 0.7593540549278259}},\n",
       " u'period , though it is likely that he went to [Frankfurt] and the Netherlands .': {'boosted': 10.0,\n",
       "  u'gold': u'Frankfurt am Main',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.9304287433624268,\n",
       "   u'1. FFC Frankfurt': -2.0917036533355713,\n",
       "   u'Commerzbank-Arena': -0.5949131846427917,\n",
       "   u'Eintracht Frankfurt': -1.5756582021713257,\n",
       "   u'Frankfurt (Oder)': 1.90959894657135,\n",
       "   u'Frankfurt Central Station': -0.46778616309165955,\n",
       "   u'Frankfurt Cup': 0,\n",
       "   u'Frankfurt Grand Prix (tennis)': 0,\n",
       "   u'Frankfurt International Airport': 0.09352626651525497,\n",
       "   u'Frankfurt Stock Exchange': 0.30856746435165405,\n",
       "   u'Frankfurt am Main': 1.5119824409484863,\n",
       "   u'Free City of Frankfurt': 1.8138810396194458,\n",
       "   u'Goethe University Frankfurt': 1.074343204498291,\n",
       "   u'Grand Duchy of Frankfurt': -1.4233711957931519,\n",
       "   u'XXNILXX': 0}},\n",
       " u'radioactive debris was blown into the atmosphere , including 910 [kCi] of Iodine-131 -LRB- sup 131 / sup I -RRB- ,': {'boosted': 10.0,\n",
       "  u'gold': u'Curie',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': 0.10306374728679657,\n",
       "   u'Curie': 2.649035930633545,\n",
       "   u'KCi': 2.816521406173706,\n",
       "   u'XXNILXX': 0}},\n",
       " u'second half of the 18th century , Ukrainians from the [Transcarpathian] Region formed agricultural settlements in Hungary , primarily in the': {'boosted': 10.0,\n",
       "  u'gold': u'Zakarpattia Oblast',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.5982035398483276,\n",
       "   u'Carpathian Ruthenia': 2.3288838863372803,\n",
       "   u'Transcarpathian': 3.9218180179595947,\n",
       "   u'XXNILXX': 0,\n",
       "   u'Zakarpattia Oblast': 2.72979474067688}},\n",
       " u\"than '' Pachinko Sexy Reaction 2 '' -RRB- developed by [Sammy] in 1998 .\": {'boosted': 10.0,\n",
       "  u'gold': u'Sammy',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.0987889766693115,\n",
       "   u'Darren Sammy': -1.4175739288330078,\n",
       "   u'Hendra Samuel Simorangkir': 0,\n",
       "   u'Sammy': 1.7507010698318481,\n",
       "   u'Sammy (Cracker)': 0,\n",
       "   u'Sammy (Prison Break character)': 0.3921399712562561,\n",
       "   u'Sammy (TV series)': 0,\n",
       "   u'Sammy (band)': 0,\n",
       "   u'Sammy (comics)': -0.5497931241989136,\n",
       "   u'Sammy (song)': 0,\n",
       "   u'Sammy Ameobi': -1.7167162895202637,\n",
       "   u'Sammy Babitzin': -1.5776399374008179,\n",
       "   u'Sammy Corporation': 2.5160491466522217,\n",
       "   u'Sammy Gravano': 0.07643619924783707,\n",
       "   u'Sammy Hagar': 1.101197361946106,\n",
       "   u'Sammy Ofer': 0.5100281238555908,\n",
       "   u'Sammy Sosa': 0.5628306865692139,\n",
       "   u'Sega Sammy Holdings': 0.009981234557926655,\n",
       "   u'XXNILXX': 0}},\n",
       " u'the NTS in late 1963 under an agreement between the [Atomic Energy Commission] and the U.S. Air Force .': {'boosted': 10.0,\n",
       "  u'gold': u'Atomic Energy Commission',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.336196184158325,\n",
       "   u'Atomic Energy Commission': 2.262821912765503,\n",
       "   u'Atomic Energy Commission of India': 0.6407929062843323,\n",
       "   u\"Commissariat \\xe0 l'\\xe9nergie atomique et aux \\xe9nergies alternatives\": 0,\n",
       "   u'Commission': -0.15079735219478607,\n",
       "   u'County commission': -1.019161343574524,\n",
       "   u'Energy Commission': 0,\n",
       "   u'Energy Commission of Malaysia': 0,\n",
       "   u'European Commission': 0.6305187940597534,\n",
       "   u'Government agency': 0.3585527539253235,\n",
       "   u'Pakistan Atomic Energy Commission': 0.8879343271255493,\n",
       "   u'Sicilian Mafia Commission': -0.9851098656654358,\n",
       "   u'Taxi Services Commission': -0.3832438588142395,\n",
       "   u'The Commission (mafia)': -1.1528993844985962,\n",
       "   u'U.S. Securities and Exchange Commission': -0.4692952632904053,\n",
       "   u'United Nations Atomic Energy Commission': 0.35642486810684204,\n",
       "   u'United States Atomic Energy Commission': 3.467440366744995,\n",
       "   u'XXNILXX': 0}},\n",
       " u'the River Usk on the Bristol Packet Wharf in the [city centre] .': {'boosted': 10.0,\n",
       "  u'gold': u'Newport city centre',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.4972631931304932,\n",
       "   u'Belfast City Centre': 1.1718910932540894,\n",
       "   u'Bristol city centre': 1.8532181978225708,\n",
       "   u'Cardiff city centre': 2.0808351039886475,\n",
       "   u'Centre (French region)': 0.41943320631980896,\n",
       "   u'Centre County, Pennsylvania': 0.576153576374054,\n",
       "   u'Centrism': -0.023414019495248795,\n",
       "   u'Indre By': 0.5952804088592529,\n",
       "   u'Leeds city centre': 1.0765780210494995,\n",
       "   u'Manchester city centre': 1.0940430164337158,\n",
       "   u'Newport city centre': 1.6466844081878662,\n",
       "   u'Swansea City Centre': 1.8725942373275757,\n",
       "   u'XXNILXX': 0,\n",
       "   u'centre (ice hockey)': -1.0839979648590088,\n",
       "   u'centre (rugby union)': -0.20296013355255127,\n",
       "   u'city centre': 5.322578430175781}},\n",
       " u'the source of the children s illness was contact with [calves] at the farm .': {'boosted': 10.0,\n",
       "  u'gold': u'Calf',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.184058904647827,\n",
       "   u'Birth': 0.3590506911277771,\n",
       "   u'Calf': 2.9201998710632324,\n",
       "   u'Calf muscle': -0.5136963129043579,\n",
       "   u'Calves': 3.0568978786468506,\n",
       "   u'Cattle': 1.0800914764404297,\n",
       "   u'Gastrocnemius muscle': 0.14321698248386383,\n",
       "   u'Ice calving': -0.6882404685020447,\n",
       "   u'XXNILXX': 0,\n",
       "   u'calf (anatomy)': -0.505111038684845,\n",
       "   u'calve': 0.29704609513282776}},\n",
       " u'to have been inspired by the Dogme 95 manifesto for [cinematic] minimalism and authenticity .': {'boosted': 10.0,\n",
       "  u'gold': u'Film',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.055180549621582,\n",
       "   u'Cinematic (EP)': 0,\n",
       "   u'Cinematic (Illy album)': 0,\n",
       "   u'Cinematic platformer': 0.8391621112823486,\n",
       "   u'Cinematic techniques': -0.8321423530578613,\n",
       "   u'Cinematic_(Adrian_Borland_album)': 0.5262945294380188,\n",
       "   u'Cinematography': -0.0035608382895588875,\n",
       "   u'Cut scene': 0.01348173152655363,\n",
       "   u'Film': 3.24212908744812,\n",
       "   u'Filmmaking': 0.5929996371269226,\n",
       "   u'Marvel Cinematic Universe': -0.19496628642082214,\n",
       "   u'XXNILXX': 0,\n",
       "   u'cinematic': 3.3595409393310547,\n",
       "   u'cinematic rock': 0}},\n",
       " u\"was a United States Marine Corps lance corporal who was [posthumous] ly presented the nation 's highest honor '' mdash ;\": {'boosted': 10.0,\n",
       "  u'gold': u'Posthumous recognition',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.0966310501098633,\n",
       "   u'Posthumous': 3.4187896251678467,\n",
       "   u'Posthumous (EP)': 0,\n",
       "   u'Posthumous name': -0.7106016874313354,\n",
       "   u'Posthumous recognition': 0.632470965385437,\n",
       "   u'Posthumous work': -0.08574287593364716,\n",
       "   u\"Shakespeare's reputation\": -2.14644193649292,\n",
       "   u'XXNILXX': 0,\n",
       "   u'posthumou': 0,\n",
       "   u'posthumous birth': -0.9997878074645996}},\n",
       " u'was conducted jointly by the CDC , , and the [Montgomery County] Health Department .': {'boosted': 10.0,\n",
       "  u'gold': u'Montgomery County, Pennsylvania',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.3754119873046875,\n",
       "   u'Counties of Estonia': -1.6058512926101685,\n",
       "   u'Counties of Hungary': -1.702496886253357,\n",
       "   u'Counties of Iran': -1.4176160097122192,\n",
       "   u'Counties of Ireland': -2.3920419216156006,\n",
       "   u'Counties of Norway': -0.3878001868724823,\n",
       "   u'Counties of Romania': -1.6344856023788452,\n",
       "   u'Counties of Sweden': -1.7354410886764526,\n",
       "   u\"Counties of the People's Republic of China\": -1.6497353315353394,\n",
       "   u'Montgomery County': 1.7186126708984375,\n",
       "   u'Montgomery County, Alabama': 1.3871185779571533,\n",
       "   u'Montgomery County, Arkansas': 0.41240277886390686,\n",
       "   u'Montgomery County, Maryland': 3.729935884475708,\n",
       "   u'Montgomery County, New York': 0.9158684015274048,\n",
       "   u'Montgomery County, Ohio': 0.3119455873966217,\n",
       "   u'Montgomery County, Pennsylvania': 2.539870023727417,\n",
       "   u'Montgomery County, Texas': 0.9795820713043213,\n",
       "   u'Montgomery County, Virginia': 0.9419838190078735,\n",
       "   u'Powiat': -1.7253388166427612,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"with measuring the earth on a global scale , and [surveying] -LRB- '' Ingenieurgeod sie '' -RRB- , which is concerned\": {'boosted': 10.0,\n",
       "  u'gold': u'Geophysical survey',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -1.7565046548843384,\n",
       "   u'Aerial survey': 1.8038055896759033,\n",
       "   u'Amphibious reconnaissance': -0.20321397483348846,\n",
       "   u'Archaeological field survey': 1.5602734088897705,\n",
       "   u'Astronomical survey': 0.46068406105041504,\n",
       "   u'Cave survey': 0.8697915077209473,\n",
       "   u'Geological survey': 0.723440945148468,\n",
       "   u'Geophysical survey': 0.9130355715751648,\n",
       "   u'Hydrographic survey': 1.3145627975463867,\n",
       "   u'Pacific Railroad Surveys': 0.6153885722160339,\n",
       "   u'Position fixing': 0.4630812406539917,\n",
       "   u'Public Land Survey System': -0.1723952740430832,\n",
       "   u'Quantity surveyor': -0.01613546349108219,\n",
       "   u'Survey methodology': 0.12919466197490692,\n",
       "   u'Wardriving': -0.02899896539747715,\n",
       "   u'XXNILXX': 0,\n",
       "   u'surveying': 5.304356575012207}},\n",
       " u'with the cult of an Islamic jinn ; a Roman [genius] ; as well as with an ancient Carthaginian site dedicated': {'boosted': 10.0,\n",
       "  u'gold': u'Genius (mythology)',\n",
       "  u'training': True,\n",
       "  u'vals': {u'-NIL-': -2.2549099922180176,\n",
       "   u'Enrico Fermi': 0.644614577293396,\n",
       "   u'Genius': 3.715977668762207,\n",
       "   u'Genius (1999 film)': -0.9169766902923584,\n",
       "   u'Genius (2012 film)': 0,\n",
       "   u'Genius (Krizz Kaliko album)': 0,\n",
       "   u'Genius (TV series)': -1.5550379753112793,\n",
       "   u'Genius (comics)': -1.2704209089279175,\n",
       "   u'Genius (mythology)': 1.3585917949676514,\n",
       "   u'Genius (song)': 0,\n",
       "   u'KYE Systems Corp.': -1.094813346862793,\n",
       "   u'MacArthur Fellows Program': 0.14419624209403992,\n",
       "   u'Mark E. Smith': 0.4187415838241577,\n",
       "   u'XXNILXX': 0,\n",
       "   u'geniu': -1.3247451782226562,\n",
       "   u'genius (literature)': 0.2829609811306}}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWrongItems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(4):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[linear_final_l.W, linear_final_l.b]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76039624],\n",
       "       [ 1.87864506],\n",
       "       [ 1.95898724],\n",
       "       [ 0.71105427],\n",
       "       [-0.02050496],\n",
       "       [ 0.03962371],\n",
       "       [-0.13201174]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_lin_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "findWrongItems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surface_counts['The Silent World of Nicholas Quinn'.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for i in range(5): print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2].get_value(borrow=True)[0] = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
