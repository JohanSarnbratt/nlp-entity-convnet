{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikipediaReader(object):\n",
    "    \n",
    "    title_rg = re.compile('.*<title>(.*)</title>.*')\n",
    "    link_rg = re.compile('\\[\\[([^\\]]*)\\]\\]')\n",
    "    redirect_rg = re.compile('.*<redirect title=\"(.*)\" />')\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        \n",
    "    def read(self):\n",
    "        current_page = None\n",
    "        look_for_next_page = True\n",
    "        page_text = None\n",
    "\n",
    "        title_rg = self.title_rg\n",
    "      \n",
    "        with open(self.fname) as f:\n",
    "            try:\n",
    "                while True:\n",
    "                    line = f.next()\n",
    "                    if look_for_next_page:\n",
    "                        if '<page>' not in line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            look_for_next_page = False\n",
    "                    if '<title>' in line:\n",
    "                        current_page = title_rg.match(line).group(1)\n",
    "                    elif '<redirect' in line:\n",
    "                        redirect_page = self.redirect_rg.match(line).group(1)\n",
    "                        self.readRedirect(current_page, redirect_page)\n",
    "                        look_for_next_page = True\n",
    "                    elif '<text' in line:\n",
    "                        lines = [ line[line.index('>')+2:] ]\n",
    "                        while True:\n",
    "                            line = f.next()\n",
    "                            if '</text>' in line:\n",
    "                                lines.append(line[:line.index('</text>')])\n",
    "                                look_for_next_page = True\n",
    "                                page_text = '\\n'.join(lines)\n",
    "                                self.readPage(current_page, page_text)\n",
    "                                break\n",
    "                            else:\n",
    "                                lines.append(line)\n",
    "            except StopIteration as e:\n",
    "                pass\n",
    "            \n",
    "    def getLinkTargets(self, content):\n",
    "        ret = self.link_rg.findall(content)\n",
    "        def s(v):\n",
    "            a = v.split('|')\n",
    "            pg = a[0].replace(' ', '_').lower()\n",
    "            txt = a[-1]\n",
    "            return pg, txt\n",
    "        return [s(r) for r in ret]\n",
    "            \n",
    "    def readPage(self, title, content):\n",
    "        pass\n",
    "    \n",
    "    def readRedirect(self, title, target):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "\n",
    "#wordvectors = WordVectors(fname='../GoogleNews-vectors-negative300.bin', negvectors=False)\n",
    "wordvectors = WordVectors(fname='../enwiki-20141208-pages-articles-multistream-links-output5.bin', negvectors=True, sentence_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850513"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvecs import EmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiLinkingExp(WikipediaReader):\n",
    "    \n",
    "    wiki_re = [\n",
    "        (re.compile('&amp;'), '&'),\n",
    "        (re.compile('&lt;'), '<'),\n",
    "        (re.compile('&gt;'), '>'),\n",
    "        (re.compile('<ref[^<]*<\\/ref>'), ''),\n",
    "        (re.compile('<.*?>'), ''),\n",
    "        (re.compile('\\[http:[^\\] ]*'), ''),\n",
    "        (re.compile('\\|(thumb|left|right|\\d+px)', re.IGNORECASE), ''),\n",
    "        (re.compile('\\[\\[image:[^\\[\\]]*\\|', re.IGNORECASE), ''),\n",
    "        (re.compile('\\[\\[category:([^|\\]]*)[^]]*\\]\\]', re.IGNORECASE), '\\\\1'),\n",
    "        (re.compile('\\[\\[[a-z\\-]*:[^\\]]\\]\\]'), ''),\n",
    "        (re.compile('\\[\\[[^\\|\\]]*\\|'), '[['),\n",
    "        (re.compile('{{[^}]*}}'), ''),\n",
    "        (re.compile('{[^}]*}'), ''),\n",
    "        (re.compile('[\\[|\\]]'), ''),\n",
    "        (re.compile('&[^;]*;'), ' '),\n",
    "        (re.compile('[^a-zA-Z0-9 ]'), ''),\n",
    "        (re.compile('\\n+'), ' ')\n",
    "        # TODO: clean up some remaining issues with parsing the wiki text\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _wikiToText(cls, txt):\n",
    "        txt = txt.lower()\n",
    "        for r in cls.wiki_re:\n",
    "            txt = r[0].sub(r[1], txt)\n",
    "        return txt\n",
    "    \n",
    "    run_training = False\n",
    "    num_words_to_use = 200  # set by the value set in the word vectors\n",
    "    batch_size = 2000\n",
    "    num_negative_samples = 1\n",
    "    num_words_per_conv = 3\n",
    "    \n",
    "    def __init__(self, fname, wordvecs=wordvectors):\n",
    "        super(WikiLinkingExp, self).__init__(fname)\n",
    "        self.wordvecs = wordvecs\n",
    "        self.current_batch = []\n",
    "        self.page_titles = set()\n",
    "        self.num_words_to_use = self.wordvecs.sentence_length\n",
    "        \n",
    "        # do an inital load of the data\n",
    "        self.read()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "        self.train_cnt = 0\n",
    "        self.train_res = []\n",
    "        \n",
    "    def _setup(self):\n",
    "        self.y_batch = T.ivector('y_labels')\n",
    "        self.x_words_batch = T.imatrix('x_words')\n",
    "        self.x_links_batch = T.imatrix('y_links')\n",
    "        \n",
    "        self.sentence_l = InputLayer((None, self.num_words_to_use), input_var=self.x_words_batch)\n",
    "        self.link_l = InputLayer((None,1), input_var=self.x_links_batch)\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix())\n",
    "        \n",
    "        self.sentence_emb_l = EmbeddingLayer(\n",
    "            self.sentence_l, \n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.link_emb_l = EmbeddingLayer(\n",
    "            self.link_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.sentence_conv_l = lasagne.layers.Conv2DLayer(\n",
    "            self.sentence_emb_l,\n",
    "            num_filters=150,\n",
    "            filter_size=(self.num_words_per_conv, self.wordvecs.vector_size),\n",
    "            name='conv_sent1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.sentence_pool_l = lasagne.layers.MaxPool2DLayer(\n",
    "            self.sentence_conv_l,\n",
    "            name='maxing_sent1',\n",
    "            pool_size=(self.num_words_to_use - self.num_words_per_conv, 1),\n",
    "        )\n",
    "        \n",
    "        self.combined_l = lasagne.layers.ConcatLayer(\n",
    "            (self.link_emb_l, self.sentence_pool_l,)\n",
    "        )\n",
    "        \n",
    "        self.dropped_l = lasagne.layers.DropoutLayer(\n",
    "            self.combined_l,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.dense1_l = lasagne.layers.DenseLayer(\n",
    "            self.dropped_l,\n",
    "            num_units=100,\n",
    "            name='dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.dropped2_l = lasagne.layers.DropoutLayer(\n",
    "            self.dense1_l,\n",
    "            p=.25\n",
    "        )\n",
    "        \n",
    "        self.out_l = lasagne.layers.DenseLayer(\n",
    "            self.dropped2_l,\n",
    "            num_units=2,\n",
    "            name='dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.output_vec = lasagne.layers.get_output(self.out_l)\n",
    "        self.result_vec = self.output_vec[:,0] - self.output_vec[:,1]\n",
    "        self.loss_vec = T.nnet.binary_crossentropy(T.clip(self.result_vec + .5, .001, .999), self.y_batch)\n",
    "        self.output_diff = T.neq(self.result_vec > 0, self.y_batch > .5)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(self.out_l)\n",
    "        self.updates = lasagne.updates.adagrad(self.loss_vec.mean(), self.all_params, .01)  # TODO: variable learning rate??\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_words_batch, self.x_links_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.output_diff.sum(), self.loss_vec.mean(), self.loss_vec],\n",
    "            updates=self.updates\n",
    "        )\n",
    "        \n",
    "        self.loss_func = theano.function(\n",
    "            [self.x_words_batch, self.x_links_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.loss_vec, self.output_diff.sum()]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.run_training = True\n",
    "        self.loss_sum = 0.0\n",
    "        self.diff_sum = 0\n",
    "        self.sample_cnt = 0\n",
    "        self.current_batch = []\n",
    "        \n",
    "        self.read()\n",
    "        \n",
    "        r = self.train_cnt, float(self.loss_sum) / self.sample_cnt, float(self.diff_sum) / self.sample_cnt\n",
    "        self.train_cnt += 1\n",
    "        self.train_res.append(r)\n",
    "        return r\n",
    "        tr\n",
    "        \n",
    "    def readPage(self, title, content):\n",
    "        # would be nice to use tf-idf here for the words from the document that should look at, but then won't have that much meanning....\n",
    "        links = self.getLinkTargets(content)\n",
    "        words = self._wikiToText(content).split()[:self.num_words_to_use]\n",
    "        wordsv = self.wordvecs.tokenize(words)\n",
    "        titlev = self.wordvecs.get_location(title)\n",
    "        linksv = self.wordvecs.tokenize(links)\n",
    "        if self.run_training:\n",
    "            for l in linksv:\n",
    "                self.current_batch.append((titlev, wordsv, l, 1))\n",
    "            for l in random.sample(self.page_titles, len(linksv)*self.num_negative_samples):\n",
    "                self.current_batch.append((titlev, wordsv, l, 0))\n",
    "            \n",
    "            if len(self.current_batch) >= self.batch_size:\n",
    "                pass  # TODO:\n",
    "        else:\n",
    "            self.page_titles.add(titlev)\n",
    "        \n",
    "    def train_batch(self):\n",
    "        labels = np.array([r[3] for r in self.current_batch]).astype('int32')\n",
    "        targets = np.array([[r[2]] for r in self.current_batch]).astype('int32')\n",
    "        words = np.array([r[1] for r in self.current_batch]).astype('int32')\n",
    "        \n",
    "        loss_sum, diff_sum, _, _ = self.train_func(words, targets, labels)\n",
    "        self.loss_sum += loss_sum\n",
    "        self.diff_sum += diff_sum\n",
    "        self.sample_cnt += len(self.current_batch)\n",
    "        self.current_batch = []\n",
    "\n",
    "        \n",
    "wikiexp = WikiLinkingExp('../enwiki-test-small.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikiexp.page_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-71ad0fb8643c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwikiexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-3d30ff460791>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_sum\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff_sum\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_cnt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_cnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "wikiexp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110400"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikiexp.current_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array([[r[2]] for r in wikiexp.current_batch]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiexp.train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
