{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikipediaReader(object):\n",
    "    \n",
    "    title_rg = re.compile('.*<title>(.*)</title>.*')\n",
    "    link_rg = re.compile('\\[\\[([^\\]]*)\\]\\]')\n",
    "    redirect_rg = re.compile('.*<redirect title=\"(.*)\" />')\n",
    "    not_link_match = re.compile('[^a-zA-Z0-9_]')    \n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        \n",
    "    def read(self):\n",
    "        current_page = None\n",
    "        look_for_next_page = True\n",
    "        page_text = None\n",
    "\n",
    "        title_rg = self.title_rg\n",
    "      \n",
    "        with open(self.fname) as f:\n",
    "            try:\n",
    "                while True:\n",
    "                    line = f.next()\n",
    "                    if look_for_next_page:\n",
    "                        if '<page>' not in line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            look_for_next_page = False\n",
    "                    if '<title>' in line:\n",
    "                        current_page = title_rg.match(line).group(1)\n",
    "                    elif '<redirect' in line:\n",
    "                        redirect_page = self.redirect_rg.match(line).group(1)\n",
    "                        self.readRedirect(current_page, redirect_page)\n",
    "                        look_for_next_page = True\n",
    "                    elif '<text' in line:\n",
    "                        lines = [ line[line.index('>')+2:] ]\n",
    "                        if '</text>' in lines[0]:\n",
    "                            page_text = lines[0][:lines[0].index('</text>')]\n",
    "                            look_for_next_page = True\n",
    "                            self.readPage(current_page, page_text)\n",
    "                        else:\n",
    "                            while True:\n",
    "                                line = f.next()\n",
    "                                if '</text>' in line:\n",
    "                                    lines.append(line[:line.index('</text>')])\n",
    "                                    look_for_next_page = True\n",
    "                                    page_text = '\\n'.join(lines)\n",
    "                                    self.readPage(current_page, page_text)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    lines.append(line)\n",
    "            except StopIteration as e:\n",
    "                pass\n",
    "    \n",
    "    @classmethod\n",
    "    def getLinkTargets(cls, content):\n",
    "        ret = cls.link_rg.findall(content)\n",
    "        def s(v):\n",
    "            a = v.split('|')\n",
    "            pg = a[0].replace(' ', '_').replace('(', '_lrb_').replace(')', '_rrb_').lower()\n",
    "            pg = cls.not_link_match.sub('', pg)\n",
    "            txt = a[-1]\n",
    "            if '://' not in v:\n",
    "                return pg, txt\n",
    "        return [a for a in [s(r) for r in ret] if a is not None]\n",
    "            \n",
    "    def readPage(self, title, content):\n",
    "        pass\n",
    "    \n",
    "    def readRedirect(self, title, target):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "\n",
    "#wordvectors = WordVectors(fname='../GoogleNews-vectors-negative300.bin', negvectors=False)\n",
    "wordvectors = WordVectors(\n",
    "    fname='/data/matthew/enwiki-20141208-pages-articles-multistream-links-output5.bin',\n",
    "    redir_fname='/data/matthew/enwiki-20141208-redirects.json',\n",
    "    negvectors=True,\n",
    "    sentence_length=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850513"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "org_wvectors = set(wordvectors.vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6594902"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.redirects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiLinkingExp(WikipediaReader):\n",
    "    \n",
    "    wiki_re = [\n",
    "        (re.compile('&amp;'), '&'),\n",
    "        (re.compile('&lt;'), '<'),\n",
    "        (re.compile('&gt;'), '>'),\n",
    "        (re.compile('<ref[^<]*<\\/ref>'), ''),\n",
    "        (re.compile('<.*?>'), ''),\n",
    "        (re.compile('\\[http:[^\\] ]*'), ''),\n",
    "        (re.compile('\\|(thumb|left|right|\\d+px)', re.IGNORECASE), ''),\n",
    "        (re.compile('\\[\\[image:[^\\[\\]]*\\|', re.IGNORECASE), ''),\n",
    "        (re.compile('\\[\\[category:([^|\\]]*)[^]]*\\]\\]', re.IGNORECASE), '\\\\1'),\n",
    "        (re.compile('\\[\\[[a-z\\-]*:[^\\]]\\]\\]'), ''),\n",
    "        (re.compile('\\[\\[[^\\|\\]]*\\|'), '[['),\n",
    "        (re.compile('{{[^}]*}}'), ''),\n",
    "        (re.compile('{[^}]*}'), ''),\n",
    "        (re.compile('[\\[|\\]]'), ''),\n",
    "        (re.compile('&[^;]*;'), ' '),\n",
    "        (re.compile('[^a-zA-Z0-9 ]'), ''),\n",
    "        (re.compile('\\n+'), ' ')\n",
    "        # TODO: clean up some remaining issues with parsing the wiki text\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _wikiToText(cls, txt):\n",
    "        txt = txt.lower()\n",
    "        for r in cls.wiki_re:\n",
    "            txt = r[0].sub(r[1], txt)\n",
    "        return txt\n",
    "    \n",
    "    run_training = False\n",
    "    num_words_to_use = 200  # set by the value set in the word vectors\n",
    "    batch_size = 20000\n",
    "    num_negative_samples = 1\n",
    "    num_words_per_conv = 3\n",
    "    \n",
    "    def __init__(self, fname, wordvecs=wordvectors):\n",
    "        super(WikiLinkingExp, self).__init__(fname)\n",
    "        self.wordvecs = wordvecs\n",
    "        self.current_batch = []\n",
    "        self.page_titles = set()\n",
    "        self.num_words_to_use = self.wordvecs.sentence_length\n",
    "        \n",
    "        # do an inital load of the data\n",
    "        self.read()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "        self.train_cnt = 0\n",
    "        self.train_res = []\n",
    "        \n",
    "    def _setup(self):\n",
    "        self.y_batch = T.ivector('y_labels')\n",
    "        self.x_words_batch = T.imatrix('x_words')\n",
    "        self.x_links_batch = T.imatrix('y_links')\n",
    "        \n",
    "        self.sentence_l = InputLayer((None, self.num_words_to_use), input_var=self.x_words_batch)\n",
    "        self.link_l = InputLayer((None,1), input_var=self.x_links_batch)\n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix())\n",
    "        \n",
    "        self.sentence_emb_l = EmbeddingLayer(\n",
    "            self.sentence_l, \n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.link_emb_l = EmbeddingLayer(\n",
    "            self.link_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.sentence_conv_l = lasagne.layers.Conv2DLayer(\n",
    "            self.sentence_emb_l,\n",
    "            num_filters=150,\n",
    "            filter_size=(self.num_words_per_conv, self.wordvecs.vector_size),\n",
    "            name='conv_sent1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.sentence_pool_l = lasagne.layers.MaxPool2DLayer(\n",
    "            self.sentence_conv_l,\n",
    "            name='maxing_sent1',\n",
    "            pool_size=(self.num_words_to_use - self.num_words_per_conv, 1),\n",
    "        )\n",
    "        \n",
    "        self.combined_l = lasagne.layers.ConcatLayer(\n",
    "            (lasagne.layers.FlattenLayer(self.link_emb_l), lasagne.layers.FlattenLayer(self.sentence_pool_l),)\n",
    "        )\n",
    "        \n",
    "        self.dropped_l = lasagne.layers.DropoutLayer(\n",
    "            self.combined_l,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.dense1_l = lasagne.layers.DenseLayer(\n",
    "            self.dropped_l,\n",
    "            num_units=100,\n",
    "            name='dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.dropped2_l = lasagne.layers.DropoutLayer(\n",
    "            self.dense1_l,\n",
    "            p=.25\n",
    "        )\n",
    "        \n",
    "        self.out_l = lasagne.layers.DenseLayer(\n",
    "            self.dropped2_l,\n",
    "            num_units=2,\n",
    "            name='dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        )\n",
    "        \n",
    "        self.output_vec = lasagne.layers.get_output(self.out_l)\n",
    "        self.result_vec = self.output_vec[:,0] - self.output_vec[:,1]\n",
    "        self.loss_vec = T.nnet.binary_crossentropy(T.clip(self.result_vec + .5, .001, .999), self.y_batch)\n",
    "        self.output_diff = T.neq(self.result_vec > 0, self.y_batch > .5)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(self.out_l)\n",
    "        self.updates = lasagne.updates.adagrad(self.loss_vec.mean(), self.all_params, .01)  # TODO: variable learning rate??\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_words_batch, self.x_links_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.output_diff.sum(), self.loss_vec.mean(), self.loss_vec],\n",
    "            updates=self.updates\n",
    "        )\n",
    "        \n",
    "        self.loss_func = theano.function(\n",
    "            [self.x_words_batch, self.x_links_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.loss_vec, self.output_diff.sum()]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.run_training = True\n",
    "        self.loss_sum = 0.0\n",
    "        self.diff_sum = 0\n",
    "        self.sample_cnt = 0\n",
    "        self.current_batch = []\n",
    "        \n",
    "        self.read()\n",
    "        if len(self.current_batch) > 0:\n",
    "            self.train_batch()\n",
    "        \n",
    "        r = self.train_cnt, float(self.loss_sum) / self.sample_cnt, float(self.diff_sum) / self.sample_cnt\n",
    "        self.train_cnt += 1\n",
    "        self.train_res.append(r)\n",
    "        return r\n",
    "        \n",
    "    def readPage(self, title, content):\n",
    "        # would be nice to use tf-idf here for the words from the document that should look at, but then won't have that much meanning....\n",
    "        links = [r[0] for r in self.getLinkTargets(content)]\n",
    "        words = self._wikiToText(content).split()[:self.num_words_to_use]\n",
    "        self.d_words = words\n",
    "        self.d_links = links\n",
    "        wordsv = self.wordvecs.tokenize(words)\n",
    "        self.d_wordsv = wordsv\n",
    "        titlev = self.wordvecs.get_location(title)\n",
    "        self.d_titlev = titlev\n",
    "        linksv = self.wordvecs.tokenize(links)\n",
    "        if self.run_training:\n",
    "            for l in linksv:\n",
    "                self.current_batch.append((titlev, wordsv, l, 1))\n",
    "            for l in random.sample(self.page_titles, len(linksv)*self.num_negative_samples):\n",
    "                self.current_batch.append((titlev, wordsv, l, 0))\n",
    "            \n",
    "            if len(self.current_batch) >= self.batch_size:\n",
    "                self.train_batch()\n",
    "        else:\n",
    "            self.page_titles.add(titlev)\n",
    "        \n",
    "    def train_batch(self):\n",
    "        labels = np.array([r[3] for r in self.current_batch]).astype('int32')\n",
    "        targets = np.array([[r[2]] for r in self.current_batch]).astype('int32')\n",
    "        words = np.array([r[1] for r in self.current_batch]).astype('int32')\n",
    "        \n",
    "        loss_sum, diff_sum, _, _ = self.train_func(words, targets, labels)\n",
    "        self.loss_sum += loss_sum\n",
    "        self.diff_sum += diff_sum\n",
    "        self.sample_cnt += len(self.current_batch)\n",
    "        self.current_batch = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "wikiexp = WikiLinkingExp('/data/matthew/enwiki-1e7_lines.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3921"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikiexp.page_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102947"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors) - 4850513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.reverse_word_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_words = list(set(wordvectors.vectors) - org_wvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'michael_gross__lrb_actor_rrb_',\n",
       " '1850alfheim',\n",
       " 'clamp__lrb_disambiguation_rrb_',\n",
       " 'concord1782',\n",
       " 'filealtair_8800_computerjpg',\n",
       " 'httpbabylon5warnerbroscom',\n",
       " 'filesc_oak_oti601_mozartjpg',\n",
       " 'abc__lrb_the_jackson_5_song_rrb_',\n",
       " 'battlecruisers6',\n",
       " 'concord1789',\n",
       " 'avocationfrom',\n",
       " 'periodsprehistoric',\n",
       " 'filered_wine_glassjpg',\n",
       " 'fileleredoutablephotojpg',\n",
       " 'parabolic_arc',\n",
       " 'imagenm142_x_3jpg',\n",
       " 'truncation__lrb_geometry_rrb_',\n",
       " 'certified_credit_professional',\n",
       " 'fileelectron_dotsvg',\n",
       " 'filebaburnama_illustrationjpg',\n",
       " 'category1890_deaths',\n",
       " 'khomska',\n",
       " u'college_of_cambridge',\n",
       " 'categorycaves_of_maharashtra',\n",
       " 'capitaltheir',\n",
       " 'filehkljpg',\n",
       " u'vim_(text_editor)',\n",
       " 'berlinoriginating',\n",
       " '2002_national_league_division_series',\n",
       " 'anglobretons',\n",
       " 'khomsky',\n",
       " 'james_o_eastland',\n",
       " 'personequivalence',\n",
       " 'johannes_palmberg',\n",
       " 'computerassisted_reviewing',\n",
       " u'wolf&#039;s_lair',\n",
       " 'countrysince',\n",
       " 'wood2',\n",
       " 'fileplan_amphipolisensvg',\n",
       " 'dual__lrb_cognitive_architecture_rrb_',\n",
       " 'cardiff_roman_fort',\n",
       " 'populationpopulationaccording',\n",
       " 'fileseagate_hard_diskjpg',\n",
       " u'lauda_(song)',\n",
       " 'george_t_knight__lrb_universalist_rrb_',\n",
       " 'albrecht_iii_elector_of_brandenburg',\n",
       " 'victor_ivan_santos',\n",
       " 'schsische_staatskapelle_dresden',\n",
       " '1997_florida_marlins_season',\n",
       " 'lastodiyear',\n",
       " 'filechateaudeversaillescourjpg',\n",
       " 'languagesbranchesthe',\n",
       " 'burkina_fasounited_states_relations',\n",
       " u'encyclopedia_of_the_history_of_arabic_science',\n",
       " 'claude_e_shannon',\n",
       " 'category1297_births',\n",
       " 'filebernardo_ochinopng',\n",
       " 'alexandria_egypt',\n",
       " 'list_of_legume_dishes',\n",
       " 'bortle_darksky_scale',\n",
       " 'ptbobina',\n",
       " 'brightestcetus',\n",
       " 'ayatananevertheless',\n",
       " 'highly_sensitive',\n",
       " '0201514591referencesexternal',\n",
       " 'fileantimony_massivejpg',\n",
       " 'englandisle',\n",
       " 'john_ellis__lrb_historian_rrb_',\n",
       " '198687_eredivisie',\n",
       " '1295in',\n",
       " 'invasion_of_cayenne__lrb_1809_rrb_',\n",
       " 'negro_national_league__lrb_19201931_rrb_',\n",
       " 'maria_marchioness_of_tortosa',\n",
       " 'bainbridge_ross_county_ohio',\n",
       " 'authoritycanon',\n",
       " 'filesyriac_estrangela_semkatsvg',\n",
       " 'bhagwan_mahavir_world_peace',\n",
       " 'talesbabylon',\n",
       " 'category970_births',\n",
       " 'timbergeographically',\n",
       " 'william_stanley__lrb_elizabethan_rrb_',\n",
       " 'rdxsvg',\n",
       " 'a_culture_of_conspiracy_apocalyptic_visions_in_contemporary_america',\n",
       " 'categorydivisor_function',\n",
       " u'senate_(burundi)',\n",
       " u'radovich_v._national_football_league',\n",
       " 'fileseal_of_alexios_komnenos_as_grand_domestic_of_the_westjpg',\n",
       " 'categoryenglish_game_show_hosts',\n",
       " u'bulgarians_in_south_america',\n",
       " '2008edbr',\n",
       " 'selfgovernance_of_singapore',\n",
       " u'mpeg-4_part_14',\n",
       " '2000densityus',\n",
       " 'preventionchief2name',\n",
       " '1e_065756',\n",
       " 'Circuit Park Zandvoort',\n",
       " 'battle_of_telamon__lrb_224_bc_rrb_',\n",
       " 'categorypeople_from_perth_and_kinross',\n",
       " 'phelon_amp_moorepanther_motorcycles',\n",
       " 'wolfgang_schreiber',\n",
       " 'filehallstatt_latenepng',\n",
       " 'meetingssituation',\n",
       " 'housefinlandia',\n",
       " 'clocks__lrb_song_rrb_',\n",
       " 'boatkindu',\n",
       " 'advanced_technical_intelligence_center',\n",
       " '153areasqmi',\n",
       " 'categoryparks_in_skagit_county_washington',\n",
       " 'concert_communications_services',\n",
       " 'filefriedrich_wilhelm_adam_sertuernerjpg',\n",
       " 'massimo__lrb_family_rrb_',\n",
       " 'filemassifmontblanc7438jpg',\n",
       " 'venusditto',\n",
       " u'data-flow_analysis',\n",
       " 'filerasterscansvg',\n",
       " 'history_of_tulsa_oklahomasecond_main_era_of_oil_production_19151930',\n",
       " 'larvaetaxonomythe',\n",
       " 'plakassociatedacts',\n",
       " 'brainsigns',\n",
       " 'battle_of_the_centaurs__lrb_michelangelo_rrb_',\n",
       " 'mozambique__lrb_portugal_rrb_',\n",
       " 'lifede',\n",
       " 'categorypopulated_places_established_in_1811',\n",
       " 'city_of_federal_subject_significance',\n",
       " 'categorypeople_from_poissy',\n",
       " 'categorytreaties_of_vietnam',\n",
       " 'grand_duke_alexander_alexandrovich_of_russia',\n",
       " 'filedeseret_small_engsvg',\n",
       " 'wideevery',\n",
       " 'filegodfreyknellerisaacnewton1689jpg',\n",
       " '15latm',\n",
       " u'religion_and_agriculture',\n",
       " '2014populationdensitykm2',\n",
       " 'imagecipro_250_mgjpg',\n",
       " u'roger_clinton,_sr.',\n",
       " 'deathsusan',\n",
       " 'firthraymond',\n",
       " 'major_league_baseball_allstar_gametie_games_rain_delays_and_homefield_advantage_in_world_series',\n",
       " 'chord__lrb_truss_construction_rrb_',\n",
       " u'king_william&#039;s_town',\n",
       " 'a_c_mcgiffert',\n",
       " 'cockpit__lrb_aviation_rrb_',\n",
       " 'fileibm_rs6000_aix_servers_ibmcom_1998__lrb_2_rrb_jpeg',\n",
       " 'modified_conditiondecision_coverage',\n",
       " 'asroma1415third',\n",
       " 'zeta_geminorum',\n",
       " 'wiktraison_dtre',\n",
       " 'theatre_directorthe_director_in_theatre_history',\n",
       " 'wars1628',\n",
       " 'devontorridgewest',\n",
       " 'softwaretechnology',\n",
       " 'math_forum',\n",
       " 'reconquista__lrb_chile_rrb_',\n",
       " 'categoryirishamerican_culture',\n",
       " 'wiktfreiheit',\n",
       " 'parliamentariansthe',\n",
       " 'fileaa_example2png',\n",
       " 'categorypuzzles',\n",
       " 'arcasthe',\n",
       " 'Politics of Dominica',\n",
       " 'picks_theorem',\n",
       " 'categorycanadian_surveyors',\n",
       " 'reliability_trial',\n",
       " 'filea_chronicle_of_england__page_057__alfred_plans_the_capture_of_the_danish_fleetjpg',\n",
       " 'squadron__lrb_cavalry_rrb_',\n",
       " 'a16_highway__lrb_australia_rrb_',\n",
       " 'category1667_births',\n",
       " 'double_recursion',\n",
       " 'allthuh',\n",
       " 'categorytrade_routes',\n",
       " 'm_butterfly__lrb_film_rrb_',\n",
       " 'chilean_national_plebiscite_1988',\n",
       " 'demarx_treitzsaurwein',\n",
       " 'godbeauty',\n",
       " 'imagecrookes_tube2_diagramsvg',\n",
       " 'wolfsbanesmonkshoods',\n",
       " 'love_letters__lrb_1945_film_rrb_',\n",
       " 'categorycorinth',\n",
       " u'whitefoot_(ward)',\n",
       " 'khatchaturov',\n",
       " 'mcdonnell_douglas_f4j_phantom_ii',\n",
       " 'Bj\\xc3\\xb6rn Borg',\n",
       " 'a4__lrb_croatia_rrb_',\n",
       " 'advent__lrb_publisher_rrb_',\n",
       " 'orange__lrb_brand_rrb_',\n",
       " 'knucklecracking',\n",
       " 'lorenz_sz_4042',\n",
       " 'acceptedcurrencycode',\n",
       " u'list_of_sri_lankan_sweets_and_desserts',\n",
       " '1884hydroponicumthe',\n",
       " 'anna_of_saxony__lrb_d_1512_rrb_',\n",
       " 'columbus__lrb_novel_rrb_',\n",
       " 'filequail_07_bg_041506jpg',\n",
       " 'Cayuga Lake',\n",
       " 'presentism__lrb_philosophy_of_time_rrb_',\n",
       " 'william_klein__lrb_photographer_rrb_',\n",
       " 'imagetruncated_dodecahedronpng',\n",
       " 'fileyeager_supersonic_flight_1947ogg',\n",
       " 'bentley_mulsanne__lrb_2010_rrb_']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors['bentley_mulsanne__lrb_2010_rrb_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiexp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(15):\n",
    "    wikiexp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.7031445864080539, 0.4959940066309615),\n",
       " (1, 0.6932051934312186, 0.49712190767661313),\n",
       " (2, 0.6931725602219134, 0.499164116296863),\n",
       " (3, 0.6931445327150977, 0.5001160418260647),\n",
       " (4, 0.6931527751636032, 0.4997162713593471),\n",
       " (5, 0.6931564770285159, 0.4989996174445295),\n",
       " (6, 0.693160117523893, 0.4988504208110176)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiexp.train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "620000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiexp.sample_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69308540504146954"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiexp.loss_sum / wikiexp.sample_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4994910810810811"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(wikiexp.diff_sum) / wikiexp.sample_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010488"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors) - 4850513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'anarchism',\n",
       " 'is',\n",
       " 'a',\n",
       " 'political',\n",
       " 'philosophy',\n",
       " 'that',\n",
       " 'advocates',\n",
       " 'stateless',\n",
       " 'societies',\n",
       " 'often',\n",
       " 'defined',\n",
       " 'as',\n",
       " 'selfgoverned',\n",
       " 'voluntary',\n",
       " 'institutions',\n",
       " 'but',\n",
       " 'several',\n",
       " 'authors',\n",
       " 'have',\n",
       " 'more',\n",
       " 'specific',\n",
       " 'based',\n",
       " 'on',\n",
       " 'nonhierarchical',\n",
       " 'free',\n",
       " 'associations',\n",
       " 'holds',\n",
       " 'the',\n",
       " 'state',\n",
       " 'to',\n",
       " 'be',\n",
       " 'undesirable',\n",
       " 'unnecessary',\n",
       " 'or',\n",
       " 'harmfulthe',\n",
       " 'following',\n",
       " 'sources',\n",
       " 'cite',\n",
       " 'while',\n",
       " 'antistatism',\n",
       " 'central',\n",
       " 'entails',\n",
       " 'opposing',\n",
       " 'authority',\n",
       " 'hierarchical',\n",
       " 'organisation',\n",
       " 'in',\n",
       " 'conduct',\n",
       " 'of',\n",
       " 'human',\n",
       " 'relations',\n",
       " 'including',\n",
       " 'not',\n",
       " 'limited',\n",
       " 'systemas',\n",
       " 'subtle',\n",
       " 'and',\n",
       " 'antidogmatic',\n",
       " 'draws',\n",
       " 'many',\n",
       " 'currents',\n",
       " 'thought',\n",
       " 'strategy',\n",
       " 'does',\n",
       " 'offer',\n",
       " 'fixed',\n",
       " 'body',\n",
       " 'doctrine',\n",
       " 'from',\n",
       " 'single',\n",
       " 'particular',\n",
       " 'world',\n",
       " 'view',\n",
       " 'instead',\n",
       " 'fluxing',\n",
       " 'flowing',\n",
       " 'there',\n",
       " 'are',\n",
       " 'types',\n",
       " 'traditions',\n",
       " 'all',\n",
       " 'which',\n",
       " 'mutually',\n",
       " 'exclusive',\n",
       " 'anarchist',\n",
       " 'schools',\n",
       " 'can',\n",
       " 'differ',\n",
       " 'fundamentally',\n",
       " 'supporting',\n",
       " 'anything',\n",
       " 'extreme',\n",
       " 'individualism',\n",
       " 'complete',\n",
       " 'collectivism',\n",
       " 'strains',\n",
       " 'been',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'categories',\n",
       " 'social',\n",
       " 'individualist',\n",
       " 'similar',\n",
       " 'dual',\n",
       " 'classifications',\n",
       " 'usually',\n",
       " 'considered',\n",
       " 'radical',\n",
       " 'leftwing',\n",
       " 'ideology',\n",
       " 'much',\n",
       " 'economics',\n",
       " 'legal',\n",
       " 'reflect',\n",
       " 'antiauthoritarian',\n",
       " 'interpretations',\n",
       " 'communism',\n",
       " 'syndicalism',\n",
       " 'mutualism',\n",
       " 'participatory',\n",
       " 'economicsthe',\n",
       " 'tendency',\n",
       " 'movement',\n",
       " 'has',\n",
       " 'represented',\n",
       " 'by',\n",
       " 'anarchocommunism',\n",
       " 'Anarchism',\n",
       " ('political_philosophy', 'political philosophy'),\n",
       " ('stateless_society', 'stateless societies'),\n",
       " ('self-governance', 'self-governed'),\n",
       " ('hierarchy', 'hierarchical'),\n",
       " ('free_association_(communism_and_anarchism)', 'free associations'),\n",
       " ('peter_kropotkin', 'Peter Kropotkin'),\n",
       " ('an_anarchist_faq', 'An Anarchist FAQ'),\n",
       " ('state_(polity)', 'state'),\n",
       " ('the_globe_and_mail', 'The Globe and Mail'),\n",
       " ('routledge_encyclopedia_of_philosophy',\n",
       "  'Routledge Encyclopedia of Philosophy'),\n",
       " ('authority', 'authority'),\n",
       " ('hierarchical_organisation', 'hierarchical organisation'),\n",
       " ('international_of_anarchist_federations',\n",
       "  'International of Anarchist Federations'),\n",
       " ('murray_bookchin', 'Murray Bookchin'),\n",
       " ('emma_goldman', 'Emma Goldman'),\n",
       " ('anarchism_and_other_essays', 'Anarchism and Other Essays'),\n",
       " ('benjamin_tucker', 'Benjamin Tucker'),\n",
       " ('george_woodcock', 'George Woodcock'),\n",
       " ('mikhail_bakunin', 'Mikhail Bakunin'),\n",
       " ('anarchist_schools_of_thought', 'Anarchist schools of thought'),\n",
       " ('individualism', 'individualism'),\n",
       " ('social_anarchism', 'social'),\n",
       " ('individualist_anarchism', 'individualist anarchism'),\n",
       " ('geoffrey_ostergaard', 'Ostergaard, Geoffrey'),\n",
       " ('the_new_york_times', 'The New York Times'),\n",
       " ('anarchist_economics', 'anarchist economics'),\n",
       " ('anarchist_law', 'anarchist legal philosophy'),\n",
       " ('libertarian_socialism', 'anti-authoritarian interpretations'),\n",
       " ('anarcho-communism', 'communism'),\n",
       " ('collectivist_anarchism', 'collectivism'),\n",
       " ('anarcho-syndicalism', 'syndicalism'),\n",
       " ('mutualism_(economic_theory)', 'mutualism'),\n",
       " ('participatory_economics', 'participatory economics'),\n",
       " ('anarchist_communism', 'anarcho-communism'),\n",
       " ('anarcho-syndicalism', 'anarcho-syndicalism'),\n",
       " ('alexandre_skirda', 'Skirda, Alexandre'),\n",
       " ('anarcho-communist', 'anarcho-communist'),\n",
       " ('anarcho-syndicalist', 'anarcho-syndicalist'),\n",
       " ('confederaci\\xc3\\xb3n_nacional_del_trabajo', 'CNT'),\n",
       " ('federico_urales', 'Federico Urales'),\n",
       " ('miguel_gimenez_igualada', 'Miguel Gimenez Igualada'),\n",
       " ('iberian_anarchist_federation', 'Iberian Anarchist Federation'),\n",
       " ('synthesis_anarchism', 'synthesist'),\n",
       " ('f\\xc3\\xa9d\\xc3\\xa9ration_anarchiste',\n",
       "  'F\\xc3\\xa9d\\xc3\\xa9ration Anarchiste'),\n",
       " ('charles-auguste_bontemps', 'Charles-Auguste Bontemps'),\n",
       " ('italian_anarchist_federation', 'Italian Anarchist Federation'),\n",
       " ('self-defense', 'self-defense'),\n",
       " ('non-violence', 'non-violence'),\n",
       " ('anarcho-pacifism', 'anarcho-pacifism'),\n",
       " ('coercion', 'coercive'),\n",
       " ('propaganda_of_the_deed', 'propaganda of the deed'),\n",
       " ('wikt:anarchism', 'anarchism'),\n",
       " ('anarchy', 'anarchy'),\n",
       " ('-ism', '-ism'),\n",
       " ('online_etymology_dictionary', 'Online etymology dictionary'),\n",
       " ('merriam-webster', 'Merriam-Webster'),\n",
       " ('privative', 'privative'),\n",
       " ('privative_alpha', '\\xe1\\xbc\\x80\\xce\\xbd'),\n",
       " ('archon', 'archon'),\n",
       " ('infinitive', 'infinitive'),\n",
       " ('maximilien_de_robespierre', 'Maximilien de Robespierre'),\n",
       " ('william_godwin', 'William Godwin'),\n",
       " ('wilhelm_weitling', 'Wilhelm Weitling'),\n",
       " ('pierre-joseph_proudhon', 'Pierre-Joseph Proudhon'),\n",
       " ('daniel_gu\\xc3\\xa9rin', 'Daniel Gu\\xc3\\xa9rin'),\n",
       " ('foundation_for_economic_education', 'Foundation for Economic Education'),\n",
       " ('dwight_macdonald', 'Dwight Macdonald'),\n",
       " ('george_woodcock', 'Woodcock, George'),\n",
       " ('libertarianism', 'libertarianism'),\n",
       " ('libertarian_anarchism', 'libertarian anarchism'),\n",
       " ('market_economy', 'market society')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors.reverse_word_location[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
