{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import EmbeddingLayer, InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors\n",
    "\n",
    "wordvectors = WordVectors(fname=\"../GoogleNews-vectors-negative300.bin\", negvectors=False)\n",
    "\n",
    "from sentiment_sents import Sentiment\n",
    "\n",
    "# just load the sentences from the CNN system\n",
    "sentiment = Sentiment(\"prevwork/CNN_sentence/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "class SentimentExp(object):\n",
    "    \n",
    "    def __init__(self, train_X, train_Y, wordvecs=wordvectors):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.wordvecs = wordvecs\n",
    "        \n",
    "        self.input_size = 10\n",
    "        self.batch_size = 10\n",
    "        \n",
    "        self.learning_rate = .01\n",
    "        self.momentum = .9\n",
    "        \n",
    "        self.train_X_rep = np.array([[self.getRep(x)] for x in self.train_X])\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "    def getRep(self, sent):\n",
    "        ret = []\n",
    "        for i in xrange(self.input_size):\n",
    "            if i < len(sent):\n",
    "                ret.append(self.wordvecs[sent[i]])\n",
    "            else:\n",
    "                ret.append(np.zeros(self.wordvecs.vector_size))\n",
    "        return np.matrix(ret).reshape((1, self.input_size, self.wordvecs.vector_size))\n",
    "\n",
    "    def _setup(self):\n",
    "        self.x_batch = T.tensor4('x')\n",
    "        self.y_batch = T.ivector('y')\n",
    "        \n",
    "        self.input_l = lasagne.layers.InputLayer((self.batch_size, 1, self.input_size, self.wordvecs.vector_size))\n",
    "        \n",
    "        self.first_l = lasagne.layers.Conv2DLayer(\n",
    "            self.input_l,\n",
    "            num_filters=100,\n",
    "            filter_size=(2, self.wordvecs.vector_size),\n",
    "            name='conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.first_l_max = lasagne.layers.MaxPool2DLayer(\n",
    "            self.first_l,\n",
    "            pool_size=(1,9)\n",
    "        )\n",
    "        \n",
    "        self.hidden1_l = lasagne.layers.DenseLayer(\n",
    "            self.first_l_max,\n",
    "            num_units=50,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.hidden1_l_drop = lasagne.layers.DropoutLayer(\n",
    "            self.hidden1_l,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.out_l = lasagne.layers.DenseLayer(\n",
    "            self.hidden1_l_drop,\n",
    "            num_units=1,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.output = lasagne.layers.get_output(self.out_l, self.x_batch)\n",
    "        \n",
    "        self.loss_vec_old = (self.output.reshape((self.output.size,)) - self.y_batch) ** 2\n",
    "        self.output_diff = T.neq((self.output.flatten() > .5),(self.y_batch > .5)).sum()\n",
    "        self.loss_vec = lasagne.objectives.binary_crossentropy(T.clip(self.output.reshape((self.output.size,)), .01, .99), self.y_batch)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(self.out_l)\n",
    "        \n",
    "        self.updates = lasagne.updates.adagrad(self.loss_vec.mean(), self.all_params, .01)\n",
    "        #self.updates = lasagne.updates.apply_momentum(self.updates_adagrad)\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            [self.x_batch, self.y_batch],\n",
    "            [self.loss_vec.mean(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "        )\n",
    "        \n",
    "        self.loss_func = theano.function(\n",
    "            [self.x_batch, self.y_batch],\n",
    "            [self.loss_vec.sum(), self.loss_vec, self.output_diff],\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        for s in xrange(0, len(self.train_X_rep), self.batch_size):\n",
    "            X_vals = np.array(self.train_X_rep[s:(s + self.batch_size)])\n",
    "            y_vals = np.array(self.train_Y[s:(s + self.batch_size)]).astype('int32')\n",
    "            loss, _ = self.train_func(X_vals, y_vals)\n",
    "            \n",
    "    def test_loss(self, test_X, test_Y):\n",
    "        test_X_rep = np.array([[self.getRep(x)] for x in test_X])\n",
    "        loss_sum = 0.0\n",
    "        wrong = 0.0\n",
    "        for s in xrange(0, len(test_X_rep), self.batch_size):\n",
    "            X_vals = np.array(self.train_X_rep[s:(s + self.batch_size)])\n",
    "            y_vals = np.array(self.train_Y[s:(s + self.batch_size)]).astype('int32')\n",
    "            loss, _, output_diff = self.loss_func(X_vals, y_vals)\n",
    "            wrong += output_diff\n",
    "            loss_sum += loss\n",
    "        return loss_sum / len(test_X_rep), wrong / len(test_X_rep)\n",
    "    \n",
    "experiment = SentimentExp(sentiment.train_X, sentiment.train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9271358860780028, 0.49742086752637749)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(30):\n",
    "    experiment.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.66375977811338371, 0.41031652989449002)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.train_X, sentiment.train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.66549026150221469, 0.40726846424384527)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65781617586048158, 0.4033997655334115) (0.65864224859370946, 0.4033997655334115)\n",
      "(0.65284003624535103, 0.39214536928487692) (0.65007462346657308, 0.38065650644783117)\n",
      "(0.64309654065091437, 0.37409144196951932) (0.64461577900407996, 0.37268464243845251)\n",
      "(0.63415913546176195, 0.36354044548651815) (0.63584843480132358, 0.36318874560375147)\n",
      "(0.62133706366877117, 0.35252051582649474) (0.61819202019044595, 0.35076201641266119)\n",
      "(0.61186462088244442, 0.34138335287221572) (0.61136543486472883, 0.3399765533411489)\n",
      "(0.60211213987674261, 0.33024618991793669) (0.60446660743327818, 0.33259085580304809)\n",
      "(0.58853938584680199, 0.31254396248534583) (0.59016738084616105, 0.32051582649472449)\n",
      "(0.57889038805932092, 0.30996483001172331) (0.58027425585488246, 0.31359906213364597)\n",
      "(0.56627723388382178, 0.29683470105509963) (0.56742439470765882, 0.30386869871043376)\n"
     ]
    }
   ],
   "source": [
    "for a in xrange(10):\n",
    "    for i in xrange(30):\n",
    "        experiment.train()\n",
    "    print experiment.test_loss(sentiment.train_X, sentiment.train_Y), experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in xrange(10):\n",
    "    for i in xrange(30):\n",
    "        experiment.train()\n",
    "    print experiment.test_loss(sentiment.train_X, sentiment.train_Y), experiment.test_loss(sentiment.test_X, sentiment.test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_func = theano.function(\n",
    "    [experiment.x_batch, experiment.y_batch],\n",
    "    [experiment.loss_vec.mean(), experiment.loss_vec, experiment.output, \n",
    "     T.grad(experiment.loss_vec.mean(), experiment.out_l.get_params()[0]),\n",
    "     experiment.out_l.get_params()[0], experiment.y_batch, \n",
    "     #lasagne.layers.get_output(experiment.first_l, experiment.x_batch)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.512237808911631),\n",
       " array([ 0.58180769,  0.2986674 ,  0.01005034,  0.30346001,  0.97740184,\n",
       "         0.92071332,  0.36261844,  0.52049098,  0.31382833,  0.83333974]),\n",
       " array([[ 0.55888716],\n",
       "        [ 0.74180609],\n",
       "        [-0.02160361],\n",
       "        [ 0.73825941],\n",
       "        [ 0.62371252],\n",
       "        [ 0.60176513],\n",
       "        [ 0.69585189],\n",
       "        [ 0.59422872],\n",
       "        [ 0.73064444],\n",
       "        [ 0.43459542]]),\n",
       " array([[-0.1001953 ],\n",
       "        [-0.05368159],\n",
       "        [-0.13112429],\n",
       "        [-0.29577643],\n",
       "        [ 0.11312037],\n",
       "        [-0.16495259],\n",
       "        [-0.09471503],\n",
       "        [-0.31290631],\n",
       "        [-0.02202011],\n",
       "        [-0.04184033],\n",
       "        [ 0.33743156],\n",
       "        [ 0.0953935 ],\n",
       "        [ 0.37693405],\n",
       "        [-0.1261617 ],\n",
       "        [-0.31001282],\n",
       "        [-0.09407441],\n",
       "        [-0.01603207],\n",
       "        [ 0.03523483],\n",
       "        [-0.28481946],\n",
       "        [ 0.28543564],\n",
       "        [ 0.03463661],\n",
       "        [ 0.09321152],\n",
       "        [ 0.53869045],\n",
       "        [ 0.54057898],\n",
       "        [ 0.19591136],\n",
       "        [ 0.10608613],\n",
       "        [-0.06206725],\n",
       "        [ 0.75469908],\n",
       "        [ 0.40794675],\n",
       "        [-0.33006513],\n",
       "        [-0.05392863],\n",
       "        [ 0.0120901 ],\n",
       "        [ 0.26725183],\n",
       "        [ 0.07622103],\n",
       "        [-0.26185877],\n",
       "        [-0.18341788],\n",
       "        [-0.20857736],\n",
       "        [-0.16111269],\n",
       "        [ 0.16760234],\n",
       "        [-0.12767818],\n",
       "        [ 0.08177532],\n",
       "        [ 0.08238657],\n",
       "        [-0.20998697],\n",
       "        [ 0.08814245],\n",
       "        [-0.34431877],\n",
       "        [ 0.05732444],\n",
       "        [-0.05542198],\n",
       "        [ 0.65578781],\n",
       "        [-0.19639453],\n",
       "        [ 0.13576821]]),\n",
       " array([[-0.06899973],\n",
       "        [-0.10581331],\n",
       "        [ 0.05537106],\n",
       "        [-0.06183905],\n",
       "        [-0.06940733],\n",
       "        [-0.08401966],\n",
       "        [ 0.06789562],\n",
       "        [-0.07303723],\n",
       "        [-0.03875243],\n",
       "        [-0.12476217],\n",
       "        [-0.05222287],\n",
       "        [ 0.05933965],\n",
       "        [-0.07029432],\n",
       "        [ 0.05573864],\n",
       "        [-0.0344918 ],\n",
       "        [ 0.05102806],\n",
       "        [ 0.06188749],\n",
       "        [-0.09443296],\n",
       "        [ 0.07689063],\n",
       "        [-0.07822788],\n",
       "        [ 0.07853906],\n",
       "        [ 0.08751604],\n",
       "        [-0.04782302],\n",
       "        [-0.05697691],\n",
       "        [-0.05813   ],\n",
       "        [ 0.10818347],\n",
       "        [-0.05698897],\n",
       "        [-0.05847661],\n",
       "        [-0.05371098],\n",
       "        [ 0.06913483],\n",
       "        [-0.06115504],\n",
       "        [-0.06156155],\n",
       "        [-0.07971827],\n",
       "        [-0.08016819],\n",
       "        [ 0.06899803],\n",
       "        [ 0.06064925],\n",
       "        [ 0.05711848],\n",
       "        [ 0.03316941],\n",
       "        [ 0.04328091],\n",
       "        [-0.05714878],\n",
       "        [ 0.04716154],\n",
       "        [-0.05013402],\n",
       "        [ 0.06555512],\n",
       "        [ 0.0747276 ],\n",
       "        [ 0.05935291],\n",
       "        [ 0.0435267 ],\n",
       "        [ 0.06793583],\n",
       "        [ 0.06635798],\n",
       "        [ 0.07450909],\n",
       "        [-0.07913106]]),\n",
       " array([1, 1, 0, 1, 0, 0, 1, 1, 1, 1], dtype=int32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_func(np.array(experiment.train_X_rep[0:10]),np.array(experiment.train_Y[0:10]).astype('int32'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
