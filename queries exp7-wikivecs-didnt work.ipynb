{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change it to not use the document when looking for the link quality, to see how much that is actually helping\n",
    "\n",
    "also changing loss to be `gold - (max {other} + c)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer\n",
    "import json\n",
    "import re\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links5-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs, length=200)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                wordvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content):\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613263"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "    \n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "    \n",
    "    def __init__(self): #, wikipedia_dump_fname, wordvec=wordvectors, queries=queries, redirects=page_redirects):\n",
    "        #self.wordvecs = wordvec\n",
    "        #self.queries = queries\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        self.num_words_to_use_conv = 5\n",
    "        #self.redirects = redirects\n",
    "        #self.page_content = {}\n",
    "        #self.wikipedia_dump_fname = wikipedia_dump_fname\n",
    "        \n",
    "        #self._process_queries()\n",
    "        \n",
    "        self._setup()\n",
    "        \n",
    "#     def _process_queries(self):\n",
    "#         queried_pages = set()\n",
    "#         for docs, q in self.queries.iteritems():\n",
    "#             self.wordvecs.tokenize(docs)\n",
    "#             for sur, v in q.iteritems():\n",
    "#                 self.wordvecs.tokenize(sur)\n",
    "#                 for link in v['vals'].keys():\n",
    "#                     self.wordvecs.tokenize(link)\n",
    "#                     tt = WikiRegexes.convertToTitle(link)\n",
    "#                     #self.wordvecs.tokenize(tt)\n",
    "#                     queried_pages.add(tt)\n",
    "\n",
    "#         added_pages = set()\n",
    "#         for title in queried_pages:\n",
    "#             if title in self.redirects:\n",
    "#                 #self.wordvecs.tokenize(self.redirects[title])\n",
    "#                 added_pages.add(self.redirects[title])\n",
    "#         queried_pages |= added_pages\n",
    "        \n",
    "#         self.queried_pages = queried_pages\n",
    "                \n",
    "#         class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "            \n",
    "#             def readPage(ss, title, content):\n",
    "#                 tt = ss.convertToTitle(title)\n",
    "#                 if tt in queried_pages:\n",
    "#                     cnt = ss._wikiToText(content)\n",
    "#                     self.page_content[tt] = self.wordvecs.tokenize(cnt)\n",
    "        \n",
    "#         GetWikipediaWords(self.wikipedia_dump_fname).read()\n",
    "               \n",
    "        \n",
    "    def _setup(self):\n",
    "        #self.x_document_input = T.imatrix('x_doc')\n",
    "        \n",
    "        #self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "        \n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "        \n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        self.y_grouping = T.imatrix('y_grouping')\n",
    "        \n",
    "        \n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX))\n",
    "        \n",
    "#         self.document_l = lasagne.layers.InputLayer(\n",
    "#             (None,self.document_length), \n",
    "#             input_var=self.x_document_input\n",
    "#         )\n",
    "    \n",
    "#         self.document_embedding_l = EmbeddingLayer(\n",
    "#             self.document_l,\n",
    "#             W=self.embedding_W,\n",
    "#             add_word_params=False,\n",
    "#         )\n",
    "        \n",
    "#         self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.document_embedding_l,\n",
    "#             num_filters=500,\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             name='document_conv1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "#             self.document_conv1_l,\n",
    "#             name='document_pool1',\n",
    "#             pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "#             mode='sum',\n",
    "#         )\n",
    "\n",
    "#         self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.document_max_l,\n",
    "#             num_units=250,\n",
    "#             name='doucment_dens1',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         document_output_length = 200\n",
    "        \n",
    "#         self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop1,\n",
    "#             num_units=225,\n",
    "#             name='document_dens2',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens2,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop2,\n",
    "#             num_units=document_output_length,\n",
    "#             name='document_dens3',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.document_output = lasagne.layers.get_output(self.document_dens3)\n",
    "        \n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "        \n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "        \n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "                \n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length), \n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "        \n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "        \n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "        \n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "        \n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.document_aligned_l = InputLayer(\n",
    "#             (None, document_output_length),\n",
    "#             input_var=self.document_output[self.x_document_id,:]\n",
    "#         )\n",
    "        \n",
    "        ##############################################\n",
    "        ## changed to not use the documented\n",
    "        \n",
    "#         self.source_l = lasagne.layers.ConcatLayer(\n",
    "#             [self.document_aligned_l, self.surface_dens1]\n",
    "#         )\n",
    "        \n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_dens1,   # CHANGED\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "        \n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "        \n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape(\n",
    "            (self.x_matches_surface.shape[0], 1, 1, 1)).astype(theano.config.floatX)\n",
    "                \n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "        \n",
    "        self.target_matched_surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None,1,1,1),\n",
    "            input_var=matched_surface_reshaped,\n",
    "        )\n",
    "        \n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_combined_feats_l = lasagne.layers.ConcatLayer(\n",
    "            [self.target_embedding_l, self.target_matched_surface_input_l],\n",
    "            axis=3\n",
    "        )\n",
    "        \n",
    "        self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "            (None,self.sentence_length),\n",
    "            input_var=self.x_target_words,\n",
    "        )\n",
    "        \n",
    "        self.target_words_embedding_l = EmbeddingLayer(\n",
    "            self.target_words_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "        \n",
    "        self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.target_words_embedding_l,\n",
    "            name='target_wrds_conv1',\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            num_filters=350,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.target_words_conv1_l,\n",
    "            name='target_wrds_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "        \n",
    "        self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "            [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "             lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "        )\n",
    "        \n",
    "        self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.target_merge_l,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=400,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "        \n",
    "        self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.target_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "        \n",
    "        self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.target_drop1,\n",
    "            name='target_wrds_dens1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_combined_feats_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "        \n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "        \n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "        \n",
    "        \n",
    "    \n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "        \n",
    "        \n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input],\n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "        \n",
    "        \n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "        \n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "        \n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "        \n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "        \n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "    \n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) * \n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "        \n",
    "        #self.golds = self.res_cap[self.y_answer]\n",
    "                \n",
    "#         def maxOverRange(indx):\n",
    "#             #return T.max(self.res_cap[T.arange(indx[0],indx[1])]) - self.res_cap[indx[2]]\n",
    "#             #return -( self.res_l[indx[2]] - T.log(T.exp(self.res_l[T.arange(indx[0],indx[1])]).sum()) )\n",
    "#             return -( self.res_l[indx[2]] - self.res_l[indx[0]])\n",
    "        \n",
    "#         # build a tensor to make a matrix with one set on each dimention\n",
    "#         self.grouped, grouped_update = theano.scan(maxOverRange, sequences=self.y_grouping)\n",
    "        \n",
    "        def setSubSelector(indx, outputs):\n",
    "            r = T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "            return T.set_subtensor(r[indx[2], indx[3]], 0)  # set the gold item to zero\n",
    "            #return T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "        \n",
    "        num_target_samples = self.res_l.shape[0]\n",
    "        \n",
    "        select_seq = T.concatenate([\n",
    "            self.y_grouping,\n",
    "            T.arange(self.y_grouping.shape[0]).reshape((self.y_grouping.shape[0], 1))\n",
    "        ], axis=1)\n",
    "        \n",
    "        self.selecting_matrix, _ = theano.scan(\n",
    "            setSubSelector,\n",
    "            outputs_info=T.zeros((num_target_samples, num_target_samples)),\n",
    "            #n_steps=self.y_grouping.shape[0]\n",
    "            sequences=select_seq,\n",
    "        )\n",
    "        \n",
    "        self.groupped_elems = T.dot(self.selecting_matrix[-1], T.diag(self.res_l))\n",
    "        #self.groupped_res = T.log(self.groupped_elems.sum(axis=0)[T.arange(self.y_grouping.shape[0])])\n",
    "        self.groupped_res = self.groupped_elems.max(axis=0)[T.arange(self.y_grouping.shape[0])]\n",
    "        self.loss_vec = T.max(self.groupped_res + 1 - self.res_l[self.y_grouping[:,2]], 0)\n",
    "        \n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff, \n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2)\n",
    "            #lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "        \n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "        \n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "        \n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "        \n",
    "        # this one works reasonably well\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "        \n",
    "        #self.loss_vec = self.grouped\n",
    "        \n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "        \n",
    "        self.updates = lasagne.updates.adadelta(self.loss_vec.mean(), self.all_params)\n",
    "        \n",
    "        self.func_inputs = [\n",
    "            #self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, #self.x_document_id,\n",
    "            self.x_target_input, self.x_matches_surface, self.x_link_id, \n",
    "            self.y_answer, self.y_grouping\n",
    "        ]  # self.x_target_words,\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "        \n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "        \n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.current_learning_groups = []\n",
    "        self.learning_targets = []\n",
    "        \n",
    "    def compute_batch(self, isTraining=True):\n",
    "        if isTraining:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        \n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "        \n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] != isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_gold_loc = -1\n",
    "                target_group_start = len(self.current_target_input)\n",
    "                for target in targets['vals'].keys():\n",
    "                    # skip the items that we don't know the gold for\n",
    "                    if not targets['gold'] and isTraining:\n",
    "                        continue\n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    cnt = self.wordvecs.get_location(WikiRegexes.convertToTitle(target))\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                if target_gold_loc is not None or not isTraining:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    gold_loc = (len(self.current_target_goal) + target_gold_loc)\n",
    "                    self.current_target_goal += [gold_loc] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                    target_group_end = len(self.current_target_input)\n",
    "                    self.current_learning_groups.append(\n",
    "                        [target_group_start, target_group_end,\n",
    "                         gold_loc])\n",
    "                \n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                #return\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links\n",
    "        \n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "            \n",
    "        return self.total_loss / self.total_links\n",
    "        \n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_vec = func(\n",
    "            #self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, #self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_target_id, \n",
    "            self.current_target_goal, self.current_learning_groups,  \n",
    "            # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            l[0]['vals'][ l[1] ] = res_vec[i]\n",
    "        self.reset_accums()\n",
    "        \n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "        \n",
    "\n",
    "        \n",
    "queries_exp = EntityVectorLinkExp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, {0, 1, 2, 3, 4})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_documents),set(queries_exp.current_link_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 87)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_surface_link),len(set(queries_exp.current_target_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_learning_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_exp.current_target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.190234110474\n",
      "CPU times: user 1h 31min 40s, sys: 25.4 s, total: 1h 32min 5s\n",
      "Wall time: 12min 10s\n"
     ]
    }
   ],
   "source": [
    "%time print queries_exp.compute_batch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28422, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28422, 0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28325, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28325, 0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(5):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True)))\n",
    "exp_results.append(('testing state', evalCurrentState(False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0012442890062829277),\n",
       " (1, 0.0012170055321185696),\n",
       " (2, 0.0011066949465393097),\n",
       " (3, 0.0010886187919973039),\n",
       " (4, 0.001062282717236833),\n",
       " ('testing run', 0.0011542985617437999)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 0.07273366127898806)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2846, 0.07273366127898806)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-67f61744c2ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevalCurrentState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-af4020d63a4f>\u001b[0m in \u001b[0;36mevalCurrentState\u001b[1;34m(trainingData, numSamples)\u001b[0m\n\u001b[0;32m     17\u001b[0m                         \u001b[0mall_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_measured\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_correct\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mall_measured\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "evalCurrentState(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517581, 0.1446556191204855)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(517581, 0.1446556191204855)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(True, 50000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28420, 0.6482758620689655)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28420, 0.6482758620689655)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 1000000\n",
    "\n",
    "exp_results.append('increase training item size')\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, 10000000)))\n",
    "\n",
    "for i in xrange(5):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, 10000000)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, 10000000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.13266118701557572),\n",
       " (1, 0.12355173346257406),\n",
       " (2, 0.11546770283164702),\n",
       " (3, 0.1092563206515393),\n",
       " (4, 0.10597121073958943),\n",
       " ('testing run', 0.10553999777445534),\n",
       " ('training state', (2837, 0.6669016566795911)),\n",
       " ('testing state', (2911, 0.6413603572655445)),\n",
       " 'increase training item size',\n",
       " ('testing run', 0.10553945666484769),\n",
       " ('testing state', (93705, 0.6441491916119737)),\n",
       " (0, 0.10374660023013943),\n",
       " (1, 0.10251044899899585),\n",
       " (2, 0.10196588812502484),\n",
       " (3, 0.10156279893654875),\n",
       " (4, 0.10114781341023653),\n",
       " ('testing run', 0.10223237171087719),\n",
       " ('training state', (517581, 0.29967483350432106)),\n",
       " ('testing state', (93705, 0.6602529214022731))]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.17593555299493149),\n",
       " (1, 0.15775220255413391),\n",
       " (2, 0.14848182698742496),\n",
       " (3, 0.14236914357040895),\n",
       " (4, 0.13783007839308653),\n",
       " (5, 0.13423812280731193),\n",
       " (6, 0.13121966899002982),\n",
       " (7, 0.12862972168658454),\n",
       " (8, 0.12631712098052536),\n",
       " (9, 0.12426717171407249),\n",
       " ('testing run', 0.1413455562823264),\n",
       " ('training state', (2837, 0.4532957349312654)),\n",
       " ('testing state', (2911, 0.4002061147372037))]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True)))\n",
    "exp_results.append(('testing state', evalCurrentState(False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(40):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True)))\n",
    "exp_results.append(('testing state', evalCurrentState(False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.17593555299493149),\n",
       " (1, 0.15775220255413391),\n",
       " (2, 0.14848182698742496),\n",
       " (3, 0.14236914357040895),\n",
       " (4, 0.13783007839308653),\n",
       " (5, 0.13423812280731193),\n",
       " (6, 0.13121966899002982),\n",
       " (7, 0.12862972168658454),\n",
       " (8, 0.12631712098052536),\n",
       " (9, 0.12426717171407249),\n",
       " ('testing run', 0.1413455562823264),\n",
       " ('training state', (2837, 0.4532957349312654)),\n",
       " ('testing state', (2911, 0.4002061147372037)),\n",
       " (0, 0.12236985757816354),\n",
       " (1, 0.12059972496635762),\n",
       " (2, 0.11895977317063225),\n",
       " (3, 0.1174358839507175),\n",
       " (4, 0.11597574485376827),\n",
       " (5, 0.11460117983657973),\n",
       " (6, 0.11325147423622688),\n",
       " (7, 0.111944676801859),\n",
       " (8, 0.11068471110399238),\n",
       " (9, 0.10949150958354345),\n",
       " (10, 0.10830747762660752),\n",
       " (11, 0.10713663977902753),\n",
       " (12, 0.10603463418394098),\n",
       " (13, 0.10496975296489068),\n",
       " (14, 0.10389930968826112),\n",
       " (15, 0.10287206584124167),\n",
       " (16, 0.10198909359236984),\n",
       " (17, 0.10101071914622954),\n",
       " (18, 0.10005361757370523),\n",
       " (19, 0.099186495055756704),\n",
       " ('testing run', 0.1268706949775481),\n",
       " ('training state', (2837, 0.511455763130067)),\n",
       " ('testing state', (2911, 0.436276193747853)),\n",
       " (0, 0.09849837920546764),\n",
       " (1, 0.097698546409034887),\n",
       " (2, 0.097075523181157308),\n",
       " (3, 0.096816848899850316),\n",
       " (4, 0.095847795627837795),\n",
       " (5, 0.095811814457839539),\n",
       " (6, 0.095286312285218985),\n",
       " (7, 0.095347366337436554),\n",
       " (8, 0.094968447271246551),\n",
       " (9, 0.094486658409979163),\n",
       " (10, 0.094518153559176421),\n",
       " (11, 0.09381142901907405),\n",
       " (12, 0.093892072727546377),\n",
       " (13, 0.094111169847651638),\n",
       " (14, 0.093616564648754952),\n",
       " (15, 0.093568483859272356),\n",
       " (16, 0.093905551900288098),\n",
       " (17, 0.093421156281244477),\n",
       " (18, 0.093096323674479994),\n",
       " (19, 0.093169388328459682),\n",
       " (20, 0.093711386090270118),\n",
       " (21, 0.093561076344735317),\n",
       " (22, 0.093002866433248552),\n",
       " (23, 0.092943069405748646),\n",
       " (24, 0.092529010040117279),\n",
       " (25, 0.092779144082206222),\n",
       " (26, 0.093137603244260014),\n",
       " (27, 0.092978746911643054),\n",
       " (28, 0.092823896228901512),\n",
       " (29, 0.092511161617749166),\n",
       " (30, 0.09208544358775167),\n",
       " (31, 0.092074376455745344),\n",
       " (32, 0.091859192424799341),\n",
       " (33, 0.092501059213462147),\n",
       " (34, 0.091677118410154135),\n",
       " (35, 0.091884870273230404),\n",
       " (36, 0.091736864211866956),\n",
       " (37, 0.09145859082632142),\n",
       " (38, 0.091433888039341937),\n",
       " (39, 0.091999758494181474),\n",
       " ('testing run', 0.12345066003482361),\n",
       " ('training state', (2837, 0.4934790271413465)),\n",
       " ('testing state', (2911, 0.42493988320164894))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28207, 0.21292586946502642)\n",
      "(28420, 0.0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "nan in some of the parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-185dc698c158>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mexp_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-e04089dc69d5>\u001b[0m in \u001b[0;36mcompute_batch\u001b[1;34m(self, isTraining)\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_target_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[1;31m#return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_links\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_training_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_links\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-e04089dc69d5>\u001b[0m in \u001b[0;36mrun_batch\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[1;31m# self.current_target_words,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         )\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_links\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_target_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-e04089dc69d5>\u001b[0m in \u001b[0;36mcheck_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mborrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nan in some of the parameters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: nan in some of the parameters"
     ]
    }
   ],
   "source": [
    "queries_exp.num_training_items = 500000\n",
    "exp_results.append('increse size to 500,000')\n",
    "\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, 500000)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, 500000)))\n",
    "\n",
    "for i in xrange(20):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "    \n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, 500000)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, 500000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(target_simple1.W, True),\n",
       " (target_simple1.b, True),\n",
       " (surface_cxt_conv1.W, True),\n",
       " (surface_cxt_conv1.b, True),\n",
       " (surface_conv1.W, True),\n",
       " (surface_conv1.b, True),\n",
       " (surface_dens1.W, True),\n",
       " (surface_dens1.b, True),\n",
       " (source_dens1.W, True),\n",
       " (source_dens1.b, True),\n",
       " (source_dens12.W, True),\n",
       " (source_dens12.b, True),\n",
       " (source_dens2.W, True),\n",
       " (source_dens2.b, True),\n",
       " (document_conv1.W, True),\n",
       " (document_conv1.b, True),\n",
       " (doucment_dens1.W, True),\n",
       " (doucment_dens1.b, True),\n",
       " (document_dens2.W, True),\n",
       " (document_dens2.b, True)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(v, np.isnan(v.get_value(borrow=True)).all()) for v in queries_exp.all_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "nan in some of the parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-ce744c8ab48d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-e04089dc69d5>\u001b[0m in \u001b[0;36mcheck_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mborrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_params\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nan in some of the parameters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: nan in some of the parameters"
     ]
    }
   ],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-d171a235d488>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqueries_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-185-faadc1d6e009>\u001b[0m in \u001b[0;36mcompute_batch\u001b[1;34m(self, isTraining)\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_links\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "queries_exp.compute_batch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qwerqwer():\n",
    "    tt = 0\n",
    "    for qu in queries_exp.queries.values():\n",
    "        if qu.values()[0]['training'] == True:\n",
    "            pass\n",
    "        else:\n",
    "            tt += 1\n",
    "#         for en in qu.values():\n",
    "#             for e in en:\n",
    "#                 if en['training'] is False:\n",
    "#                     tt += 1\n",
    "    return tt\n",
    "qwerqwer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evalCurrentState(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct = 0\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if all_trained > numSamples:\n",
    "                        break\n",
    "                    all_measured += 1\n",
    "                    all_trained += len(en['vals'].values())\n",
    "                    m = max(en['vals'].values())\n",
    "                    if en['vals'][en['gold']] == m and m != 0:\n",
    "                        all_correct += 1\n",
    "           \n",
    "    r = all_measured, float(all_correct) / all_measured\n",
    "    print r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91257, 0.0012492192379762648)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91257, 0.0012492192379762648)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalCurrentState(False, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def augmentTrainingData():\n",
    "    for quk in queries.keys():\n",
    "        qu = queries[quk]\n",
    "        for enk in qu.keys():\n",
    "            en = qu[enk]\n",
    "            if not en['gold']:\n",
    "                del qu[enk]\n",
    "        if not qu:\n",
    "            del queries[quk]\n",
    "    for qu in queries.values():\n",
    "        training = random.random() > .15\n",
    "        for en in qu.values():\n",
    "            en['training'] = training\n",
    "augmentTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[29].values()[0]['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u' [Kidnapping] charges were dropped again Vivian .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Chibok schoolgirls kidnapping': 0,\n",
       "   u'Kidnapping': 0}},\n",
       " u\", Vivian took Nicky away during Carly 's wedding to [Bo Brady] .\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Bo Brady': 0,\n",
       "   u'Brady': 0,\n",
       "   u'Brady Township, Butler County, Pennsylvania': 0,\n",
       "   u'Brady Township, Clarion County, Pennsylvania': 0,\n",
       "   u'Brady Township, Lycoming County, Pennsylvania': 0,\n",
       "   u'Brady family': 0,\n",
       "   u'Brady, Montana': 0,\n",
       "   u'Brady, Nebraska': 0,\n",
       "   u'Brady, Texas': 0,\n",
       "   u'Darren Brady': 0,\n",
       "   u'Garry Brady': 0,\n",
       "   u'Greg Brady (broadcaster)': 0,\n",
       "   u'Jon Brady': 0,\n",
       "   u'Liam Brady': 0,\n",
       "   u'Mathew Brady': 0,\n",
       "   u'Robbie Brady': 0,\n",
       "   u'Shannon Brady': 0,\n",
       "   u'Tom Brady': 0,\n",
       "   u'Tom Brady (rugby union)': 0}},\n",
       " u'After John went to [Lugano] with Danielle , she followed and discovered that Danielle was': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'2013\\u201314 FC Lugano season': 0,\n",
       "   u'BSI Challenger Lugano': 0,\n",
       "   u'Canton of Lugano': 0,\n",
       "   u'Club Atl\\xe9tico Lugano': 0,\n",
       "   u'Diego Lugano': 0,\n",
       "   u'F.C. Lugano': 0,\n",
       "   u'FC Lugano': 0,\n",
       "   u'HC Lugano': 0,\n",
       "   u'Lake Lugano': 0,\n",
       "   u'Lugano': 0,\n",
       "   u'Lugano Airport': 0,\n",
       "   u'Lugano District': 0,\n",
       "   u'Lugano Tigers': 0,\n",
       "   u'Lugano railway station': 0,\n",
       "   u'Memorial Mario Albisetti': 0,\n",
       "   u'Roman Catholic Diocese of Lugano': 0,\n",
       "   u'Trolleybuses in Lugano': 0,\n",
       "   u'University of Lugano': 0,\n",
       "   u'WTA Swiss Open': 0}},\n",
       " u\"At Shawn-Douglas Brady 's [birthday] party at the Penthouse Grill , Vivian lured Carly to\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Birthday': 0,\n",
       "   u'Birthday (Angel)': 0,\n",
       "   u'Birthday (Beatles song)': 0,\n",
       "   u'Birthday (ClariS album)': 0,\n",
       "   u'Birthday (Katy Perry song)': 0,\n",
       "   u'Birthday (Selena Gomez song)': 0,\n",
       "   u'Birthday (Taproot song)': 0,\n",
       "   u'Birthday (The Cr\\xfcxshadows EP)': 0,\n",
       "   u'Birthday (The Sugarcubes song)': 0,\n",
       "   u'Birthday (company)': 0,\n",
       "   u\"Buddha's Birthday\": 0,\n",
       "   u'Canada Day': 0,\n",
       "   u'Hebrew birthday': 0,\n",
       "   u'Martin Luther King, Jr. Day': 0,\n",
       "   u'Nativity of Mary': 0,\n",
       "   u'September 27': 0,\n",
       "   u'Vesak': 0,\n",
       "   u'Victoria Day': 0,\n",
       "   u\"Washington's Birthday\": 0}},\n",
       " u'He suspected [foul play] and began investigating the Alamains .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'American football plays': 0,\n",
       "   u'Crime': 0,\n",
       "   u'Cyrano de Bergerac (play)': 0,\n",
       "   u'Drama': 0,\n",
       "   u'Foul (sports)': 0,\n",
       "   u'Homicide': 0,\n",
       "   u'Murder': 0,\n",
       "   u'Play (2011 film)': 0,\n",
       "   u'Play (Jennifer Lopez song)': 0,\n",
       "   u'Play (Moby album)': 0,\n",
       "   u'Play (Namie Amuro album)': 0,\n",
       "   u'Play (UK magazine)': 0,\n",
       "   u'Play (US magazine)': 0,\n",
       "   u'Play (activity)': 0,\n",
       "   u'Play (group)': 0,\n",
       "   u'Play (theatre)': 0,\n",
       "   u'Play from scrimmage': 0,\n",
       "   u'Theatre': 0,\n",
       "   u'Word play': 0}},\n",
       " u'Her fall was somewhat broken by an [awning] down below and Vivian went into a coma .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0, u'Awning': 0}},\n",
       " u'Ivan brought Dr. Wu , a [Chinese] herbs specialist .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'China': 0,\n",
       "   u'Chinese American': 0,\n",
       "   u'Chinese Canadian': 0,\n",
       "   u'Chinese Indonesians': 0,\n",
       "   u'Chinese characters': 0,\n",
       "   u'Chinese cuisine': 0,\n",
       "   u'Chinese culture': 0,\n",
       "   u'Chinese language': 0,\n",
       "   u'Chinese mythology': 0,\n",
       "   u'Chinese people': 0,\n",
       "   u'Han Chinese': 0,\n",
       "   u'History of China': 0,\n",
       "   u'Malaysian Chinese': 0,\n",
       "   u'Mandarin Chinese': 0,\n",
       "   u'Overseas Chinese': 0,\n",
       "   u'Simplified Chinese characters': 0,\n",
       "   u'Standard Chinese': 0,\n",
       "   u'Traditional Chinese characters': 0,\n",
       "   u'Varieties of Chinese': 0}},\n",
       " u'Meanwhile , Vivian also took care of her friend , [Carly Manning] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Carly Manning': 0,\n",
       "   u'Charlie Manning': 0,\n",
       "   u'Danny Manning': 0,\n",
       "   u'Electoral district of Manning': 0,\n",
       "   u'Eli Manning': 0,\n",
       "   u'Ernest Manning': 0,\n",
       "   u'Jeremy Manning': 0,\n",
       "   u'Manning': 0,\n",
       "   u'Manning River': 0,\n",
       "   u'Manning, Alberta': 0,\n",
       "   u'Manning, Iowa': 0,\n",
       "   u'Manning, North Dakota': 0,\n",
       "   u'Manning, Oregon': 0,\n",
       "   u'Manning, South Carolina': 0,\n",
       "   u'Manning, Western Australia': 0,\n",
       "   u'Paul Manning (cyclist)': 0,\n",
       "   u'Peyton Manning': 0,\n",
       "   u'Raymond B. Manning': 0,\n",
       "   u'Sharon Manning': 0}},\n",
       " u'She and [Bo Brady] began questioning Vivian , but Vivian maintained that the baby': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Bo Brady': 0,\n",
       "   u'Brady': 0,\n",
       "   u'Brady Township, Butler County, Pennsylvania': 0,\n",
       "   u'Brady Township, Clarion County, Pennsylvania': 0,\n",
       "   u'Brady Township, Lycoming County, Pennsylvania': 0,\n",
       "   u'Brady family': 0,\n",
       "   u'Brady, Montana': 0,\n",
       "   u'Brady, Nebraska': 0,\n",
       "   u'Brady, Texas': 0,\n",
       "   u'Darren Brady': 0,\n",
       "   u'Garry Brady': 0,\n",
       "   u'Greg Brady (broadcaster)': 0,\n",
       "   u'Jon Brady': 0,\n",
       "   u'Liam Brady': 0,\n",
       "   u'Mathew Brady': 0,\n",
       "   u'Robbie Brady': 0,\n",
       "   u'Shannon Brady': 0,\n",
       "   u'Tom Brady': 0,\n",
       "   u'Tom Brady (rugby union)': 0}},\n",
       " u'She is the ex-wife of [Victor Kiriakis] , Steven Jones , and Stefano DiMera .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Kiriakis family': 0,\n",
       "   u'Philip Kiriakis': 0,\n",
       "   u'Victor Kiriakis': 0}},\n",
       " u'The doctors told her that she had a fatal [heart disease] and was going to die .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Atherosclerosis': 0,\n",
       "   u'Cardiology': 0,\n",
       "   u'Cardiovascular disease': 0,\n",
       "   u'Cardiovascular_disease': 0,\n",
       "   u'Coronary artery disease': 0,\n",
       "   u'Disease': 0,\n",
       "   u'Disease (The Ark song)': 0,\n",
       "   u'Disease (album)': 0,\n",
       "   u'Disease (song)': 0,\n",
       "   u'Genetic disorder': 0,\n",
       "   u'Globalization and disease': 0,\n",
       "   u'Hypertensive heart disease': 0,\n",
       "   u'Infection': 0,\n",
       "   u'Myocardial infarction': 0,\n",
       "   u'Neurological disorder': 0,\n",
       "   u'Occupational disease': 0,\n",
       "   u'Oral and maxillofacial pathology': 0,\n",
       "   u'Plant pathology': 0,\n",
       "   u'Tropical disease': 0}},\n",
       " u\"Vivian Alamain is a fictional villainess on the [soap opera] '' Days of Our Lives '' played by actress Louise\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Another World (TV series)': 0,\n",
       "   u'Chinese opera': 0,\n",
       "   u'Dramatic programming': 0,\n",
       "   u'French opera': 0,\n",
       "   u'Indian soap opera': 0,\n",
       "   u'Italian opera': 0,\n",
       "   u'Opera': 0,\n",
       "   u'Opera (Super Junior song)': 0,\n",
       "   u'Opera (film)': 0,\n",
       "   u'Opera (magazine)': 0,\n",
       "   u'Opera (song)': 0,\n",
       "   u'Opera (web browser)': 0,\n",
       "   u'Operatic pop': 0,\n",
       "   u'Philippine drama': 0,\n",
       "   u'Rebelde': 0,\n",
       "   u'Rock opera': 0,\n",
       "   u'Sadko (opera)': 0,\n",
       "   u'Soap opera': 0,\n",
       "   u'Telenovela': 0}},\n",
       " u'Vivian first appeared in Salem in [March 1992] when she arrived at the Alamain Mansion to find Alphonse': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'/March': 0,\n",
       "   u'1992': 0,\n",
       "   u'Aaron March': 0,\n",
       "   u'CMLL Super Viernes (March 2014)': 0,\n",
       "   u'March': 0,\n",
       "   u'March (music)': 0,\n",
       "   u'March (novel)': 0,\n",
       "   u'March (territorial entity)': 0,\n",
       "   u'March 1966': 0,\n",
       "   u'March 1971': 0,\n",
       "   u'March 1973': 0,\n",
       "   u'March 1975': 0,\n",
       "   u'March 2006': 0,\n",
       "   u'March District': 0,\n",
       "   u'March Engineering': 0,\n",
       "   u'March railway station': 0,\n",
       "   u'March, Cambridgeshire': 0,\n",
       "   u'Solomon March': 0,\n",
       "   u'Welsh Marches': 0}},\n",
       " u'While shopping around the hospital , Vivian noticed that [Caroline Brady] was in the hospital to cure a peptic ulcer .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Brady': 0,\n",
       "   u'Brady Township, Butler County, Pennsylvania': 0,\n",
       "   u'Brady Township, Clarion County, Pennsylvania': 0,\n",
       "   u'Brady Township, Lycoming County, Pennsylvania': 0,\n",
       "   u'Brady family': 0,\n",
       "   u'Brady, Montana': 0,\n",
       "   u'Brady, Nebraska': 0,\n",
       "   u'Brady, Texas': 0,\n",
       "   u'Caroline Brady': 0,\n",
       "   u'Darren Brady': 0,\n",
       "   u'Garry Brady': 0,\n",
       "   u'Greg Brady (broadcaster)': 0,\n",
       "   u'Jon Brady': 0,\n",
       "   u'Liam Brady': 0,\n",
       "   u'Mathew Brady': 0,\n",
       "   u'Robbie Brady': 0,\n",
       "   u'Shannon Brady': 0,\n",
       "   u'Tom Brady': 0,\n",
       "   u'Tom Brady (rugby union)': 0}},\n",
       " u'by an awning down below and Vivian went into a [coma] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Coma': 0,\n",
       "   u'Coma (1978 film)': 0,\n",
       "   u'Coma (Danish band)': 0,\n",
       "   u\"Coma (Guns N' Roses song)\": 0,\n",
       "   u'Coma (Max Sharam song)': 0,\n",
       "   u'Coma (Pendulum song)': 0,\n",
       "   u'Coma (South Korean miniseries)': 0,\n",
       "   u'Coma (U.S. miniseries)': 0,\n",
       "   u'Coma (band)': 0,\n",
       "   u'Coma (cometary)': 0,\n",
       "   u'Coma (musician)': 0,\n",
       "   u'Coma (novel)': 0,\n",
       "   u'Coma (optics)': 0,\n",
       "   u'Coma Berenices': 0,\n",
       "   u'Coma Cluster': 0,\n",
       "   u'Comet': 0,\n",
       "   u'Diabetic coma': 0,\n",
       "   u\"Drew's in a Coma\": 0,\n",
       "   u'Induced coma': 0}},\n",
       " u'got in an accident , but she was saved by [Billie Reed] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Adam Reed (footballer, born 1991)': 0,\n",
       "   u'Addison Reed': 0,\n",
       "   u'Billie Reed': 0,\n",
       "   u'Billie Reed (Days of our Lives)': 0,\n",
       "   u'Howie Reed': 0,\n",
       "   u'Jack Reed (rugby league)': 0,\n",
       "   u'Jamie Reed (footballer)': 0,\n",
       "   u'Jeff Reed (American football)': 0,\n",
       "   u'Lou Reed': 0,\n",
       "   u'Malcolm Reed': 0,\n",
       "   u'Phragmites': 0,\n",
       "   u'Reed': 0,\n",
       "   u'Reed College': 0,\n",
       "   u'Reed, Hertfordshire': 0,\n",
       "   u'Rick Reed (pitcher)': 0,\n",
       "   u'Stanley Forman Reed': 0,\n",
       "   u'Steve Reed (baseball)': 0,\n",
       "   u'Whitney Reed': 0,\n",
       "   u'Willis Reed': 0}},\n",
       " u'in the hospital and forged a few entries in her [diary] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Charles Pooter': 0,\n",
       "   u'Diary': 0,\n",
       "   u'Diary (Alicia Keys song)': 0,\n",
       "   u'Diary (Ralph Towner album)': 0,\n",
       "   u'Diary (Sunny Day Real Estate album)': 0,\n",
       "   u'Diary (TV series)': 0,\n",
       "   u'Diary (Thelma Aoyama album)': 0,\n",
       "   u'Diary (Tino Coury song)': 0,\n",
       "   u'Diary (film)': 0,\n",
       "   u'Diary (novel)': 0,\n",
       "   u'Diary (stationery)': 0,\n",
       "   u'Diary novel': 0,\n",
       "   u'Goebbels Diaries': 0,\n",
       "   u\"John Evelyn's Diary\": 0,\n",
       "   u'My Opposition': 0,\n",
       "   u'Nikki Bungaku': 0,\n",
       "   u'Poetic diary': 0,\n",
       "   u'Samuel Pepys': 0,\n",
       "   u'The Diary of a Young Girl': 0}},\n",
       " u\"jewel heist at one of the Alamain 's parties in [1984] .\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'1984': 0,\n",
       "   u'1984 Grand Prix motorcycle racing season': 0,\n",
       "   u'1984 NASCAR Winston Cup Series': 0,\n",
       "   u'1984 NBA draft': 0,\n",
       "   u'1984 NCAA Division I-A football season': 0,\n",
       "   u'1984 NFL season': 0,\n",
       "   u'1984 NHL Entry Draft': 0,\n",
       "   u'1984 Summer Olympics': 0,\n",
       "   u'1984 Winter Olympics': 0,\n",
       "   u'1984 in film': 0,\n",
       "   u'1984 in literature': 0,\n",
       "   u'1984 in music': 0,\n",
       "   u'1984 in television': 0,\n",
       "   u'1984 in video gaming': 0,\n",
       "   u'Australian federal election, 1984': 0,\n",
       "   u'Nineteen Eighty-Four': 0,\n",
       "   u'UEFA Euro 1984': 0,\n",
       "   u'United States House of Representatives elections, 1984': 0,\n",
       "   u'United States presidential election, 1984': 0}},\n",
       " u'one with detergent and one with a heavy dose of [morphine] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Blood on the Dance Floor: HIStory in the Mix': 0,\n",
       "   u'Come On/Morphine': 0,\n",
       "   u'Morphine': 0,\n",
       "   u'Morphine (band)': 0,\n",
       "   u'Morphine (film)': 0}},\n",
       " u\"opera '' Days of Our Lives '' played by actress [Louise Sorel] from 1992 until 2000 .\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Albert Sorel': 0,\n",
       "   u'Battle of Sorel': 0,\n",
       "   u'Georges Sorel': 0,\n",
       "   u'Louise Sorel': 0,\n",
       "   u'Saint-Joseph-de-Sorel, Quebec': 0,\n",
       "   u'Sorel (company)': 0,\n",
       "   u'Sorel (horse)': 0,\n",
       "   u'Sorel (liqueur)': 0,\n",
       "   u'Sorel \\xc9perviers': 0,\n",
       "   u'Sorel, Somme': 0,\n",
       "   u'Sorel-Tracy': 0}},\n",
       " u\"own , and bought her servant 's son 's birth [certificate] to cover up the entire matter .\": {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Academic Certificate': 0,\n",
       "   u'Academic certificate': 0,\n",
       "   u'Australian Qualifications Framework': 0,\n",
       "   u'British Board of Film Classification': 0,\n",
       "   u'Certificate (HETAC)': 0,\n",
       "   u'Certificate (complexity)': 0,\n",
       "   u'Certificate in Education': 0,\n",
       "   u'Certificate of Higher Education': 0,\n",
       "   u'Certificate of authenticity': 0,\n",
       "   u'Certification': 0,\n",
       "   u'Diploma': 0,\n",
       "   u'Graduate certificate': 0,\n",
       "   u'Higher National Certificate': 0,\n",
       "   u'Motion picture rating system': 0,\n",
       "   u'Pilot certification in the United States': 0,\n",
       "   u'Primality certificate': 0,\n",
       "   u'Professional certification': 0,\n",
       "   u'Public key certificate': 0,\n",
       "   u'Stock certificate': 0}},\n",
       " u'pure trouble , and Victor finally discovered him on his [yacht] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Armed yacht': 0,\n",
       "   u\"Disney's Yacht Club Resort\": 0,\n",
       "   u'Granma (yacht)': 0,\n",
       "   u'Jona Bechtolt': 0,\n",
       "   u'Livadia_(yacht,_1880)': 0,\n",
       "   u'Luxury yacht': 0,\n",
       "   u'SMY Hohenzollern': 0,\n",
       "   u'Sailboat': 0,\n",
       "   u'Sailing at the 1964 Summer Olympics': 0,\n",
       "   u'Sailing yacht': 0,\n",
       "   u'Steam yacht': 0,\n",
       "   u'USS Oneida (SP-432)': 0,\n",
       "   u'USS Sequoia (presidential yacht)': 0,\n",
       "   u'Yacht': 0,\n",
       "   u'Yacht (band)': 0,\n",
       "   u'Yacht (dice game)': 0,\n",
       "   u'Yacht club': 0,\n",
       "   u'Yacht issue': 0,\n",
       "   u'Yacht racing': 0}},\n",
       " u'that Caroline Brady was in the hospital to cure a [peptic ulcer] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'Cancer': 0,\n",
       "   u'Corneal ulcer': 0,\n",
       "   u'Cutaneous condition': 0,\n",
       "   u'Diabetic foot ulcer': 0,\n",
       "   u'Genital ulcer': 0,\n",
       "   u'Mouth ulcer': 0,\n",
       "   u'Peptic ulcer': 0,\n",
       "   u'Pressure ulcer': 0,\n",
       "   u'Ulcer': 0,\n",
       "   u'Ulcer (dermatology)': 0,\n",
       "   u'Ulcer_(dermatology)': 0,\n",
       "   u'Venous ulcer': 0}},\n",
       " u'the ex-wife of Victor Kiriakis , Steven Jones , and [Stefano DiMera] .': {u'gold': u'',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': 0,\n",
       "   u'DiMera family (Days of our Lives)': 0,\n",
       "   u'Santo DiMera': 0,\n",
       "   u'Stefano DiMera': 0}}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.values()[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "    queries_exp.func_inputs,\n",
    "    [queries_exp.loss_vec, queries_exp.source_out, queries_exp.target_out] + T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params),\n",
    "    updates=queries_exp.updates,\n",
    "    on_unused_input='ignore',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = gg_func(\n",
    "    queries_exp.current_documents,\n",
    "    queries_exp.current_surface_link, queries_exp.current_surface_context, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_matches_surface, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24814822, 0.24238527, 0.23680143)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0].max(), res1[0].mean(), res1[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  76.69731903,  172.44116211,  139.30476379, -182.91343689,\n",
       "        172.44116211,  172.44116211,  -13.79522324,   97.6740036 ,\n",
       "         11.84405231,   36.98944092], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res1[1][0], res1[2][0:10,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.24216416,  0.24294616,  0.24274334,  0.23833458,  0.24294616,\n",
       "        0.24294616,  0.24024701,  0.24193516,  0.24127102,  0.24146625], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.7740622e-06"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[3:][0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[surface_cxt_conv1.W,\n",
       " surface_cxt_conv1.b,\n",
       " surface_conv1.W,\n",
       " surface_conv1.b,\n",
       " surface_dens1.W,\n",
       " surface_dens1.b,\n",
       " source_dens1.W,\n",
       " source_dens1.b,\n",
       " source_dens12.W,\n",
       " source_dens12.b,\n",
       " source_dens2.W,\n",
       " source_dens2.b,\n",
       " document_conv1.W,\n",
       " document_conv1.b,\n",
       " doucment_dens1.W,\n",
       " doucment_dens1.b,\n",
       " document_dens2.W,\n",
       " document_dens2.b]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24253285"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.04426765,  -8.85146523,  -6.5927825 ,  -5.64352655,\n",
       "        -6.2810235 , -10.34654236,  -6.09899616,   1.8380394 ,\n",
       "        -3.23892975,  -6.04222679,  -8.91859531,   1.09592962,\n",
       "        -5.27822304, -12.33982086,  -5.90433264], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(res1[2][0,:],res1[1][0:15,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.4103508"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[2][0,:].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.82824624"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[1][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[0]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(v, np.isnan(v.get_value(borrow=True)).all()) for v in queries_exp.all_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_score],\n",
    "            T.grad(queries_exp.loss_vec.mean(), lasagne.layers.get_all_params(queries_exp.target_dens2)),\n",
    "#           T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params),\n",
    "    #             [queries_exp.target_out, queries_exp.source_aligned_l, \n",
    "#              T.dot(queries_exp.target_out, queries_exp.source_aligned_l.T).diagonal(),\n",
    "#              queries_exp.target_out.norm(2, axis=1) * queries_exp.source_aligned_l.norm(2, axis=1),\n",
    "#              T.batched_dot(queries_exp.target_out, queries_exp.source_aligned_l),\n",
    "#              lasagne.layers.get_output(queries_exp.target_dens2),\n",
    "#              queries_exp.target_out.norm(2, axis=1),\n",
    "#              #T.grad(queries_exp.loss_vec.mean(), queries_exp.all_params)\n",
    "#             ],\n",
    "        #[queries_exp.res_l, queries_exp.loss_vec.sum(), queries_exp.loss_vec],\n",
    "    on_unused_input='ignore',\n",
    "    mode='DebugMode'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_func = theano.function(\n",
    "            [queries_exp.x_document_input,\n",
    "             queries_exp.x_surface_text_input, queries_exp.x_document_id,\n",
    "             queries_exp.x_target_input, queries_exp.x_link_id, queries_exp.y_answer],\n",
    "            #[self.res_cap, self.loss_vec.sum(), self.loss_vec],\n",
    "            [queries_exp.res_cap],\n",
    "        mode='DebugMode',\n",
    "        on_unused_input='ignore',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gg_grad_res = gg_func(\n",
    "        queries_exp.current_documents,\n",
    "        queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "        queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    "    )\n",
    "except Exception as e:\n",
    "    eeee = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueError('Bad input argument to theano function with name \"<ipython-input-19-c6e818cc55a5>:8\"  at index 4(0-based)',\n",
       "           'setting an array element with a sequence.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.isnan(v).any() for v in gg_grad_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res = gg_func(\n",
    "    queries_exp.current_documents,\n",
    "    queries_exp.current_surface_text, queries_exp.current_link_id,\n",
    "    queries_exp.current_target_input, queries_exp.current_target_id, queries_exp.current_target_goal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(gg_res[5]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0].shape, gg_res[1].shape, gg_res[2].shape, gg_res[3].shape, gg_res[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.inner(gg_res[0], gg_res[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[0] * gg_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.dot(gg_res[0], gg_res[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa.diagonal().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gg_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.queried_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_link_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(5):\n",
    "    exp_results.append((i, queries_exp.compute_batch()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss / queries_exp.total_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time queries_exp.run_batch(queries_exp.test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
