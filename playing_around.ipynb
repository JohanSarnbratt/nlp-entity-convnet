{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import EmbeddingLayer, InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordvecs import WordVectors\n",
    "\n",
    "wordvectors = WordVectors(fname=\"../enwiki-20141208-pages-articles-multistream-links-output4.bin\", negvectors=True)\n",
    "\n",
    "from sentiment_sents import Sentiment\n",
    "\n",
    "# just load the sentences from the CNN system\n",
    "sentiment = Sentiment(\"prevwork/CNN_sentence/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6091842"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = InputLayer((3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.arange(3*5).reshape((3,5)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1 = EmbeddingLayer(l_in, input_size=3, output_size=5, W=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "output = get_output(l1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = theano.function([x], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([[0,2],[1,2]]).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   1.,   2.,   3.,   4.],\n",
       "        [ 10.,  11.,  12.,  13.,  14.]],\n",
       "\n",
       "       [[  5.,   6.,   7.,   8.,   9.],\n",
       "        [ 10.,  11.,  12.,  13.,  14.]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to attempt to make the simple linear reg working, just to experiment with theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61099448, -0.11652368,  2.05125713,  0.38902261, -0.90401012,\n",
       "        0.20400009, -1.50504069,  3.3007176 ,  0.13135793,  1.44839975])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfeats = 10\n",
    "\n",
    "\n",
    "x = T.matrix('x')\n",
    "y = T.vector('y')\n",
    "w = theano.shared(np.random.randn(nfeats),name='w')\n",
    "b = theano.shared(0.0,name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_1 = 1 / (1 + T.exp(-T.dot(x,w) - b))\n",
    "prediction = p_1 > 0.5\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)\n",
    "cost = xent.mean() + .01 * (w**2).sum()\n",
    "gw, gb = T.grad(cost, [w,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function(inputs=[x,y],outputs=[prediction,xent], updates=((w, w-0.1*gw),(b,b - 0.1*gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = (np.random.randn(400, nfeats), np.random.randint(size=400, low=0, high=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{Composite{GT(scalar_sigmoid(i0), i1)}} [@A] ''   15\n",
      " |Elemwise{neg,no_inplace} [@B] ''   13\n",
      " | |Elemwise{Composite{((-i0) - i1)}}[(0, 0)] [@C] ''   10\n",
      " |   |CGemv{inplace} [@D] ''   8\n",
      " |   | |AllocEmpty{dtype='float64'} [@E] ''   5\n",
      " |   | | |Shape_i{0} [@F] ''   1\n",
      " |   | |   |x [@G]\n",
      " |   | |TensorConstant{1.0} [@H]\n",
      " |   | |x [@G]\n",
      " |   | |w [@I]\n",
      " |   | |TensorConstant{0.0} [@J]\n",
      " |   |InplaceDimShuffle{x} [@K] ''   0\n",
      " |     |b [@L]\n",
      " |TensorConstant{(1,) of 0.5} [@M]\n",
      "Elemwise{Composite{((i0 * scalar_softplus(i1)) - (i2 * i3 * scalar_softplus(i4)))}}[(0, 1)] [@N] ''   17\n",
      " |y [@O]\n",
      " |Elemwise{Composite{((-i0) - i1)}}[(0, 0)] [@C] ''   10\n",
      " |TensorConstant{(1,) of -1.0} [@P]\n",
      " |Elemwise{sub,no_inplace} [@Q] ''   4\n",
      " | |TensorConstant{(1,) of 1.0} [@R]\n",
      " | |y [@O]\n",
      " |Elemwise{neg,no_inplace} [@B] ''   13\n",
      "CGemv{inplace} [@S] ''   19\n",
      " |w [@I]\n",
      " |TensorConstant{-0.1} [@T]\n",
      " |InplaceDimShuffle{1,0} [@U] 'x.T'   2\n",
      " | |x [@G]\n",
      " |Elemwise{Composite{((i0 * i1 * i2 * i3) + ((scalar_sigmoid((-i4)) * i5) / i6))}}[(0, 0)] [@V] ''   16\n",
      " | |Assert{msg='Theano Assert failed!'} [@W] ''   14\n",
      " | | |sigmoid [@X] ''   12\n",
      " | | | |Elemwise{Composite{((-i0) - i1)}}[(0, 0)] [@C] ''   10\n",
      " | | |Elemwise{eq,no_inplace} [@Y] ''   6\n",
      " | |   |Shape_i{0} [@Z] ''   3\n",
      " | |   | |y [@O]\n",
      " | |   |Shape_i{0} [@F] ''   1\n",
      " | |TensorConstant{(1,) of -1.0} [@P]\n",
      " | |Elemwise{inv,no_inplace} [@BA] ''   11\n",
      " | | |Elemwise{Cast{float64}} [@BB] ''   9\n",
      " | |   |InplaceDimShuffle{x} [@BC] ''   7\n",
      " | |     |Shape_i{0} [@Z] ''   3\n",
      " | |y [@O]\n",
      " | |Elemwise{Composite{((-i0) - i1)}}[(0, 0)] [@C] ''   10\n",
      " | |Elemwise{sub,no_inplace} [@Q] ''   4\n",
      " | |Elemwise{Cast{float64}} [@BB] ''   9\n",
      " |TensorConstant{0.998} [@BD]\n",
      "Elemwise{Composite{(i0 - (i1 * i2))}}[(0, 0)] [@BE] ''   20\n",
      " |b [@L]\n",
      " |TensorConstant{0.1} [@BF]\n",
      " |Sum{acc_dtype=float64} [@BG] ''   18\n",
      "   |Elemwise{Composite{((i0 * i1 * i2 * i3) + ((scalar_sigmoid((-i4)) * i5) / i6))}}[(0, 0)] [@V] ''   16\n"
     ]
    }
   ],
   "source": [
    "theano.printing.debugprint(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(10000):\n",
    "    pred, err = train(D[0], D[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67402486455032262"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82726345, -1.34371496, -2.04404689, -0.94441741, -0.11134401,\n",
       "        0.21794162,  0.3532185 , -0.59029774, -0.31446545,  0.52206763])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.05472259, -0.12900095, -0.19193356,  0.05842622, -0.06939324,\n",
       "        -0.01532577,  0.03732123, -0.18237269,  0.03827691,  0.09780588]),\n",
       " array(0.16938777211978998))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.get_value(), b.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([D[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going to see if we can make a few layers of a cnn given that went through the example of log reg from the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context(sentence):\n",
    "    ret = []\n",
    "    for w in sentence.lower().split():\n",
    "        ret.append(wordvectors[w])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "class SentimentExp(object):\n",
    "    \n",
    "    def __init__(self, train_X, train_Y, wordvecs=wordvectors):\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = theano.shared(np.array(train_Y, 'int32'))\n",
    "        self.wordvecs = wordvecs\n",
    "        \n",
    "        self.input_size = 10\n",
    "        self.batch_size = 10\n",
    "        \n",
    "        self.learning_rate = .01\n",
    "        self.momentum = .9\n",
    "        \n",
    "        self.train_X_rep = theano.shared(lasagne.utils.floatX([[self.getRep(x)] for x in self.train_X]))\n",
    "                \n",
    "        self._setup()\n",
    "        \n",
    "    def getRep(self, sent):\n",
    "        ret = []\n",
    "        for i in xrange(self.input_size):\n",
    "            if i < len(sent):\n",
    "                ret.append(self.wordvecs[sent[i]])\n",
    "            else:\n",
    "                ret.append(np.zeros(self.wordvecs.vector_size))\n",
    "        return np.matrix(ret).reshape((1, self.input_size, self.wordvecs.vector_size))\n",
    "    \n",
    "    def _setup(self):\n",
    "        self.y_batch = T.ivectors('y')\n",
    "        self.x_batch = T.tensor4('x')  # have to match the dimention of the input layer\n",
    "        \n",
    "        self.batch_index = T.iscalar('batch_index')\n",
    "        self.batch_slice = slice(self.batch_index * self.batch_size, (self.batch_index + 1) * self.batch_size)\n",
    "        \n",
    "        self.input_l = InputLayer(shape=(self.batch_size, 1, self.input_size, self.wordvecs.vector_size), name='x')\n",
    "        self.conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.input_l,\n",
    "            num_filters=32, \n",
    "            filter_size=5,\n",
    "            name='conv1'\n",
    "        )\n",
    "        #self.conv2_l = lasagne.layers.Conv2DLayer(\n",
    "        #   self.conv1_l,\n",
    "        #    num_filters=32,\n",
    "        #    filter_size=5,\n",
    "        #)\n",
    "        self.output_l = lasagne.layers.DenseLayer(self.conv1_l, num_units=1, name='dens2')\n",
    "        \n",
    "        self.output_v = lasagne.layers.get_output(self.output_l, self.x_batch)\n",
    "        self.loss_v = lasagne.objectives.binary_crossentropy(self.output_v, self.y_batch).mean()\n",
    "        \n",
    "        self.output_test = lasagne.layers.get_output(self.output_l, self.x_batch, deterministic=True)\n",
    "        self.loss_eval = lasagne.objectives.binary_crossentropy(self.output_test, self.y_batch)\n",
    "        \n",
    "        self.all_params = lasagne.layers.get_all_params(self.output_l)\n",
    "        self.updates = lasagne.updates.nesterov_momentum(self.loss_v, self.all_params, \n",
    "                                                         self.learning_rate, self.momentum)\n",
    "        \n",
    "        self.prediction = T.argmax(self.output_v)\n",
    "        self.accuract = T.mean(T.eq(self.prediction, self.y_batch), dtype=theano.config.floatX)\n",
    "        \n",
    "        self.iter_train = theano.function(\n",
    "            [self.batch_index], self.loss_v,\n",
    "            updates=self.updates,\n",
    "            givens={\n",
    "                self.x_batch: self.train_X_rep[self.batch_slice],\n",
    "                self.y_batch: self.train_Y[self.batch_slice]\n",
    "            },\n",
    "            mode='DebugMode'\n",
    "        )\n",
    "        \n",
    "        self.test_valid = theano.function(\n",
    "            [self.x_batch, self.y_batch], [self.loss_eval, self.prediction, self.accuract],\n",
    "        )\n",
    "            \n",
    "    def train(self):\n",
    "        pass\n",
    "            \n",
    "experiment = SentimentExp(sentiment.train_X, sentiment.train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.config.optimizer = 'None'  # disable the optimizer so that we can maybe get more debugging information\n",
    "theano.config.exception_verbosity = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1, 10, 100), (10, 1, 10, 100))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.input_l.shape, experiment.train_X_rep[experiment.batch_slice].eval({experiment.batch_index:0}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.train_Y[experiment.batch_slice].eval({experiment.batch_index:0}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/tensor/nnet/conv.py:771: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  1, val, bval, 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An optimization (probably local_mul_canonizer) inserted an apply node that raise an error.\nThe information we have about this optimizations is:Elemwise{true_div,no_inplace}.0\n  Elemwise{true_div,no_inplace} [@A] ''   \n   |Elemwise{mul,no_inplace} [@B] ''   \n   | |TensorConstant{(1, 1) of 2.0} [@C]\n   | |Elemwise{mul,no_inplace} [@D] ''   \n   | | |TensorConstant{(1, 1) of -1.0} [@E]\n   | | |Elemwise{second,no_inplace} [@F] ''   \n   | |   |DimShuffle{x,x} [@G] ''   \n   | |   | |Elemwise{true_div,no_inplace} [@H] ''   \n   | |   |Elemwise{true_div} [@I] ''   \n   | |     |Elemwise{true_div} [@J] ''   \n   | |     |DimShuffle{x,x} [@K] ''   \n   | |DimShuffle{x,0} [@L] ''   \n   |   |Subtensor{int32:int32:} [@M] ''   \n   |     |<TensorType(int32, vector)> [@N]\n   |     |ScalarFromTensor [@O] ''   \n   |     | |Elemwise{mul,no_inplace} [@P] ''   \n   |     |ScalarFromTensor [@Q] ''   \n   |       |Elemwise{mul,no_inplace} [@R] ''   \n   |Elemwise{add,no_inplace} [@S] ''   \n     |Elemwise{add,no_inplace} [@T] ''   \n     | |dot [@U] ''   \n     | | |Flatten{2} [@V] ''   \n     | | | |Elemwise{mul,no_inplace} [@W] ''   \n     | | |dens2.W [@X]\n     | |DimShuffle{x,0} [@Y] ''   \n     |   |dens2.b [@Z]\n     |Elemwise{Abs} [@BA] ''   \n       |Elemwise{add,no_inplace} [@T] ''   \n\n\nThe original exception: \nDimension mismatch; shapes are (*, 10), (10, 1)\nApply node that caused the error: Elemwise{true_div,no_inplace}(Elemwise{mul,no_inplace}.0, Elemwise{add,no_inplace}.0)\nInputs types: [TensorType(float64, row), TensorType(float64, matrix)]\nInputs shapes: [(1, 10), (10, 1)]\nInputs strides: [(80, 8), (8, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[]]\n\nDebugprint of the apply node: \nElemwise{true_div,no_inplace} [@A] <TensorType(float64, matrix)> ''   \n |Elemwise{mul,no_inplace} [@B] <TensorType(float64, row)> ''   \n | |DimShuffle{x,x} [@C] <TensorType(float64, (True, True))> ''   \n | | |TensorConstant{-2.0} [@D] <TensorType(float64, scalar)>\n | |Elemwise{true_div} [@E] <TensorType(float64, (True, True))> ''   \n | | |Elemwise{true_div} [@F] <TensorType(float64, (True, True))> ''   \n | | | |TensorConstant{(1, 1) of 1.0} [@G] <TensorType(float64, (True, True))>\n | | | |DimShuffle{x,x} [@H] <TensorType(float64, (True, True))> ''   \n | | |   |Subtensor{int64} [@I] <TensorType(float64, scalar)> ''   \n | | |     |Elemwise{Cast{float64}} [@J] <TensorType(float64, vector)> ''   \n | | |     | |MakeVector [@K] <TensorType(int64, vector)> ''   \n | | |     |   |TensorConstant{10} [@L] <TensorType(int64, scalar)>\n | | |     |   |Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)))}}[(0, 2)] [@M] <TensorType(int64, scalar)> ''   \n | | |     |     |Elemwise{Composite{(i0 * (i1 + i2))}} [@N] <TensorType(int32, scalar)> ''   \n | | |     |     | |TensorConstant{10} [@O] <TensorType(int32, scalar)>\n | | |     |     | |TensorConstant{1} [@P] <TensorType(int32, scalar)>\n | | |     |     | |batch_index [@Q] <TensorType(int32, scalar)>\n | | |     |     |TensorConstant{0} [@R] <TensorType(int8, scalar)>\n | | |     |     |Shape_i{0} [@S] <TensorType(int64, scalar)> ''   \n | | |     |     | |<TensorType(int32, vector)> [@T] <TensorType(int32, vector)>\n | | |     |     |TensorConstant{-1} [@U] <TensorType(int8, scalar)>\n | | |     |     |Elemwise{mul,no_inplace} [@V] <TensorType(int32, scalar)> ''   \n | | |     |       |TensorConstant{10} [@O] <TensorType(int32, scalar)>\n | | |     |       |batch_index [@Q] <TensorType(int32, scalar)>\n | | |     |Constant{1} [@W] <int64>\n | | |DimShuffle{x,x} [@X] <TensorType(float64, (True, True))> ''   \n | |   |Subtensor{int64} [@Y] <TensorType(float64, scalar)> ''   \n | |     |Elemwise{Cast{float64}} [@J] <TensorType(float64, vector)> ''   \n | |     |Constant{0} [@Z] <int64>\n | |DimShuffle{x,0} [@BA] <TensorType(int32, row)> ''   \n |   |Subtensor{int32:int32:} [@BB] <TensorType(int32, vector)> ''   \n |     |<TensorType(int32, vector)> [@T] <TensorType(int32, vector)>\n |     |ScalarFromTensor [@BC] <int32> ''   \n |     | |Elemwise{mul,no_inplace} [@V] <TensorType(int32, scalar)> ''   \n |     |ScalarFromTensor [@BD] <int32> ''   \n |       |Elemwise{Composite{(i0 * (i1 + i2))}} [@N] <TensorType(int32, scalar)> ''   \n |Elemwise{add,no_inplace} [@BE] <TensorType(float64, matrix)> ''   \n   |Elemwise{add,no_inplace} [@BF] <TensorType(float64, matrix)> ''   \n   | |Dot22 [@BG] <TensorType(float64, matrix)> ''   \n   | | |Flatten{2} [@BH] <TensorType(float64, matrix)> ''   \n   | | | |Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)] [@BI] <TensorType(float64, 4D)> ''   \n   | | |   |TensorConstant{(1, 1, 1, 1) of 0.5} [@BJ] <TensorType(float64, (True, True, True, True))>\n   | | |   |Elemwise{add,no_inplace} [@BK] <TensorType(float64, 4D)> ''   \n   | | |   | |ConvOp{('imshp', (1, 10, 100)),('kshp', (5, 5)),('nkern', 32),('bsize', 10),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (1, 10, 100)),('kshp_logical', (5, 5)),('kshp_logical_top_aligned', True)} [@BL] <TensorType(float64, 4D)> ''   \n   | | |   | | |Subtensor{int32:int32:} [@BM] <TensorType(float64, 4D)> ''   \n   | | |   | | | |<TensorType(float64, 4D)> [@BN] <TensorType(float64, 4D)>\n   | | |   | | | |ScalarFromTensor [@BC] <int32> ''   \n   | | |   | | | |ScalarFromTensor [@BD] <int32> ''   \n   | | |   | | |conv1.W [@BO] <TensorType(float64, 4D)>\n   | | |   | |InplaceDimShuffle{x,0,x,x} [@BP] <TensorType(float64, (True, False, True, True))> ''   \n   | | |   |   |conv1.b [@BQ] <TensorType(float64, vector)>\n   | | |   |ConvOp{('imshp', (1, 10, 100)),('kshp', (5, 5)),('nkern', 32),('bsize', 10),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (1, 10, 100)),('kshp_logical', (5, 5)),('kshp_logical_top_aligned', True)} [@BL] <TensorType(float64, 4D)> ''   \n   | | |   |InplaceDimShuffle{x,0,x,x} [@BP] <TensorType(float64, (True, False, True, True))> ''   \n   | | |dens2.W [@BR] <TensorType(float64, matrix)>\n   | |InplaceDimShuffle{x,0} [@BS] <TensorType(float64, row)> ''   \n   |   |dens2.b [@BT] <TensorType(float64, vector)>\n   |Elemwise{Abs} [@BU] <TensorType(float64, matrix)> ''   \n     |Elemwise{add,no_inplace} [@BF] <TensorType(float64, matrix)> ''   \n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-655c6822f86d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/compile/debugmode.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_isfinite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2214\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2215\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2216\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2217\u001b[0m                     \u001b[1;31m# put back the filter_checks_isfinite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/compile/debugmode.pyc\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1943\u001b[0m                             \u001b[0mexc_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m                             raise_with_op(node, thunk_c,\n\u001b[1;32m-> 1945\u001b[1;33m                                           (exc_type, exc_value, exc_trace))\n\u001b[0m\u001b[0;32m   1946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1947\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mthunk_py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    293\u001b[0m     exc_value = exc_type(str(exc_value) + detailed_err_msg +\n\u001b[0;32m    294\u001b[0m                          '\\n' + '\\n'.join(hints))\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/compile/debugmode.pyc\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1917\u001b[0m                                       \"output storage\", i)\n\u001b[0;32m   1918\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1919\u001b[1;33m                             \u001b[0mthunk_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1920\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m                             \u001b[1;31m# shouldn't have put it into the list in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/matthew/.virtualenvs/nlp-convnet/lib/python2.7/site-packages/theano/tensor/elemwise.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, output_storage)\u001b[0m\n\u001b[0;32m    815\u001b[0m                 base_exc_str = 'Dimension mismatch; shapes are %s' % (\n\u001b[0;32m    816\u001b[0m                                ', '.join(msg))\n\u001b[1;32m--> 817\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_exc_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;31m# Determine the shape of outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: An optimization (probably local_mul_canonizer) inserted an apply node that raise an error.\nThe information we have about this optimizations is:Elemwise{true_div,no_inplace}.0\n  Elemwise{true_div,no_inplace} [@A] ''   \n   |Elemwise{mul,no_inplace} [@B] ''   \n   | |TensorConstant{(1, 1) of 2.0} [@C]\n   | |Elemwise{mul,no_inplace} [@D] ''   \n   | | |TensorConstant{(1, 1) of -1.0} [@E]\n   | | |Elemwise{second,no_inplace} [@F] ''   \n   | |   |DimShuffle{x,x} [@G] ''   \n   | |   | |Elemwise{true_div,no_inplace} [@H] ''   \n   | |   |Elemwise{true_div} [@I] ''   \n   | |     |Elemwise{true_div} [@J] ''   \n   | |     |DimShuffle{x,x} [@K] ''   \n   | |DimShuffle{x,0} [@L] ''   \n   |   |Subtensor{int32:int32:} [@M] ''   \n   |     |<TensorType(int32, vector)> [@N]\n   |     |ScalarFromTensor [@O] ''   \n   |     | |Elemwise{mul,no_inplace} [@P] ''   \n   |     |ScalarFromTensor [@Q] ''   \n   |       |Elemwise{mul,no_inplace} [@R] ''   \n   |Elemwise{add,no_inplace} [@S] ''   \n     |Elemwise{add,no_inplace} [@T] ''   \n     | |dot [@U] ''   \n     | | |Flatten{2} [@V] ''   \n     | | | |Elemwise{mul,no_inplace} [@W] ''   \n     | | |dens2.W [@X]\n     | |DimShuffle{x,0} [@Y] ''   \n     |   |dens2.b [@Z]\n     |Elemwise{Abs} [@BA] ''   \n       |Elemwise{add,no_inplace} [@T] ''   \n\n\nThe original exception: \nDimension mismatch; shapes are (*, 10), (10, 1)\nApply node that caused the error: Elemwise{true_div,no_inplace}(Elemwise{mul,no_inplace}.0, Elemwise{add,no_inplace}.0)\nInputs types: [TensorType(float64, row), TensorType(float64, matrix)]\nInputs shapes: [(1, 10), (10, 1)]\nInputs strides: [(80, 8), (8, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[]]\n\nDebugprint of the apply node: \nElemwise{true_div,no_inplace} [@A] <TensorType(float64, matrix)> ''   \n |Elemwise{mul,no_inplace} [@B] <TensorType(float64, row)> ''   \n | |DimShuffle{x,x} [@C] <TensorType(float64, (True, True))> ''   \n | | |TensorConstant{-2.0} [@D] <TensorType(float64, scalar)>\n | |Elemwise{true_div} [@E] <TensorType(float64, (True, True))> ''   \n | | |Elemwise{true_div} [@F] <TensorType(float64, (True, True))> ''   \n | | | |TensorConstant{(1, 1) of 1.0} [@G] <TensorType(float64, (True, True))>\n | | | |DimShuffle{x,x} [@H] <TensorType(float64, (True, True))> ''   \n | | |   |Subtensor{int64} [@I] <TensorType(float64, scalar)> ''   \n | | |     |Elemwise{Cast{float64}} [@J] <TensorType(float64, vector)> ''   \n | | |     | |MakeVector [@K] <TensorType(int64, vector)> ''   \n | | |     |   |TensorConstant{10} [@L] <TensorType(int64, scalar)>\n | | |     |   |Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i4, i1, i2), i1), i2), i1), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(Composite{Switch(LT(i0, i1), i2, i0)}(Composite{Switch(LT(i0, i1), (i0 + i2), i0)}(i0, i1, i2), i1, i3), i2), i1)))}}[(0, 2)] [@M] <TensorType(int64, scalar)> ''   \n | | |     |     |Elemwise{Composite{(i0 * (i1 + i2))}} [@N] <TensorType(int32, scalar)> ''   \n | | |     |     | |TensorConstant{10} [@O] <TensorType(int32, scalar)>\n | | |     |     | |TensorConstant{1} [@P] <TensorType(int32, scalar)>\n | | |     |     | |batch_index [@Q] <TensorType(int32, scalar)>\n | | |     |     |TensorConstant{0} [@R] <TensorType(int8, scalar)>\n | | |     |     |Shape_i{0} [@S] <TensorType(int64, scalar)> ''   \n | | |     |     | |<TensorType(int32, vector)> [@T] <TensorType(int32, vector)>\n | | |     |     |TensorConstant{-1} [@U] <TensorType(int8, scalar)>\n | | |     |     |Elemwise{mul,no_inplace} [@V] <TensorType(int32, scalar)> ''   \n | | |     |       |TensorConstant{10} [@O] <TensorType(int32, scalar)>\n | | |     |       |batch_index [@Q] <TensorType(int32, scalar)>\n | | |     |Constant{1} [@W] <int64>\n | | |DimShuffle{x,x} [@X] <TensorType(float64, (True, True))> ''   \n | |   |Subtensor{int64} [@Y] <TensorType(float64, scalar)> ''   \n | |     |Elemwise{Cast{float64}} [@J] <TensorType(float64, vector)> ''   \n | |     |Constant{0} [@Z] <int64>\n | |DimShuffle{x,0} [@BA] <TensorType(int32, row)> ''   \n |   |Subtensor{int32:int32:} [@BB] <TensorType(int32, vector)> ''   \n |     |<TensorType(int32, vector)> [@T] <TensorType(int32, vector)>\n |     |ScalarFromTensor [@BC] <int32> ''   \n |     | |Elemwise{mul,no_inplace} [@V] <TensorType(int32, scalar)> ''   \n |     |ScalarFromTensor [@BD] <int32> ''   \n |       |Elemwise{Composite{(i0 * (i1 + i2))}} [@N] <TensorType(int32, scalar)> ''   \n |Elemwise{add,no_inplace} [@BE] <TensorType(float64, matrix)> ''   \n   |Elemwise{add,no_inplace} [@BF] <TensorType(float64, matrix)> ''   \n   | |Dot22 [@BG] <TensorType(float64, matrix)> ''   \n   | | |Flatten{2} [@BH] <TensorType(float64, matrix)> ''   \n   | | | |Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)] [@BI] <TensorType(float64, 4D)> ''   \n   | | |   |TensorConstant{(1, 1, 1, 1) of 0.5} [@BJ] <TensorType(float64, (True, True, True, True))>\n   | | |   |Elemwise{add,no_inplace} [@BK] <TensorType(float64, 4D)> ''   \n   | | |   | |ConvOp{('imshp', (1, 10, 100)),('kshp', (5, 5)),('nkern', 32),('bsize', 10),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (1, 10, 100)),('kshp_logical', (5, 5)),('kshp_logical_top_aligned', True)} [@BL] <TensorType(float64, 4D)> ''   \n   | | |   | | |Subtensor{int32:int32:} [@BM] <TensorType(float64, 4D)> ''   \n   | | |   | | | |<TensorType(float64, 4D)> [@BN] <TensorType(float64, 4D)>\n   | | |   | | | |ScalarFromTensor [@BC] <int32> ''   \n   | | |   | | | |ScalarFromTensor [@BD] <int32> ''   \n   | | |   | | |conv1.W [@BO] <TensorType(float64, 4D)>\n   | | |   | |InplaceDimShuffle{x,0,x,x} [@BP] <TensorType(float64, (True, False, True, True))> ''   \n   | | |   |   |conv1.b [@BQ] <TensorType(float64, vector)>\n   | | |   |ConvOp{('imshp', (1, 10, 100)),('kshp', (5, 5)),('nkern', 32),('bsize', 10),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (1, 10, 100)),('kshp_logical', (5, 5)),('kshp_logical_top_aligned', True)} [@BL] <TensorType(float64, 4D)> ''   \n   | | |   |InplaceDimShuffle{x,0,x,x} [@BP] <TensorType(float64, (True, False, True, True))> ''   \n   | | |dens2.W [@BR] <TensorType(float64, matrix)>\n   | |InplaceDimShuffle{x,0} [@BS] <TensorType(float64, row)> ''   \n   |   |dens2.b [@BT] <TensorType(float64, vector)>\n   |Elemwise{Abs} [@BU] <TensorType(float64, matrix)> ''   \n     |Elemwise{add,no_inplace} [@BF] <TensorType(float64, matrix)> ''   \n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "experiment.iter_train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_index = T.iscalar('batch_index')\n",
    "X_batch = T.matrix('x.input')\n",
    "y_batch = T.ivector('y')\n",
    "slice(batch_index * 100, (batch_index + 1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment.input.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(10,1,10,10))\n",
    "l_conv1 = lasagne.layers.Conv2DLayer(l_in, num_filters=32, filter_size=(5,5))\n",
    "l_out = lasagne.layers.DenseLayer(l_conv1, num_units=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasagne.layers.get_output(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment.train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.shared(np.array(sentiment.train_Y, 'int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T.ivector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
