{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Working on adding a lot of features just to see if it can get the score up regardless of how complicated or where the data is coming from\n",
    "\n",
    "Features that I will be adding\n",
    "\n",
    "* Taget given surface counts\n",
    "* words from the target and source document\n",
    "  * possible back prop into these vectors, idea is to replace tf-idf with some nn and back prop here\n",
    "* using a linear layer near the output to combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from theano import *\n",
    "from lasagne.layers import InputLayer, get_output\n",
    "import lasagne\n",
    "import lasagne.layers\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "from helpers import SimpleMaxingLayer, SimpleAverageLayer\n",
    "from wordvecs import WordVectors, EmbeddingLayer, WordTokenizer\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "#theano.config.linker = 'cvm_nogc'\n",
    "theano.config.openmp = True\n",
    "theano.config.openmp_elemwise_minsize = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/external-wiki2.json') as f:\n",
    "    queries = json.load(f)['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8917"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any([g['gold'] for g in v.values()]) for v in queries.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordvectors = WordVectors(\n",
    "    fname=\"/data/matthew/enwiki-20141208-pages-articles-multistream-links7-output1.bin\",\n",
    "    redir_fname='/data/matthew/enwiki-20141208-pages-articles-multistream-redirect7.json',\n",
    "    negvectors=False,\n",
    "    sentence_length=200,\n",
    ")\n",
    "wordvectors.add_unknown_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('/data/matthew/enwiki-20141208-pages-articles-multistream-redirects5.json') as f:\n",
    "#     page_redirects = json.load(f)\n",
    "page_redirects = wordvectors.redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/matthew/enwiki-20141208-pages-articles-multistream-surface-counts7.json') as f:\n",
    "    surface_counts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and make the surfaces items match what we are looking for\n",
    "surface_counts_re = re.compile('([\\.,!\\?])')\n",
    "for sk in surface_counts.keys():\n",
    "    nsk = sk.replace('(', '-lrb-').replace(')', '-rrb-')\n",
    "    nsk = surface_counts_re.sub(' \\\\1', nsk)\n",
    "    if nsk != sk:\n",
    "        surface_counts[nsk] = surface_counts[sk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wikireader import WikiRegexes, WikipediaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PreProcessedQueries(wikipedia_dump_fname, vectors=wordvectors, queries=queries, redirects=page_redirects, surface=surface_counts):\n",
    "    \n",
    "    get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    \n",
    "    wordvec = WordTokenizer(vectors, sentence_length=200)\n",
    "    documentvec = WordTokenizer(vectors, sentence_length=1)\n",
    "    \n",
    "    queried_pages = set()\n",
    "    for docs, q in queries.iteritems():\n",
    "        wordvec.tokenize(docs)\n",
    "        for sur, v in q.iteritems():\n",
    "            wrds_sur = get_words.sub(' ', sur)\n",
    "            wordvec.tokenize(wrds_sur)\n",
    "            link_sur = get_link.match(sur).group(1)\n",
    "            wordvec.tokenize(link_sur)\n",
    "            for link in v['vals'].keys():\n",
    "                wrds = get_words.sub(' ', link)\n",
    "                wordvec.tokenize(wrds)\n",
    "                tt = WikiRegexes.convertToTitle(link)\n",
    "                documentvec.get_location(tt)\n",
    "                queried_pages.add(tt)\n",
    "\n",
    "    added_pages = set()\n",
    "    for title in queried_pages:\n",
    "        if title in redirects:\n",
    "            #wordvec.tokenize(self.redirects[title])\n",
    "            documentvec.get_location(redirects[title])\n",
    "            added_pages.add(redirects[title])\n",
    "    queried_pages |= added_pages\n",
    "\n",
    "    page_content = {}\n",
    "\n",
    "#     class GetWikipediaWords(WikipediaReader, WikiRegexes):\n",
    "\n",
    "#         def readPage(ss, title, content, namespace):\n",
    "#             if namespace != 0:\n",
    "#                 return\n",
    "#             tt = ss.convertToTitle(title)\n",
    "#             if tt in queried_pages:\n",
    "#                 cnt = ss._wikiToText(content)\n",
    "#                 page_content[tt] = wordvec.tokenize(cnt)\n",
    "\n",
    "#     GetWikipediaWords(wikipedia_dump_fname).read()\n",
    "    \n",
    "    rr = redirects\n",
    "    rq = queried_pages\n",
    "    rc = page_content\n",
    "    rs = surface\n",
    "\n",
    "    class PreProcessedQueriesCls(object):\n",
    "        \n",
    "        wordvecs = wordvec\n",
    "        documentvecs = documentvec\n",
    "        queries = queries\n",
    "        redirects = rr\n",
    "        queried_pages = rq\n",
    "        page_content = rc\n",
    "        surface_counts = rs\n",
    "        \n",
    "        \n",
    "    return PreProcessedQueriesCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 91.7 ms, total: 1min 46s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%time basePreProcessedQueries = PreProcessedQueries('/data/matthew/enwiki-20141208-pages-articles-multistream.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056055"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordvectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for qu in queries.values():\n",
    "    for en in qu.values():\n",
    "        en['boosted'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanUpMultipleLinks():\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            gold_page = en['gold']\n",
    "            gold_title = WikiRegexes.convertToTitle(gold_page)\n",
    "            gold_title = page_redirects.get(gold_title, gold_title)\n",
    "            pages = set()\n",
    "            for p in en['vals'].keys():\n",
    "                wiki_title = WikiRegexes.convertToTitle(p)\n",
    "                wiki_title = page_redirects.get(wiki_title, wiki_title)\n",
    "                if wiki_title == gold_title and p != en['gold']:\n",
    "                    del en['vals'][p]\n",
    "                elif wiki_title not in pages:\n",
    "                    pages.add(wiki_title)\n",
    "                else:\n",
    "                    del en['vals'][p]\n",
    "cleanUpMultipleLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34863863"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeSingleLinkTargets():\n",
    "    \"items that the surface link set only has a single item are trival, so remove\"\n",
    "    get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "    for quk, qu in queries.items():\n",
    "        for sur in qu.keys():\n",
    "            surlink = get_link.match(sur).group(1)\n",
    "            surmatch = surlink.lower()\n",
    "            surcounts = surface_counts.get(surmatch)\n",
    "            if surcounts and len(surcounts) == 1:\n",
    "                # this is trival since there is only one item\n",
    "                del qu[sur]\n",
    "        if not qu:\n",
    "            # we removed all the links on this page, remove it otherwise the program crashes\n",
    "            del queries[quk]\n",
    "removeSingleLinkTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34793912"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(q) for q in queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EntityVectorLinkExp(basePreProcessedQueries):\n",
    "\n",
    "    batch_size = 1000 #20000\n",
    "    num_training_items = 500000 #200000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sentence_length = self.wordvecs.sentence_length\n",
    "        self.document_length = 100\n",
    "        \n",
    "        self.num_words_to_use_conv = 5\n",
    "        self.enable_boosting = False\n",
    "        self.num_negative_target_samples = 1\n",
    "        #self.enable_match_surface = False\n",
    "        #self.enable_link_counts = True\n",
    "        self.enable_train_wordvecs = False\n",
    "        self.enable_cap_boosting = True\n",
    "\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        self.x_document_input = T.imatrix('x_doc')\n",
    "\n",
    "        self.x_document_id = T.ivector('x_doc_id')\n",
    "        self.x_surface_text_input = T.imatrix('x_surface_link')\n",
    "        self.x_surface_context_input = T.imatrix('x_surface_cxt')  # TODO\n",
    "\n",
    "        self.x_target_input = T.ivector('x_target')\n",
    "        self.x_target_words = T.imatrix('x_target_words')\n",
    "        self.x_matches_surface = T.ivector('x_match_surface')\n",
    "        self.x_matches_counts = T.imatrix('x_matches_counts')\n",
    "        self.x_link_id = T.ivector('x_link_id')\n",
    "\n",
    "        #self.y_score = T.vector('y')\n",
    "        self.y_answer = T.ivector('y_ans')  # contains the location of the gold answer so we can compute the loss\n",
    "        self.y_grouping = T.imatrix('y_grouping')\n",
    "        self.y_boosted = T.vector('y_boosted')\n",
    "\n",
    "\n",
    "        self.embedding_W = theano.shared(self.wordvecs.get_numpy_matrix().astype(theano.config.floatX),name='embedding_W')\n",
    "        self.embedding_W_docs = theano.shared(self.documentvecs.get_numpy_matrix().astype(theano.config.floatX),name='embedding_W_docs')\n",
    "        \n",
    "        self.document_l = lasagne.layers.InputLayer(\n",
    "            (None,self.document_length),\n",
    "            input_var=self.x_document_input\n",
    "        )\n",
    "\n",
    "        self.document_embedding_l = EmbeddingLayer(\n",
    "            self.document_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.document_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.document_embedding_l,\n",
    "            num_filters=500,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='document_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.document_max_l = lasagne.layers.Pool2DLayer(\n",
    "            self.document_conv1_l,\n",
    "            name='document_pool1',\n",
    "            pool_size=(self.document_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        document_output_length = 200\n",
    "        \n",
    "        self.document_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.document_max_l,\n",
    "            num_units=document_output_length,\n",
    "            name='doucment_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.document_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop1,\n",
    "#             num_units=225,\n",
    "#             name='document_dens2',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.document_drop2 = lasagne.layers.DropoutLayer(\n",
    "#             self.document_dens2,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.document_dens3 = lasagne.layers.DenseLayer(\n",
    "#             self.document_drop2,\n",
    "#             num_units=document_output_length,\n",
    "#             name='document_dens3',\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "        self.document_output = lasagne.layers.get_output(self.document_dens1)\n",
    "    \n",
    "    \n",
    "        ##########################################\n",
    "        ## surface text\n",
    "\n",
    "        self.surface_context_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_context_input,\n",
    "        )\n",
    "\n",
    "        self.surface_context_embedding_l = EmbeddingLayer(\n",
    "            self.surface_context_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.surface_context_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_context_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_cxt_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_context_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_context_conv1_l, self.surface_context_l],\n",
    "#             #name='surface_context_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_context_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_context_conv1_l,\n",
    "            name='surface_cxt_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None, self.sentence_length),\n",
    "            input_var=self.x_surface_text_input\n",
    "        )\n",
    "\n",
    "        self.surface_embedding_l = EmbeddingLayer(\n",
    "            self.surface_input_l,\n",
    "            W=self.embedding_W,\n",
    "            add_word_params=self.enable_train_wordvecs,\n",
    "        )\n",
    "\n",
    "        self.surface_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "            self.surface_embedding_l,\n",
    "            num_filters=300,\n",
    "            filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "            name='surface_conv1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_avg1_l = SimpleAverageLayer(\n",
    "#             [self.surface_conv1_l, self.surface_input_l],\n",
    "#             #name='surface_avg'\n",
    "#         )\n",
    "\n",
    "        self.surface_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "            self.surface_conv1_l,\n",
    "            name='surface_pool1',\n",
    "            pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "            mode='sum',\n",
    "        )\n",
    "\n",
    "        self.surface_merged_l = lasagne.layers.ConcatLayer(\n",
    "            [self.surface_context_pool1_l, self.surface_pool1_l]\n",
    "        )\n",
    "\n",
    "        self.surface_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.surface_merged_l,\n",
    "            name='surface_dens1',\n",
    "            num_units=250,\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "#         self.surface_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.surface_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.surface_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.surface_drop1,\n",
    "#             name='surface_dens2',\n",
    "#             num_units=200,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "        ##################################################\n",
    "        ## merge the documents with the surface info\n",
    "\n",
    "        self.document_aligned_l = InputLayer(\n",
    "            (None, document_output_length),\n",
    "            input_var=self.document_output[self.x_document_id,:]\n",
    "        )\n",
    "\n",
    "        self.source_l = lasagne.layers.ConcatLayer(\n",
    "            [self.document_aligned_l, self.surface_dens1]\n",
    "        )\n",
    "\n",
    "        self.source_dens1 = lasagne.layers.DenseLayer(\n",
    "            self.source_l,\n",
    "            num_units=300,\n",
    "            name='source_dens1',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop1 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens1,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        self.source_dens12 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop1,\n",
    "            num_units=250,\n",
    "            name='source_dens12',\n",
    "            nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        )\n",
    "\n",
    "        self.source_drop12 = lasagne.layers.DropoutLayer(\n",
    "            self.source_dens12,\n",
    "            p=.25,\n",
    "        )\n",
    "\n",
    "        compared_vector_size = self.wordvecs.vector_size #+ 2 # extra space for if it matches the surface text\n",
    "\n",
    "        self.source_dens2 = lasagne.layers.DenseLayer(\n",
    "            self.source_drop12,\n",
    "            num_units=compared_vector_size,  # this is the same size as the learned wikipedia vectors\n",
    "            name='source_dens2',\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "        self.source_out = lasagne.layers.get_output(self.source_dens2)\n",
    "\n",
    "        matched_surface_reshaped = self.x_matches_surface.reshape(\n",
    "            (self.x_matches_surface.shape[0], 1, 1, 1)).astype(theano.config.floatX)\n",
    "\n",
    "        self.target_input_l = lasagne.layers.InputLayer(\n",
    "            (None,),\n",
    "            input_var=self.x_target_input\n",
    "        )\n",
    "\n",
    "        self.target_matched_surface_input_l = lasagne.layers.InputLayer(\n",
    "            (None,1,1,1),\n",
    "            input_var=matched_surface_reshaped,\n",
    "        )\n",
    "        \n",
    "        self.target_matched_counts_input_l = lasagne.layers.InputLayer(\n",
    "            (None,5),\n",
    "            input_var=self.x_matches_counts.astype(theano.config.floatX),\n",
    "        )\n",
    "\n",
    "        self.target_embedding_l = EmbeddingLayer(\n",
    "            lasagne.layers.reshape(self.target_input_l, ([0], 1)),\n",
    "            W=self.embedding_W_docs,\n",
    "            add_word_params=False,\n",
    "        )\n",
    "\n",
    "        self.target_combined_feats_l = lasagne.layers.ConcatLayer(\n",
    "            [self.target_embedding_l, self.target_matched_surface_input_l,\n",
    "            lasagne.layers.reshape(self.target_matched_counts_input_l, ([0],1,1,[1]))],\n",
    "            axis=3\n",
    "        )\n",
    "\n",
    "#         self.target_words_input_l = lasagne.layers.InputLayer(\n",
    "#             (None,self.sentence_length),\n",
    "#             input_var=self.x_target_words,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_embedding_l = EmbeddingLayer(\n",
    "#             self.target_words_input_l,\n",
    "#             W=self.embedding_W,\n",
    "#             add_word_params=False,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_conv1_l = lasagne.layers.Conv2DLayer(\n",
    "#             self.target_words_embedding_l,\n",
    "#             name='target_wrds_conv1',\n",
    "#             filter_size=(self.num_words_to_use_conv, self.wordvecs.vector_size),\n",
    "#             num_filters=350,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_words_pool1_l = lasagne.layers.Pool2DLayer(\n",
    "#             self.target_words_conv1_l,\n",
    "#             name='target_wrds_pool1',\n",
    "#             pool_size=(self.sentence_length - self.num_words_to_use_conv, 1),\n",
    "#             mode='sum',\n",
    "#         )\n",
    "\n",
    "#         self.target_merge_l = lasagne.layers.ConcatLayer(\n",
    "#             [lasagne.layers.reshape(self.target_words_pool1_l, ([0], [1])),\n",
    "#              lasagne.layers.reshape(self.target_embedding_l, ([0], [3]))]\n",
    "#         )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_merge_l,\n",
    "#             name='target_wrds_dens1',\n",
    "#             num_units=400,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_wrds_dens1',\n",
    "#             num_units=compared_vector_size,\n",
    "#             nonlinearity=lasagne.nonlinearities.linear,\n",
    "#         )\n",
    "\n",
    "        self.target_simple = lasagne.layers.DenseLayer(\n",
    "            self.target_embedding_l, #self.target_combined_feats_l,\n",
    "            name='target_simple1',\n",
    "            num_units=compared_vector_size,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "        )\n",
    "\n",
    "#         self.target_dens1 = lasagne.layers.DenseLayer(\n",
    "#             self.target_conv1_l,\n",
    "#             name='target_dens1',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "#         self.target_drop1 = lasagne.layers.DropoutLayer(\n",
    "#             self.target_dens1,\n",
    "#             p=.25,\n",
    "#         )\n",
    "\n",
    "#         self.target_dens2 = lasagne.layers.DenseLayer(\n",
    "#             self.target_drop1,\n",
    "#             name='target_dens2',\n",
    "#             num_units=300,\n",
    "#             nonlinearity=lasagne.nonlinearities.tanh,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_embedding_l)\n",
    "\n",
    "\n",
    "#         self.target_out = T.concatenate(\n",
    "#             [self.embedding_W[self.x_target_input],\n",
    "#              matched_surface_reshaped,\n",
    "#             1-matched_surface_reshaped],\n",
    "#              axis=1)\n",
    "\n",
    "\n",
    "        #self.target_out = self.embedding_W[self.x_target_input]\n",
    "        #self.target_out = lasagne.layers.get_output(self.target_dens2)\n",
    "\n",
    "        self.target_out = lasagne.layers.get_output(self.target_simple)\n",
    "\n",
    "        # compute the cosine distance between the two layers\n",
    "        self.source_aligned_l = self.source_out[self.x_link_id, :]\n",
    "\n",
    "        # this uses scan internally, which means that it comes back into python code to run the loop.....fml\n",
    "        self.dotted_vectors =  T.batched_dot(self.target_out, self.source_aligned_l)\n",
    "        # diag also does not support a C version.........\n",
    "        #self.dotted_vectors = T.dot(self.target_out, self.source_aligned_l.T).diagonal()\n",
    "\n",
    "        def augNorm(v):\n",
    "            return T.maximum(T.basic.pow(T.basic.pow(T.basic.abs_(v), 2).sum(axis=1) + .001, .5), .001)\n",
    "\n",
    "        self.res_l = self.dotted_vectors / (augNorm(self.target_out) * augNorm(self.source_aligned_l) + .001)\n",
    "        \n",
    "        self.res_cap = T.clip((T.tanh(self.res_l) + 1) / 2, .001, .999)\n",
    "        \n",
    "        #############################\n",
    "        ## Linear features combined\n",
    "        #############################\n",
    "        \n",
    "        \n",
    "        self.linear_features_combined = lasagne.layers.concat(\n",
    "            [lasagne.layers.InputLayer(\n",
    "                    (None, 1), \n",
    "                    input_var=self.res_l.reshape((self.res_l.shape[0], 1)),\n",
    "                ),\n",
    "            lasagne.layers.reshape(self.target_matched_surface_input_l, ([0],1)),\n",
    "            self.target_matched_counts_input_l],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        self.linear_features_dens_l = lasagne.layers.DenseLayer(\n",
    "            self.linear_features_combined,\n",
    "            nonlinearity=lasagne.nonlinearities.linear,\n",
    "            num_units=1,\n",
    "            name='linear_final_l',\n",
    "            W=lasagne.init.Normal(mean=1.0),\n",
    "        )\n",
    "        \n",
    "        self.linear_output = lasagne.layers.get_output(\n",
    "            lasagne.layers.reshape(self.linear_features_dens_l, ([0],))\n",
    "        )\n",
    "        \n",
    "        ########################################\n",
    "        ## true output values\n",
    "        ########################################\n",
    "        \n",
    "        self.true_output = self.linear_output\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.res_l = self.dotted_vectors / ((self.target_out.norm(1, axis=1) + .001) *\n",
    "#                                             (self.source_aligned_l.norm(1, axis=1) + .001))\n",
    "\n",
    "\n",
    "        #self.golds = self.res_cap[self.y_answer]\n",
    "\n",
    "#         def maxOverRange(indx):\n",
    "#             #return T.max(self.res_cap[T.arange(indx[0],indx[1])]) - self.res_cap[indx[2]]\n",
    "#             #return -( self.res_l[indx[2]] - T.log(T.exp(self.res_l[T.arange(indx[0],indx[1])]).sum()) )\n",
    "#             return -( self.res_l[indx[2]] - self.res_l[indx[0]])\n",
    "\n",
    "#         # build a tensor to make a matrix with one set on each dimention\n",
    "#         self.grouped, grouped_update = theano.scan(maxOverRange, sequences=self.y_grouping)\n",
    "\n",
    "        def setSubSelector(indx, outputs):\n",
    "            return T.set_subtensor(outputs[T.arange(indx[0], indx[1]), indx[3]], 1)\n",
    "\n",
    "        num_target_samples = self.linear_output.shape[0]\n",
    "\n",
    "        select_seq = T.concatenate([\n",
    "            self.y_grouping,\n",
    "            T.arange(self.y_grouping.shape[0]).reshape((self.y_grouping.shape[0], 1))\n",
    "        ], axis=1)\n",
    "\n",
    "        self.selecting_matrix, _ = theano.scan(\n",
    "            setSubSelector,\n",
    "            outputs_info=T.zeros((num_target_samples, self.y_grouping.shape[0])), #num_target_samples)),\n",
    "            #n_steps=self.y_grouping.shape[0]\n",
    "            sequences=select_seq,\n",
    "        )\n",
    "\n",
    "#         self.groupped_elems = T.dot(self.selecting_matrix[-1], \n",
    "#                                     T.diag(T.exp(self.true_output)))\n",
    "#         self.groupped_res = T.log(self.groupped_elems.sum(axis=0)[T.arange(self.y_grouping.shape[0])])\n",
    "        self.groupped_elems = T.dot(self.selecting_matrix[-1].T, \n",
    "                                   T.exp(self.true_output))\n",
    "        self.groupped_res = T.log(self.groupped_elems)\n",
    "        \n",
    "        self.loss_vec = self.groupped_res - self.true_output[self.y_grouping[:,2]]\n",
    "        \n",
    "        if self.enable_boosting:\n",
    "            self.loss_scalar = T.dot(self.y_boosted, self.loss_vec)\n",
    "        else:\n",
    "            self.loss_scalar = self.loss_vec.sum()\n",
    "\n",
    "        self.all_params = (\n",
    "            #lasagne.layers.get_all_params(self.target_dens2) +\n",
    "            # TODO: add params for the target stuff,\n",
    "            lasagne.layers.get_all_params(self.target_simple) +\n",
    "            lasagne.layers.get_all_params(self.source_dens2) +\n",
    "            lasagne.layers.get_all_params(self.linear_features_dens_l)\n",
    "            #lasagne.layers.get_all_params(self.document_dens2)\n",
    "        )\n",
    "\n",
    "        # weight the positive samples more since there are fewer of them,\n",
    "        # freaking hack\n",
    "        #self.loss_vec = -(10 * self.y_score * T.log(self.res_cap) + (1.0 - self.y_score) * T.log(1.0 - self.res_cap))\n",
    "\n",
    "        #self.loss_vec = T.nnet.binary_crossentropy(self.res_cap, self.y_score)\n",
    "\n",
    "        #self.loss_vec = T.exp(T.max(self.res_cap - self.res_cap[self.y_answer] + .1, 0)) - 1  # TODO: maybe have some squared term here or something?\n",
    "\n",
    "        # this one works reasonably well\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_cap[self.y_answer] - self.res_cap, -1.0, 0.4) + 1.0) / 1.5)\n",
    "\n",
    "        #self.loss_vec = self.grouped\n",
    "\n",
    "        #self.loss_vec = - T.log((T.clip(self.res_l[self.y_answer] - self.res_l, -40.0, 10.0) + 40.0) / 51.0)\n",
    "        #self.loss_vec = T.max(self.res_l[self.y_answer] - self.res_l + .1, 0)\n",
    "\n",
    "        self.updates = lasagne.updates.adadelta(self.loss_scalar / self.loss_vec.shape[0], self.all_params)\n",
    "\n",
    "        self.func_inputs = [\n",
    "            self.x_document_input,\n",
    "            self.x_surface_text_input, self.x_surface_context_input, self.x_document_id,\n",
    "            self.x_target_input, self.x_matches_surface, self.x_matches_counts, self.x_link_id,\n",
    "            self.y_answer, self.y_grouping, self.y_boosted\n",
    "        ]  # self.x_target_words,\n",
    "\n",
    "        ################################################################3\n",
    "        ## TODO: need to return the actual output layer instead of the res_cap, since that is something else now\n",
    "        \n",
    "        self.train_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_scalar, self.loss_vec],\n",
    "            updates=self.updates,\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "        self.test_func = theano.function(\n",
    "            self.func_inputs,\n",
    "            [self.true_output, self.loss_vec.sum(), self.loss_scalar, self.loss_vec],\n",
    "            on_unused_input='ignore',\n",
    "        )\n",
    "\n",
    "    def reset_accums(self):\n",
    "        self.current_documents = []\n",
    "        self.current_surface_context = []\n",
    "        self.current_surface_link = []\n",
    "        self.current_link_id = []\n",
    "        self.current_target_input = []\n",
    "        self.current_target_words = []\n",
    "        self.current_target_matches_surface = []\n",
    "        self.current_target_id = []\n",
    "        self.current_target_goal = []\n",
    "        self.current_learning_groups = []\n",
    "        self.learning_targets = []\n",
    "        self.current_surface_target_counts = []\n",
    "        self.current_boosted_groups = []\n",
    "        \n",
    "        self.failed_match = []\n",
    "            \n",
    "    def compute_batch(self, isTraining=True, useTrainingFunc=True):\n",
    "        if isTraining and useTrainingFunc:\n",
    "            func = self.train_func\n",
    "        else:\n",
    "            func = self.test_func\n",
    "        self.reset_accums()\n",
    "        self.total_links = 0\n",
    "        self.total_loss = 0.0\n",
    "        self.total_boosted_loss = 0.0\n",
    "\n",
    "        get_words = re.compile('[^a-zA-Z0-9 ]')\n",
    "        get_link = re.compile('.*?\\[(.*?)\\].*?')\n",
    "\n",
    "        for doc, queries in self.queries.iteritems():\n",
    "            # skip the testing documents while training and vice versa\n",
    "            if queries.values()[0]['training'] != isTraining:\n",
    "                continue\n",
    "            docid = len(self.current_documents)\n",
    "            self.current_documents.append(self.wordvecs.tokenize(doc, length=self.document_length))\n",
    "            for surtxt, targets in queries.iteritems():\n",
    "                self.current_link_id.append(docid)\n",
    "                surid = len(self.current_surface_link)\n",
    "                self.current_surface_context.append(self.wordvecs.tokenize(get_words.sub(' ' , surtxt)))\n",
    "                surlink = get_link.match(surtxt).group(1)\n",
    "                self.current_surface_link.append(self.wordvecs.tokenize(surlink))\n",
    "                surmatch = surlink.lower()\n",
    "                surcounts = self.surface_counts.get(surmatch)\n",
    "                if not surcounts:\n",
    "                    self.failed_match.append(surmatch)\n",
    "                    surcounts = {}\n",
    "                #target_page_input = []\n",
    "                target_words_input = []\n",
    "                target_matches_surface = []\n",
    "                target_inputs = []\n",
    "                target_learings = []\n",
    "                target_match_counts = []\n",
    "                target_gold_loc = -1\n",
    "                target_group_start = len(self.current_target_input)\n",
    "                \n",
    "#                 pages = []\n",
    "#                 pages_info_link = [] \n",
    "#                 gold = None\n",
    "#                 # skip the items that we don't know the gold for\n",
    "#                 if not targets['gold'] and isTraining:\n",
    "#                     continue\n",
    "#                 for target in set(targets['vals'].keys() +\n",
    "#                                  random.sample(self.documentvecs.reverse_word_location, self.num_negative_target_samples)\n",
    "#                                   ) - {None,}:\n",
    "#                     isGold = target == targets['gold']\n",
    "#                     wiki_title = WikiRegexes.convertToTitle(target)\n",
    "#                     cnt = self.documentvecs.get_location(wiki_title)\n",
    "#                     if wiki_title == 'nil':\n",
    "#                         cnt = 0  # this is the stop symbol location\n",
    "#                     if cnt is None:\n",
    "#                         # were not able to find this wikipedia document\n",
    "#                         # so just ignore tihs result since trying to train on it will cause\n",
    "#                         # issues\n",
    "#                         continue\n",
    "#                     if isGold:\n",
    "#                         gold = cnt\n",
    "#                     if cnt not in pages:\n",
    "#                         pages.append(cnt)\n",
    "#                         pages_info_link.append((targets, target))\n",
    "#                     elif isGold:\n",
    "#                         # there are two links to the same page somehow\n",
    "#                         # and we already have something that isn't linking to the gold\n",
    "#                         for i in xrange(len(pages)):\n",
    "#                             if pages[i] == cnt:\n",
    "#                                 # this links to the same page as the gold\n",
    "#                                 pages_info_link[i] = (targets, target)\n",
    "#                 for i in xrange(len(pages)):\n",
    "                    \n",
    "                    \n",
    "                for target in set(targets['vals'].keys() +\n",
    "                                 random.sample(self.documentvecs.reverse_word_location, self.num_negative_target_samples)\n",
    "                                  ) - {None,}:\n",
    "                    \n",
    "                    \n",
    "                    isGold = target == targets['gold']\n",
    "                    #cnt = self.page_content.get(WikiRegexes.convertToTitle(target))\n",
    "                    wiki_title = WikiRegexes.convertToTitle(target)\n",
    "                    cnt = self.documentvecs.get_location(wiki_title)\n",
    "                    if wiki_title == 'nil':\n",
    "                        cnt = 0  # this is the stop symbol location\n",
    "                    if cnt is None:\n",
    "                        # were not able to find this wikipedia document\n",
    "                        # so just ignore tihs result since trying to train on it will cause\n",
    "                        # issues\n",
    "                        continue\n",
    "                    if isGold:\n",
    "                        target_gold_loc = len(target_inputs)\n",
    "                    #target_page_input.append(cnt)\n",
    "                    target_words_input.append(self.wordvecs.tokenize(get_words.sub(' ', target)))\n",
    "                    target_inputs.append(cnt)  # page_content already tokenized\n",
    "                    target_matches_surface.append(int(surmatch == target.lower()))\n",
    "                    target_learings.append((targets, target))\n",
    "                    target_match_counts.append(surcounts.get(wiki_title, 0))\n",
    "                    #if wiki_title not in surcounts:\n",
    "                    #    print surcounts, wiki_title\n",
    "                if target_gold_loc is not None or not isTraining:  # if we can't get the gold item\n",
    "                    # contain the index of the gold item for these items, so it can be less then it\n",
    "                    gold_loc = (len(self.current_target_goal) + target_gold_loc)\n",
    "                    sorted_match_counts = [-4,-3,-2,-1] + sorted(set(target_match_counts))\n",
    "                    #print sorted_match_counts\n",
    "                    target_match_counts_indicators = [\n",
    "                        [\n",
    "                            int(s == sorted_match_counts[-1]),\n",
    "                            int(s == sorted_match_counts[-2]),\n",
    "                            int(s == sorted_match_counts[-3]),\n",
    "                            int(0 < s <= sorted_match_counts[-4]),\n",
    "                            int(s == 0),\n",
    "                        ]\n",
    "                        for s in target_match_counts\n",
    "                    ]\n",
    "                    self.current_target_goal += [gold_loc] * len(target_inputs)\n",
    "                    self.current_target_input += target_inputs\n",
    "                    self.current_target_id += [surid] * len(target_inputs)\n",
    "                    self.current_target_words += target_words_input   # TODO: add\n",
    "                    self.current_target_matches_surface += target_matches_surface\n",
    "                    self.current_surface_target_counts += target_match_counts_indicators\n",
    "                    target_group_end = len(self.current_target_input)\n",
    "                    self.current_learning_groups.append(\n",
    "                        [target_group_start, target_group_end,\n",
    "                         gold_loc])\n",
    "                    self.current_boosted_groups.append(targets['boosted'])\n",
    "\n",
    "                #self.current_target_goal.append(isGold)\n",
    "                self.learning_targets += target_learings\n",
    "            if len(self.current_target_id) > self.batch_size:\n",
    "                self.run_batch(func)\n",
    "                if self.total_links > self.num_training_items:\n",
    "                    return self.total_loss / self.total_links, self.total_boosted_loss / self.total_links\n",
    "\n",
    "        if len(self.current_target_id) > 0:\n",
    "            self.run_batch(func)\n",
    "\n",
    "        return self.total_loss / self.total_links, self.total_boosted_loss / self.total_links\n",
    "\n",
    "    def run_batch(self, func):\n",
    "        res_vec, loss_sum, loss_boosted, loss_vec = func(\n",
    "            self.current_documents,\n",
    "            self.current_surface_link, self.current_surface_context, self.current_link_id,\n",
    "            self.current_target_input, self.current_target_matches_surface, self.current_surface_target_counts, self.current_target_id,\n",
    "            self.current_target_goal, self.current_learning_groups, self.current_boosted_groups,\n",
    "            # self.current_target_words,\n",
    "        )\n",
    "        self.check_params()\n",
    "        self.total_links += len(self.current_target_id)\n",
    "        self.total_loss += loss_sum\n",
    "        self.total_boosted_loss += loss_boosted\n",
    "        learned_groups = []  # right...dict not hashable....\n",
    "        for i in xrange(len(res_vec)):\n",
    "            # save the results from this pass\n",
    "            l = self.learning_targets[i]\n",
    "            if l[1] in l[0]['vals']:\n",
    "                l[0]['vals'][ l[1] ] = float(res_vec[i])\n",
    "            if l[0] not in learned_groups:\n",
    "                learned_groups.append(l[0])\n",
    "        for group in learned_groups:\n",
    "            if group['gold']:\n",
    "                correct = max(group['vals']) == group['vals'][group['gold']]\n",
    "                group['boosted'] *= .4 if correct else 2.0\n",
    "                if self.enable_cap_boosting:\n",
    "                    if group['boosted'] > 10:\n",
    "                        group['boosted'] = 10.0\n",
    "                    elif group['boosted'] < 0.1:\n",
    "                        group['boosted'] = 0.1\n",
    "        self.reset_accums()\n",
    "\n",
    "    def check_params(self):\n",
    "        if any([np.isnan(v.get_value(borrow=True)).any() for v in self.all_params]):\n",
    "            raise RuntimeError('nan in some of the parameters')\n",
    "\n",
    "\n",
    "\n",
    "queries_exp = EntityVectorLinkExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentState(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct = 0\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            if en['gold']:\n",
    "                if all_trained > numSamples:\n",
    "                    break\n",
    "                all_measured += 1\n",
    "                all_trained += len(en['vals'].values())\n",
    "                m = max(en['vals'].values())\n",
    "                if en['vals'][en['gold']] == m and m != 0:\n",
    "                    all_correct += 1\n",
    "           \n",
    "    r = all_measured, float(all_correct) / all_measured\n",
    "    print r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalCurrentStateRank(trainingData=True, numSamples=50000):\n",
    "    all_measured = 0\n",
    "    all_correct_place = 0\n",
    "    p_counts = dict((k,0) for k in range(0,10))\n",
    "    all_trained = 0\n",
    "    for qu in queries.values():\n",
    "        for en in qu.values():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            if en['gold']:\n",
    "                if all_trained > numSamples:\n",
    "                    break\n",
    "                svals = sorted(en['vals'].values(), key=lambda x: -x)\n",
    "                gv = en['vals'][en['gold']]\n",
    "                if gv == 0:\n",
    "                    continue\n",
    "                all_measured += 1\n",
    "                for i in xrange(len(svals)):\n",
    "                    if svals[i] == gv:\n",
    "                        if i < 10:\n",
    "                            p_counts[i] += 1\n",
    "                        all_correct_place += i + 1\n",
    "                        break\n",
    "\n",
    "    r = all_measured, float(all_correct_place) / all_measured, p_counts\n",
    "    print r\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def augmentTrainingData():\n",
    "    for quk in queries.keys():\n",
    "        qu = queries[quk]\n",
    "        for enk in qu.keys():\n",
    "            en = qu[enk]\n",
    "            if not en['gold']:\n",
    "                del qu[enk]\n",
    "        if not qu:\n",
    "            del queries[quk]\n",
    "    for qu in queries.values():\n",
    "        training = random.random() > .15\n",
    "        for en in qu.values():\n",
    "            en['training'] = training\n",
    "augmentTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWrongItems(trainingData=True, numSamples=50):\n",
    "    ret = {}\n",
    "    for qu in queries.values():\n",
    "        for ek, en in qu.items():\n",
    "            if en['training'] != trainingData:\n",
    "                continue\n",
    "            for e in en:\n",
    "                if en['gold']:\n",
    "                    if len(ret) > numSamples:\n",
    "                        return ret\n",
    "                    m = max(en['vals'].values())\n",
    "                    g = en['vals'][en['gold']]\n",
    "                    if g != m and g != 0:\n",
    "                        ret[ek] = en\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.check_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(queries_exp.current_surface_target_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(False, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evalCurrentState(True, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for i in xrange(10):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)\n",
    "    if i % 5 == 4:\n",
    "        exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "        exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "        exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.num_training_items = 50000\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0.076904165228095206, 0.076904165228095206)),\n",
       " (1, (0.052100545509602615, 0.052100545509602615)),\n",
       " (2, (0.04875273555380287, 0.04875273555380287)),\n",
       " (3, (0.046760989848868242, 0.046760989848868242)),\n",
       " (4, (0.044899094717647269, 0.044899094717647269)),\n",
       " ('testing run', (0.048488905770622114, 0.048488905770622114)),\n",
       " ('training state', (31849, 0.8148764482401332)),\n",
       " ('testing state', (21593, 0.8012318807020794)),\n",
       " (5, (0.043408206675611044, 0.043408206675611044)),\n",
       " (6, (0.042017657503557709, 0.042017657503557709)),\n",
       " (7, (0.040299091397018288, 0.040299091397018288)),\n",
       " (8, (0.039482358343164888, 0.039482358343164888)),\n",
       " (9, (0.038430263019610986, 0.038430263019610986)),\n",
       " ('testing run', (0.048754500705015011, 0.048754500705015011)),\n",
       " ('training state', (31849, 0.8350968633238093)),\n",
       " ('testing state', (21593, 0.7992868059093224))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surface_counts['The Silent World of Nicholas Quinn'.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surface_counts['canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surface_counts['urban district']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u' [China] also placed a huge order of 30,000 7.92 mm M-26s': {'boosted': 10.0,\n",
       "  u'gold': u'Republic of China',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.5725274085998535,\n",
       "   u'A1 Team China': 0.16680650413036346,\n",
       "   u\"China men's national basketball team\": -2.3560917377471924,\n",
       "   u'China national badminton team': -1.3990291357040405,\n",
       "   u'China national football team': 0.1215636283159256,\n",
       "   u\"China women's national football team\": 0.03630630671977997,\n",
       "   u'Chinese Basketball Association': -1.432735800743103,\n",
       "   u'Chinese Football Association': -2.8019607067108154,\n",
       "   u'Cinema of China': -3.4154553413391113,\n",
       "   u'History of China': 1.7284568548202515,\n",
       "   u\"People's Republic of China\": 1.6128302812576294,\n",
       "   u'Qing dynasty': 0.36666786670684814,\n",
       "   u'Republic of China': 1.529046654701233,\n",
       "   u'Republic of China (1912\\u201349)': -0.08051319420337677,\n",
       "   u'XXNILXX': 0}},\n",
       " u' [Tagmatarkhis] Spyros Moustaklis was left brain damaged and unable to speak': {'boosted': 10.0,\n",
       "  u'gold': u'Tagmatarchis',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.841961860656738,\n",
       "   u'Tagmatarchis': -0.4758630394935608,\n",
       "   u'Tagmatarkhi': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"'' The Rocky Horror Picture Show '' is a 1975 [musical] comedy film that parodies science fiction and horror films .\": {'boosted': 10.0,\n",
       "  u'gold': u'Musical film',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.8406143188476562,\n",
       "   u'Drama Desk Award for Outstanding Musical': -0.36531227827072144,\n",
       "   u'Ethnomusicology': -0.2698984742164612,\n",
       "   u'Music video game': 3.672093629837036,\n",
       "   u'Musical': 5.304994583129883,\n",
       "   u'Musical film': 5.073089599609375,\n",
       "   u'The Phantom of the Opera (1986 musical)': 1.932352900505066,\n",
       "   u'XXNILXX': 0,\n",
       "   u'music': 2.1174449920654297,\n",
       "   u'musical play': 2.6115942001342773}},\n",
       " u', and most of the time requires usage of a [tripod] .': {'boosted': 10.0,\n",
       "  u'gold': u'Tripod (photography)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.795124530792236,\n",
       "   u'M2 tripod': 0.006402300670742989,\n",
       "   u'Sacrificial tripod': -1.1769781112670898,\n",
       "   u'Swann Morton': -2.3066937923431396,\n",
       "   u'Tripod (The War of the Worlds)': -0.5962608456611633,\n",
       "   u'Tripod (band) ': 0,\n",
       "   u'Tripod (photography)': 1.713747262954712,\n",
       "   u'Tripod (surveying)': -1.0503097772598267,\n",
       "   u'Tripod, Dublin': 0,\n",
       "   u'Tripod.com': -1.1465941667556763,\n",
       "   u'XXNILXX': 0,\n",
       "   u'tripod': 4.212184429168701,\n",
       "   u'tripod (weapon)': -1.3869012594223022}},\n",
       " u\", three Canadian destroyers , HMCS '' Cayuga '' , [HMCS '' Athabaskan ''] and HMCS Sioux , were sent to Korea to serve\": {'boosted': 10.0,\n",
       "  u'gold': u'HMCS Athabaskan (R79)',\n",
       "  u'training': False,\n",
       "  u'vals': {u\"'' Athabaskan\": 0,\n",
       "   u'-NIL-': -2.9081287384033203,\n",
       "   u'Alaskan Athabaskans': 0.6163138747215271,\n",
       "   u'Athabaskan': 2.2102880477905273,\n",
       "   u'Gwich\\u2019in language': 1.0999716520309448,\n",
       "   u\"HMCS '' Athabaskan\": 0,\n",
       "   u\"HMCS '' Athabaskan ''\": 0,\n",
       "   u'HMCS Athabaskan': 0,\n",
       "   u'HMCS Athabaskan (DDH 282)': -0.9454048275947571,\n",
       "   u'HMCS Athabaskan (G07)': -2.8248472213745117,\n",
       "   u'HMCS Athabaskan (R79)': -0.9891461730003357,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"-LRB- Pickering , and St. Andrew 's -RRB- and the [Urban District] of Haltemprice . 1955-1983 : The constituency lost the Kingston\": {'boosted': 10.0,\n",
       "  u'gold': u'Urban District',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.751587152481079,\n",
       "   u'Bakhsh': -3.05253529548645,\n",
       "   u'Banstead': -0.07474465668201447,\n",
       "   u'Chengqu, Changzhi': 0,\n",
       "   u'Districts of England': 0.7920326590538025,\n",
       "   u'Districts of Sri Lanka': 0.2891826033592224,\n",
       "   u'Districts of the Czech Republic': -3.168039321899414,\n",
       "   u'Independent cities of Germany': 1.7443878650665283,\n",
       "   u'List of districts of Nepal': -2.764585494995117,\n",
       "   u'List of districts of Serbia': -0.5659477114677429,\n",
       "   u'List of districts of Turkey': 0.3995468020439148,\n",
       "   u'List of urban districts of Vietnam': -0.14351223409175873,\n",
       "   u'Urban District': 2.6329264640808105,\n",
       "   u'Urban district (Great Britain and Ireland)': 3.6323013305664062,\n",
       "   u'XXNILXX': 0}},\n",
       " u'After the conclusion of the old-growth logging era , [homestead] ers tried to develop an agricultural economy on the cleared': {'boosted': 10.0,\n",
       "  u'gold': u'Homesteading',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.512683629989624,\n",
       "   u'Dominion Lands Act': 1.8776394128799438,\n",
       "   u'Homestead (buildings)': 4.314978122711182,\n",
       "   u'Homestead Acts': 0.8997470736503601,\n",
       "   u'Homestead High School (Fort Wayne, Indiana)': -2.0862834453582764,\n",
       "   u'Homestead High School (Mequon, Wisconsin)': -0.536432147026062,\n",
       "   u'Homestead Records': 0.6317169070243835,\n",
       "   u'Homestead principle': 1.7888280153274536,\n",
       "   u'Homestead, Florida': 1.9376167058944702,\n",
       "   u'Homestead, Iowa': -2.1530661582946777,\n",
       "   u'Homestead, Oregon': -0.9536906480789185,\n",
       "   u'Homestead, Pennsylvania': 0.4921167492866516,\n",
       "   u'Homestead-Miami Speedway': -0.9063448309898376,\n",
       "   u'Homesteading': 1.627761721611023,\n",
       "   u'Smallholding': -1.972685694694519,\n",
       "   u'XXNILXX': 0,\n",
       "   u'homestead': 3.6313400268554688}},\n",
       " u'Beckett finished the translation into [French] by November 1962 but amended the title .': {'boosted': 10.0,\n",
       "  u'gold': u'French language',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.339206695556641,\n",
       "   u'Canadian French': -1.1880742311477661,\n",
       "   u'Cinema of France': 1.0971864461898804,\n",
       "   u'First French Empire': -1.2245781421661377,\n",
       "   u'France': 4.348646640777588,\n",
       "   u'French': 2.183837413787842,\n",
       "   u'French American': 0.2948604226112366,\n",
       "   u'French Army': 0.6805271506309509,\n",
       "   u'French Canadian': 0.8937444090843201,\n",
       "   u'French Navy': -0.7129453420639038,\n",
       "   u'French Third Republic': 1.4646950960159302,\n",
       "   u'French colonial empire': 0.528211772441864,\n",
       "   u'French cuisine': 0.2872623801231384,\n",
       "   u'French language': 4.287905693054199,\n",
       "   u'French people': 2.7304272651672363,\n",
       "   u'French poetry': -0.6036384701728821,\n",
       "   u'Kingdom of France': 1.2161201238632202,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Catawba utilizes a pair of [Westinghouse] pressurized water reactor s .': {'boosted': 10.0,\n",
       "  u'gold': u'Westinghouse Electric Company',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.4997916221618652,\n",
       "   u'British Westinghouse': -1.7171103954315186,\n",
       "   u'George Westinghouse': -0.1258421689271927,\n",
       "   u'George Westinghouse College Prep': -4.55487060546875,\n",
       "   u'Northrop Grumman Electronic Systems': 0.9490973353385925,\n",
       "   u'Westinghouse': 3.464111328125,\n",
       "   u'Westinghouse Air Brake Company': 2.4327480792999268,\n",
       "   u'Westinghouse Aviation Gas Turbine Division': -1.6578240394592285,\n",
       "   u'Westinghouse Brake and Signal Company Ltd': -2.789897918701172,\n",
       "   u'Westinghouse Broadcasting': -0.0715220719575882,\n",
       "   u'Westinghouse Electric Company': 3.404874324798584,\n",
       "   u'Westinghouse Electric Corporation (1886)': 0.8494760394096375,\n",
       "   u'Westinghouse High School (Pittsburgh)': -2.441297769546509,\n",
       "   u'Westinghouse Licensing Corporation': -0.8131694197654724,\n",
       "   u'Westinghouse Rail Systems': -1.3562556505203247,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Epicureans who are being punished for their belief that the [soul] dies with the body .': {'boosted': 10.0,\n",
       "  u'gold': u'Soul',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -6.026315689086914,\n",
       "   u'Ancient Egyptian concept of the soul': -1.1430456638336182,\n",
       "   u'Jiva': -3.065136671066284,\n",
       "   u'Kia Soul': -3.452632427215576,\n",
       "   u'Memphis soul': -1.6923104524612427,\n",
       "   u'Motown': -1.1871192455291748,\n",
       "   u'Philadelphia Soul': 0.037584587931632996,\n",
       "   u'Soul': 0.9127137064933777,\n",
       "   u'Soul (Seal album)': -0.9484810829162598,\n",
       "   u'Soul (TV series)': -3.1150941848754883,\n",
       "   u'Soul (series)': 0.37618792057037354,\n",
       "   u'Soul music': 3.7879810333251953,\n",
       "   u'XXNILXX': 0,\n",
       "   u'\\u0100tman (Hinduism)': -3.484908103942871}},\n",
       " u'Expo closely co-operate with [Monitor] in Norway and Searchlight in the UK .': {'boosted': 10.0,\n",
       "  u'gold': u'Monitor (magazine)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.5343546867370605,\n",
       "   u'Computer monitor': 0.6759469509124756,\n",
       "   u'Monitor': 1.536192774772644,\n",
       "   u'Monitor (NBC Radio)': -1.0959665775299072,\n",
       "   u'Monitor (NHS)': -3.587529182434082,\n",
       "   u'Monitor (Polish newspaper)': -1.9094650745391846,\n",
       "   u'Monitor (TV series)': -2.518847942352295,\n",
       "   u'Monitor (comics)': -3.1261491775512695,\n",
       "   u'Monitor (horse)': 0,\n",
       "   u'Monitor (magazine)': -1.7343798875808716,\n",
       "   u'Monitor (warship)': 0.6407840847969055,\n",
       "   u'Monitor, Alberta': 0,\n",
       "   u'Monitor, Indiana': 0,\n",
       "   u'Monitor, Oregon': -2.4697861671447754,\n",
       "   u'Monitor, Washington': 0,\n",
       "   u'Monitors (comics)': -0.8526771068572998,\n",
       "   u'USS Monitor': 1.0305372476577759,\n",
       "   u'XXNILXX': 0}},\n",
       " u'He also made a short appearance in the movie [Woo] , with no lines .': {'boosted': 10.0,\n",
       "  u'gold': u'Woo',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -2.9432382583618164,\n",
       "   u'Alvin Woo': 0,\n",
       "   u'Garden Vista': 0,\n",
       "   u'John Woo': 0.9847010970115662,\n",
       "   u'Ng (surname)': -3.3563711643218994,\n",
       "   u'Shien Biau Woo': -5.685420989990234,\n",
       "   u'Woo': 1.4683161973953247,\n",
       "   u'Woo (Korean name)': 1.8341095447540283,\n",
       "   u'Woo (Ultra monster)': 0,\n",
       "   u'Woo (band)': 0,\n",
       "   u'Woo (film)': 1.1463854312896729,\n",
       "   u'Woo (soundtrack)': -0.22061149775981903,\n",
       "   u'Woo (surname)': -3.067412853240967,\n",
       "   u'Woo Alvin Tsung Han': 0,\n",
       "   u'Woo Sun-Hee': 0,\n",
       "   u'World Wide Web': 0.015000985935330391,\n",
       "   u'XXNILXX': 0}},\n",
       " u'He is celebrated on the 3rd of November -LRB- by [Maronites] particularly -RRB- , and on the 9th of October .': {'boosted': 10.0,\n",
       "  u'gold': u'Maronite Church',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.525219440460205,\n",
       "   u'Christian': 0.6287774443626404,\n",
       "   u'Maronite': 1.7920082807540894,\n",
       "   u'Maronite Christianity in Lebanon': 2.473389148712158,\n",
       "   u'Maronite Church': 1.9814718961715698,\n",
       "   u'Maronites in Cyprus': 2.638151168823242,\n",
       "   u'Maronites in Israel': -0.4126151204109192,\n",
       "   u'XXNILXX': 0}},\n",
       " u'In 2006 , the [Cardinals] moved from Sun Devil Stadium to University of Phoenix Stadium': {'boosted': 10.0,\n",
       "  u'gold': u'Arizona Cardinals',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.9397735595703125,\n",
       "   u'2009 St. Louis Cardinals season': -2.3155150413513184,\n",
       "   u'2010 St. Louis Cardinals season': -1.2704355716705322,\n",
       "   u'2011 St. Louis Cardinals season': -1.0804941654205322,\n",
       "   u'2012 St. Louis Cardinals season': -0.7625057697296143,\n",
       "   u'2013 St. Louis Cardinals season': -1.5662426948547363,\n",
       "   u'2014 St. Louis Cardinals season': -3.430497646331787,\n",
       "   u'2015 St. Louis Cardinals season': 0,\n",
       "   u'Arizona Cardinals': 0.9695585370063782,\n",
       "   u'Cardinal': -1.54612135887146,\n",
       "   u'Cardinal (bird)': -0.10024602711200714,\n",
       "   u'Cardinal (color)': -0.8184735178947449,\n",
       "   u'Cardinal (train)': 0.577366292476654,\n",
       "   u'Fernando Alberto dos Santos Cardinal': 0,\n",
       "   u'St. Louis Cardinals': 3.0910539627075195,\n",
       "   u'Stanford Cardinal': 1.3794094324111938,\n",
       "   u'XXNILXX': 0,\n",
       "   u'cardinal (Catholicism)': 2.0546798706054688}},\n",
       " u'In [2006] , the Cardinals moved from Sun Devil Stadium to University': {'boosted': 10.0,\n",
       "  u'gold': u'2006 NFL season',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.085140228271484,\n",
       "   u'2006': 4.7065205574035645,\n",
       "   u'2006 AFL season': -1.0757209062576294,\n",
       "   u'2006 ATP Tour': 0.10043220221996307,\n",
       "   u'2006 FIFA World Cup': 0.5773438811302185,\n",
       "   u'2006 J. League Division 1': -3.5191893577575684,\n",
       "   u'2006 NCAA Division I FBS football season': -0.26318687200546265,\n",
       "   u'2006 NFL season': -0.5662611722946167,\n",
       "   u'2006 NHL Entry Draft': -1.583653450012207,\n",
       "   u'2006 WTA Tour': -1.0867919921875,\n",
       "   u'2006 Winter Olympics': 1.3564382791519165,\n",
       "   u'2006 in comics': -0.5093133449554443,\n",
       "   u'2006 in film': -0.6639321446418762,\n",
       "   u'2006 in literature': -1.3626326322555542,\n",
       "   u'2006 in music': 0.11245213449001312,\n",
       "   u'2006 in sports': -0.3173409104347229,\n",
       "   u'2006 in television': -1.2220669984817505,\n",
       "   u'Canada 2006 Census': -0.24129875004291534,\n",
       "   u'Ice hockey at the 2006 Winter Olympics': 0.1783614605665207,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"In the 1956 motion picture '' [Carousel] '' , adapted from the classic Rodgers and Hammerstein stage\": {'boosted': 10.0,\n",
       "  u'gold': u'Carousel (film)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -1.5018033981323242,\n",
       "   u'Carousel': 7.011265277862549,\n",
       "   u'Carousel (1923 film)': 1.2932759523391724,\n",
       "   u'Carousel (Blink-182 song)': 0,\n",
       "   u'Carousel (Leila K album)': 0,\n",
       "   u'Carousel (TV channel)': -1.9355171918869019,\n",
       "   u'Carousel (Vanessa Carlton song)': -1.1660326719284058,\n",
       "   u'Carousel (advert)': 0.33548229932785034,\n",
       "   u'Carousel (ballet)': -2.650787353515625,\n",
       "   u'Carousel (film)': 2.326016426086426,\n",
       "   u'Carousel (musical)': 1.3997303247451782,\n",
       "   u'Carousel slide projector': -1.3905138969421387,\n",
       "   u'Cheshire Cat (Blink-182 album)': 0.3337072730064392,\n",
       "   u'Ford Carousel': 0,\n",
       "   u'Westfield Carousel': -0.983924150466919,\n",
       "   u'XXNILXX': 0}},\n",
       " u'It also had an [application] to modify this permit to flash-cut as its digital TV': {'boosted': 10.0,\n",
       "  u'gold': u'Application',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.315840244293213,\n",
       "   u'App Store (iOS)': -0.41857463121414185,\n",
       "   u'Application': 0.8996109366416931,\n",
       "   u'Application Layer': -1.1926203966140747,\n",
       "   u'Application binary interface': -3.9930343627929688,\n",
       "   u'Application for employment': -1.3453634977340698,\n",
       "   u'Application service provider': -0.4840250611305237,\n",
       "   u'Applied science': 0.6035935282707214,\n",
       "   u'Apply': -1.7451889514923096,\n",
       "   u'Construction permit': -1.1123921871185303,\n",
       "   u'Mobile app': 0.7668251991271973,\n",
       "   u'Patent application': -1.910249948501587,\n",
       "   u'XXNILXX': 0,\n",
       "   u'application software': 2.5620603561401367}},\n",
       " u'It gained the Metropolitan Borough of [Beverley] and the Rural District of Beverley -LRB- from the former': {'boosted': 10.0,\n",
       "  u'gold': u'Beverley',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.1682467460632324,\n",
       "   u'Beverley': 4.214641571044922,\n",
       "   u'Beverley (UK Parliament constituency)': 4.534533977508545,\n",
       "   u'Beverley (borough)': 2.1391773223876953,\n",
       "   u'Beverley Minster': 0.005275415256619453,\n",
       "   u'Beverley RUFC': -1.07249915599823,\n",
       "   u'Beverley Racecourse': 1.0728317499160767,\n",
       "   u'Beverley Rural District': 0.37919050455093384,\n",
       "   u'Beverley Uranium Mine': -0.643071174621582,\n",
       "   u'Beverley railway station': -0.5212279558181763,\n",
       "   u'Beverley, Saskatchewan': 0,\n",
       "   u'Beverley, South Australia': -1.4441020488739014,\n",
       "   u'Beverley, Western Australia': 2.776829242706299,\n",
       "   u'Blackburn Beverley': -0.29334157705307007,\n",
       "   u'Cliff Beverley': -2.0447258949279785,\n",
       "   u'Diocese of Beverley': 0.3029715418815613,\n",
       "   u'Electoral district of Beverley': 1.8383771181106567,\n",
       "   u'Patrick Beverley': -3.803757667541504,\n",
       "   u'Shire of Beverley': -1.2021169662475586,\n",
       "   u'XXNILXX': 0}},\n",
       " u'Notwithstanding the Chinese being considered among the first to document [stellar] activity , some of the oldest observatories on Earth are': {'boosted': 10.0,\n",
       "  u'gold': u'Stellar',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.9059014320373535,\n",
       "   u'Constellation': 0.27606624364852905,\n",
       "   u'Hyundai Stellar': -2.5609047412872314,\n",
       "   u'Movie star': -2.329659938812256,\n",
       "   u'Star system': -0.6464902758598328,\n",
       "   u'Stellar': -2.0447630882263184,\n",
       "   u'Stellar (South Korean band)': 0,\n",
       "   u'Stellar (band)': -1.390715479850769,\n",
       "   u'Stellar (magazine)': 0,\n",
       "   u'Stellar (song)': -2.652390480041504,\n",
       "   u'Stellar Awards': -2.399089813232422,\n",
       "   u'Stellar astronomy': 0.9888960719108582,\n",
       "   u'Stellar black hole': 0.011458801105618477,\n",
       "   u'Stellar evolution': 0.10020522773265839,\n",
       "   u'XXNILXX': 0,\n",
       "   u'star': 1.447156310081482}},\n",
       " u'Originally airing [The Box] , a music video jukebox , it later became a': {'boosted': 10.0,\n",
       "  u'gold': u'The Box (US TV channel)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.57169771194458,\n",
       "   u'Box': -3.821774959564209,\n",
       "   u'Box (Dive album)': 0,\n",
       "   u'Box (comics)': -2.7607717514038086,\n",
       "   u'Box (service)': -0.6979855895042419,\n",
       "   u'Box, Oklahoma': 0,\n",
       "   u'Box, Wiltshire': -5.526230335235596,\n",
       "   u'Buxus': -3.6418073177337646,\n",
       "   u'Madison Jeffries': -1.5530071258544922,\n",
       "   u'The Box': 0.4969896674156189,\n",
       "   u'The Box (2009 film)': -0.6164810657501221,\n",
       "   u'The Box (Orbital song)': -5.289047718048096,\n",
       "   u'The Box (TV series)': -0.10723944008350372,\n",
       "   u'The Box (UK TV channel)': -1.0134835243225098,\n",
       "   u'The Box (US TV channel)': 0.14600197970867157,\n",
       "   u'The Box (band)': -0.9416138529777527,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"Sun Devil Stadium hosted college football 's Fiesta Bowl from [1971] to 2006 .\": {'boosted': 10.0,\n",
       "  u'gold': u'1971 in sports',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.588537693023682,\n",
       "   u'1970\\u201371 NHL season': 0.22068600356578827,\n",
       "   u'1971': 6.238551139831543,\n",
       "   u'1971 Formula One season': -3.545407295227051,\n",
       "   u'1971 Grand Prix motorcycle racing season': -0.4626578688621521,\n",
       "   u'1971 ICF Canoe Sprint World Championships': 0.5249900221824646,\n",
       "   u'1971 Indianapolis 500': 0.1139555424451828,\n",
       "   u'1971 Major League Baseball Draft': -0.41961973905563354,\n",
       "   u'1971 NBA draft': -2.337193250656128,\n",
       "   u'1971 NFL season': -0.015233350917696953,\n",
       "   u'1971 NHL Amateur Draft': -1.5125555992126465,\n",
       "   u'1971 college football season': 0.27054983377456665,\n",
       "   u'1971 in aviation': -2.956012725830078,\n",
       "   u'1971 in film': -1.1905391216278076,\n",
       "   u'1971 in literature': -3.2396373748779297,\n",
       "   u'1971 in music': -0.047157540917396545,\n",
       "   u'1971 in spaceflight': -0.6943915486335754,\n",
       "   u'1971 in sports': 1.5737489461898804,\n",
       "   u'Eurovision Song Contest 1971': -3.3808441162109375,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"The Greek Military Police -LRB- [Greek] : '' '' -LRB- -RRB- , generally known in English\": {'boosted': 10.0,\n",
       "  u'gold': u'Greek language',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.480297088623047,\n",
       "   u'Ancient Greece': 1.9645615816116333,\n",
       "   u'Ancient Greek': 0.9991838335990906,\n",
       "   u'Greece': 5.1344313621521,\n",
       "   u'Greek': 3.1356964111328125,\n",
       "   u'Greek (TV series)': -4.380951404571533,\n",
       "   u'Greek alphabet': 0.025035785511136055,\n",
       "   u'Greek cuisine': -0.31878215074539185,\n",
       "   u'Greek language': 4.293643951416016,\n",
       "   u'Greek literature': 0.6304261088371277,\n",
       "   u'Greek mythology': 1.2044914960861206,\n",
       "   u'Greeks': 2.82106351852417,\n",
       "   u'Koine Greek': 1.1754380464553833,\n",
       "   u'Medieval Greek': -0.6575579047203064,\n",
       "   u'Modern Greek': 0.5228204131126404,\n",
       "   u'XXNILXX': 0}},\n",
       " u'The film was shot at [Bray Studios] and Oakley Court , a country house in Berkshire ,': {'boosted': 10.0,\n",
       "  u'gold': u'Bray Productions',\n",
       "  u'training': False,\n",
       "  u'vals': {u' Bray Studios (UK)': 0,\n",
       "   u'-NIL-': -4.93887186050415,\n",
       "   u'Album': -2.03991961479187,\n",
       "   u'Bray Productions': 1.8522213697433472,\n",
       "   u'Bray Studios (UK)': 4.259208679199219,\n",
       "   u'Cartoon Network Studios': 0.6408132910728455,\n",
       "   u'Culver Studios': 0.5584635138511658,\n",
       "   u'Dell Studio': -1.2890201807022095,\n",
       "   u'Lego Studios': 0,\n",
       "   u'Recording studio': -0.0781974345445633,\n",
       "   u'Studio (TV channel)': 0,\n",
       "   u'Studio (band)': -3.6823246479034424,\n",
       "   u'Studio (song)': -4.797089099884033,\n",
       "   u'Studios': 0.4885229468345642,\n",
       "   u'The Studio (magazine)': -1.8540078401565552,\n",
       "   u'XXNILXX': 0,\n",
       "   u'movie studio': 1.2687615156173706}},\n",
       " u'The station was analog TV channel 55 with 11.6 kW [ERP] and used an omnidirectional antenna .': {'boosted': 10.0,\n",
       "  u'gold': u'Effective radiated power',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.574253559112549,\n",
       "   u'ERP': 3.989020824432373,\n",
       "   u'Effective radiated power': 3.7856521606445312,\n",
       "   u'Electronic Road Pricing': 1.5669094324111938,\n",
       "   u'Enterprise Resource Planning': 2.3301608562469482,\n",
       "   u'Estonian Reform Party': -2.515695571899414,\n",
       "   u'Euthanasia Referendum Party': 0,\n",
       "   u'Event-related potential': -1.147439956665039,\n",
       "   u'New Centre-Right \\u2013 Union of the Centre': -0.5996959805488586,\n",
       "   u\"People's Revolutionary Army (Argentina)\": 2.1941709518432617,\n",
       "   u\"People's Revolutionary Army (Colombia)\": -0.7375576496124268,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"This piercing plays a prominent role in the [French] erotic novel , '' Story of O '' .\": {'boosted': 10.0,\n",
       "  u'gold': u'France',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.50047492980957,\n",
       "   u'Canadian French': -1.5356711149215698,\n",
       "   u'Cinema of France': 0.7146322131156921,\n",
       "   u'First French Empire': -3.280165195465088,\n",
       "   u'France': 2.3419463634490967,\n",
       "   u'French': -0.6595566868782043,\n",
       "   u'French American': -1.7178738117218018,\n",
       "   u'French Army': -1.2001521587371826,\n",
       "   u'French Canadian': -0.9480476975440979,\n",
       "   u'French Navy': -3.3835060596466064,\n",
       "   u'French Third Republic': -1.8496482372283936,\n",
       "   u'French colonial empire': -2.6700174808502197,\n",
       "   u'French cuisine': -0.4940250515937805,\n",
       "   u'French language': 5.044250965118408,\n",
       "   u'French people': 0.7955206036567688,\n",
       "   u'French poetry': -2.539541244506836,\n",
       "   u'Kingdom of France': -0.7734531164169312,\n",
       "   u'XXNILXX': 0}},\n",
       " u'We will reproduce hereafter the official version of the [Maronite] book of saints -LRB- Sinksar -RRB- along with the versions': {'boosted': 10.0,\n",
       "  u'gold': u'Maronite Church',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.446864128112793,\n",
       "   u'Christian': 1.1770578622817993,\n",
       "   u'Maronite': 4.622698783874512,\n",
       "   u'Maronite Christianity in Lebanon': 3.064682960510254,\n",
       "   u'Maronite Church': 1.3375434875488281,\n",
       "   u'Maronites in Cyprus': 2.818972110748291,\n",
       "   u'Maronites in Israel': -1.2814369201660156,\n",
       "   u'XXNILXX': 0}},\n",
       " u'a 391 acre -LRB- 1.6 km -RRB- peninsula reaching into [Lake Wylie] , near York , South Carolina .': {'boosted': 10.0,\n",
       "  u'gold': u'Lake Wylie, South Carolina',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -1.9818980693817139,\n",
       "   u'Adeola Wylie': 0,\n",
       "   u'Chalmers Wylie': -1.5025629997253418,\n",
       "   u'Lake Wylie': 6.925651550292969,\n",
       "   u'Lake Wylie, South Carolina': 4.068901062011719,\n",
       "   u'Laurentian Hills': -0.00354929082095623,\n",
       "   u'Pete Wylie': -4.842871189117432,\n",
       "   u'Ron Wylie': -3.242856979370117,\n",
       "   u'Wylie': -2.3127293586730957,\n",
       "   u'Wylie (person)': -1.5347156524658203,\n",
       "   u'Wylie High School (Abilene, Texas)': -0.030828429386019707,\n",
       "   u'Wylie High School (Wylie, Texas)': 0.3662208914756775,\n",
       "   u'Wylie, Taylor County, Texas': 1.0423511266708374,\n",
       "   u'Wylie, Texas': 2.417876720428467,\n",
       "   u'Wylie_transliteration': -0.977179229259491,\n",
       "   u'XXNILXX': 0}},\n",
       " u'array of revolutionary instruments including an armillary sphere , a [quadrant] , a theodolite and an astronomical sextant .': {'boosted': 10.0,\n",
       "  u'gold': u'Quadrant',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.237048625946045,\n",
       "   u'Cartesian coordinate system': -1.091528296470642,\n",
       "   u'Circular sector': -0.6077072620391846,\n",
       "   u'Galactic quadrant': -0.4366322159767151,\n",
       "   u'Operation Quadrant': 0,\n",
       "   u'Quadrant': -0.5619154572486877,\n",
       "   u'Quadrant (Joe Pass and Milt Jackson album)': 0,\n",
       "   u'Quadrant (architecture)': -1.8399620056152344,\n",
       "   u'Quadrant (instrument)': 1.0883584022521973,\n",
       "   u'Quadrant (magazine)': 0.413202166557312,\n",
       "   u'Quadrant (motorcycles)': -3.258289337158203,\n",
       "   u'Quadrant (plane geometry)': -2.6469664573669434,\n",
       "   u'Quadrant Cycle Company': -4.390680313110352,\n",
       "   u'Quadrants of Washington, D.C.': 1.212854027748108,\n",
       "   u'Quebec Conference, 1943': -2.198885679244995,\n",
       "   u'XXNILXX': 0}},\n",
       " u'became the Phoenix Cardinals -LRB- renamed the Arizona Cardinals in [1994] -RRB- .': {'boosted': 10.0,\n",
       "  u'gold': u'1994 NFL season',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.293853282928467,\n",
       "   u'1994': 5.499507427215576,\n",
       "   u'1994 AFL season': -1.5476995706558228,\n",
       "   u'1994 FIFA World Cup': 2.621973991394043,\n",
       "   u'1994 Formula One season': -0.5735269784927368,\n",
       "   u'1994 Grand Prix motorcycle racing season': -0.5645999312400818,\n",
       "   u'1994 J. League': -3.4253177642822266,\n",
       "   u'1994 NASCAR Winston Cup Series': 0.404609739780426,\n",
       "   u'1994 NCAA Division I-A football season': -0.09583146870136261,\n",
       "   u'1994 NFL season': 0.8790780901908875,\n",
       "   u'1994 NHL Entry Draft': -0.12567837536334991,\n",
       "   u'1994 Winter Olympics': -0.5246152877807617,\n",
       "   u'1994 in film': -0.19821631908416748,\n",
       "   u'1994 in literature': -2.43034029006958,\n",
       "   u'1994 in music': 0.02653639204800129,\n",
       "   u'1994 in television': 0.09815005958080292,\n",
       "   u'1994 in video gaming': 1.0253151655197144,\n",
       "   u'Eurovision Song Contest 1994': -2.7273852825164795,\n",
       "   u'United States House of Representatives elections, 1994': -2.3874948024749756,\n",
       "   u'XXNILXX': 0}},\n",
       " u'between the New York Jets and the Minnesota Vikings in [1975] .': {'boosted': 10.0,\n",
       "  u'gold': u'1975 NFL season',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.435290813446045,\n",
       "   u'1975': 6.855931758880615,\n",
       "   u'1975 Cricket World Cup': -0.7945477366447449,\n",
       "   u'1975 Formula One season': -3.842010974884033,\n",
       "   u'1975 Grand Prix motorcycle racing season': -1.3595101833343506,\n",
       "   u'1975 Indianapolis 500': -1.04026460647583,\n",
       "   u'1975 NASCAR Winston Cup Series': 0.1280701607465744,\n",
       "   u'1975 NBA draft': -1.7621874809265137,\n",
       "   u'1975 NFL season': 1.2188595533370972,\n",
       "   u'1975 NHL Amateur Draft': -0.39004307985305786,\n",
       "   u'1975 college football season': -1.6630287170410156,\n",
       "   u'1975 in aviation': -1.4766019582748413,\n",
       "   u'1975 in film': 0.13441649079322815,\n",
       "   u'1975 in literature': -1.1214208602905273,\n",
       "   u'1975 in music': -0.31876975297927856,\n",
       "   u'1975 in spaceflight': -0.6675888895988464,\n",
       "   u'1975 in sports': 1.8403412103652954,\n",
       "   u'Australian federal election, 1975': -1.3497925996780396,\n",
       "   u'Eurovision Song Contest 1975': -1.2856636047363281,\n",
       "   u'XXNILXX': 0}},\n",
       " u'extremely directional antenna , aimed southeast toward Sandy Springs and [North Atlanta] .': {'boosted': 10.0,\n",
       "  u'gold': u'North Atlanta, Georgia',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.666515827178955,\n",
       "   u'Atlanta Braves': -0.4635311961174011,\n",
       "   u'Atlanta Dream': -0.5734977126121521,\n",
       "   u'Atlanta Falcons': -0.8118565678596497,\n",
       "   u'Atlanta Hawks': -1.0230501890182495,\n",
       "   u'Atlanta Motor Speedway': 0.5294269919395447,\n",
       "   u'Atlanta Thrashers': -0.23068232834339142,\n",
       "   u'Atlanta metropolitan area': 3.834506034851074,\n",
       "   u'Atlanta, Georgia': 1.8467463254928589,\n",
       "   u'Atlanta, Texas': -2.7903971672058105,\n",
       "   u'Club Atl\\xe9tico Atlanta': -2.953425884246826,\n",
       "   u'Hartsfield\\u2013Jackson Atlanta International Airport': -0.6597430109977722,\n",
       "   u'North Atlanta': 0,\n",
       "   u'North Atlanta High School': -0.09669293463230133,\n",
       "   u'North Atlanta, Georgia': -0.9637510180473328,\n",
       "   u'Verizon Tennis Challenge': -3.171144485473633,\n",
       "   u'XXNILXX': 0}},\n",
       " u'facility became an NFL stadium in 1988 , when the [St. Louis Cardinals] moved west to Arizona and became the Phoenix Cardinals -LRB-': {'boosted': 10.0,\n",
       "  u'gold': u'Arizona Cardinals',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.231009006500244,\n",
       "   u'1930 St. Louis Cardinals season': -2.9334263801574707,\n",
       "   u'1990 St. Louis Cardinals season': -2.555494785308838,\n",
       "   u'2005 St. Louis Cardinals season': -2.630030632019043,\n",
       "   u'2006 St. Louis Cardinals season': -1.2120224237442017,\n",
       "   u'2009 St. Louis Cardinals season': -1.9043062925338745,\n",
       "   u'2011 St. Louis Cardinals season': -0.9458146095275879,\n",
       "   u'2012 St. Louis Cardinals season': -0.6078923940658569,\n",
       "   u'2014 St. Louis Cardinals season': -2.5521044731140137,\n",
       "   u'Arizona Cardinals': 1.492039442062378,\n",
       "   u'Cardinal': -1.713752269744873,\n",
       "   u'Cardinal (Catholicism)': -0.11701352894306183,\n",
       "   u'Cardinal (bird)': -0.10841663181781769,\n",
       "   u'Cardinal (color)': -1.737768530845642,\n",
       "   u'Cardinal (train)': 0.8988491892814636,\n",
       "   u'History of the St. Louis Cardinals (NFL)': 1.114303708076477,\n",
       "   u'Louis Cardinals': 0,\n",
       "   u'St Louis Cardinals': 4.24948787689209,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"home of the NFL 's Arizona Cardinals from 1988 to [2005] .\": {'boosted': 10.0,\n",
       "  u'gold': u'2005 in sports',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.30781364440918,\n",
       "   u'2005': 4.303284168243408,\n",
       "   u'2005 AFL season': -1.898169994354248,\n",
       "   u'2005 ATP Tour': 0.3334110379219055,\n",
       "   u'2005 Grand Prix motorcycle racing season': -1.5058174133300781,\n",
       "   u'2005 J. League Division 1': -3.6785621643066406,\n",
       "   u'2005 NCAA Division I-A football season': 0.5710694193840027,\n",
       "   u'2005 NFL season': 0.15658728778362274,\n",
       "   u'2005 NHL Entry Draft': -0.817282497882843,\n",
       "   u'2005 WTA Tour': 1.531549096107483,\n",
       "   u'2005 in baseball': 0.5849654078483582,\n",
       "   u'2005 in comics': -1.0784270763397217,\n",
       "   u'2005 in film': 0.49964725971221924,\n",
       "   u'2005 in literature': 0.0675908774137497,\n",
       "   u'2005 in music': 1.280221939086914,\n",
       "   u'2005 in sports': -0.3117693066596985,\n",
       "   u'2005 in television': -0.8012717962265015,\n",
       "   u'Eurovision Song Contest 2005': -1.7446606159210205,\n",
       "   u'United Kingdom general election, 2005': -3.2969908714294434,\n",
       "   u'XXNILXX': 0}},\n",
       " u'in that direction is 48.3 kW , and the nearly-circular [coverage area] centers on Sandy Springs , and ironically covers much less': {'boosted': 10.0,\n",
       "  u'gold': u'Broadcast range',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.068968296051025,\n",
       "   u'Area (EP)': 0,\n",
       "   u'Area (LDS Church)': 0.07691650092601776,\n",
       "   u'Area (band)': -2.104440927505493,\n",
       "   u'Area (nightclub)': -3.24202823638916,\n",
       "   u'Broadcast range': 3.1280031204223633,\n",
       "   u'Coverage (telecommunication)': 1.694075345993042,\n",
       "   u'List of places in Hong Kong': -2.71669340133667,\n",
       "   u'Surface area': -0.15742631256580353,\n",
       "   u'XXNILXX': 0,\n",
       "   u'area': 0.29132741689682007,\n",
       "   u'area bishop': -2.1188158988952637,\n",
       "   u'cellular network': 2.824779510498047,\n",
       "   u'coverage area': 0,\n",
       "   u'coverage map': 3.1292901039123535}},\n",
       " u'industrial experts , including W. Edwards Deming were Mitsui , [Mitsubishi] , and Sumitomo .': {'boosted': 10.0,\n",
       "  u'gold': u'Mitsubishi',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.093907356262207,\n",
       "   u'HKS (company)': -0.4922005534172058,\n",
       "   u'Mitsubishi': 3.445284366607666,\n",
       "   u'Mitsubishi Agricultural Machinery': -0.507567286491394,\n",
       "   u'Mitsubishi Corporation': 0.9663102030754089,\n",
       "   u'Mitsubishi Dynaboars': -0.7448594570159912,\n",
       "   u'Mitsubishi Heavy Industries': 2.037224769592285,\n",
       "   u'Mitsubishi Materials': 1.9237571954727173,\n",
       "   u'Mitsubishi Motors': 4.149908542633057,\n",
       "   u'Mitsubishi Motors Australia': -0.21020467579364777,\n",
       "   u'Mitsubishi Motors Mizushima F.C.': -1.235163688659668,\n",
       "   u'Mitsubishi Motors North America': -2.301126003265381,\n",
       "   u'Ralliart': -0.11210544407367706,\n",
       "   u'The Bank of Tokyo-Mitsubishi UFJ': 0.5042316317558289,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"is also mentioned in the '' Bibliotheca Orientalis '' of [Assemani] (11,255) and in the calendar of l ba the Jacobite\": {'boosted': 10.0,\n",
       "  u'gold': u'Joseph Simon Assemani',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.86362361907959,\n",
       "   u'Assemani': 5.0942487716674805,\n",
       "   u'Giuseppe Luigi Assemani': -0.5228284597396851,\n",
       "   u'Joseph Simon Assemani': -1.547372817993164,\n",
       "   u'Stefano Evodio Assemani': -3.7259011268615723,\n",
       "   u'XXNILXX': 0}},\n",
       " u'is celebrated by the Byzantines , the Jacobites and the [Maronites] .': {'boosted': 10.0,\n",
       "  u'gold': u'Maronite Church',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.79508638381958,\n",
       "   u'Christian': 1.0266441106796265,\n",
       "   u'Maronite': 2.356538772583008,\n",
       "   u'Maronite Christianity in Lebanon': 2.832977533340454,\n",
       "   u'Maronite Church': 1.9480968713760376,\n",
       "   u'Maronites in Cyprus': 3.0664892196655273,\n",
       "   u'Maronites in Israel': -1.5712521076202393,\n",
       "   u'XXNILXX': 0}},\n",
       " u'known as Kingston upon Hull , Haltemprice -RRB- was a [constituency] in the East Riding of Yorkshire , which was a': {'boosted': 10.0,\n",
       "  u'gold': u'United Kingdom constituencies',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.742003917694092,\n",
       "   u'Constituencies in Solomon Islands': -0.11510308086872101,\n",
       "   u'Constituencies of Belize': 0.3134809136390686,\n",
       "   u'Constituencies of Iceland': -0.5881951451301575,\n",
       "   u'Constituencies of Kenya': -4.062321186065674,\n",
       "   u'Constituencies of Namibia': -0.8506903648376465,\n",
       "   u'Constituencies of Singapore': 0.39354342222213745,\n",
       "   u'Constituency (France)': 1.4913116693496704,\n",
       "   u'Constituency Labour Party': 0.9349996447563171,\n",
       "   u'Electoral district': 3.623074531555176,\n",
       "   u'European Parliament constituency': 0.7228474020957947,\n",
       "   u'National apportionment of MP seats in the Swedish Parliament': -1.4281034469604492,\n",
       "   u'Parliament of Uganda': -1.2161710262298584,\n",
       "   u'Scottish Parliament constituencies and regions': 0.5800227522850037,\n",
       "   u'United Kingdom constituencies': 2.5921716690063477,\n",
       "   u'XXNILXX': 0}},\n",
       " u'notably at the 2000 Sydney Olympics where the Canadians topped [Yugoslavia] to win their group only to lose to eventual silver': {'boosted': 10.0,\n",
       "  u'gold': u'Yugoslavia national basketball team',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.391421794891357,\n",
       "   u'Football Association of Yugoslavia': -1.591151237487793,\n",
       "   u'Kingdom of Yugoslavia': -1.0588536262512207,\n",
       "   u'Serbia and Montenegro': 1.9459360837936401,\n",
       "   u'Serbia and Montenegro national football team': -0.429047167301178,\n",
       "   u'Serbia national basketball team': 1.0027064085006714,\n",
       "   u'Serbia national football team': -0.008418513461947441,\n",
       "   u'Socialist Republic of Yugoslavia': 0.03425697982311249,\n",
       "   u'XXNILXX': 0,\n",
       "   u'Yugoslavia': 4.867814540863037,\n",
       "   u'Yugoslavia in the Eurovision Song Contest': -1.259624719619751,\n",
       "   u'Yugoslavia national basketball team': 0.9007167220115662,\n",
       "   u'Yugoslavia national football team': 3.636859178543091,\n",
       "   u\"Yugoslavia women's national basketball team\": -1.6232343912124634}},\n",
       " u'remainder of the constituency contributed 11.6 % of the new [Boothferry] seat .': {'boosted': 10.0,\n",
       "  u'gold': u'Boothferry (UK Parliament constituency)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.193143844604492,\n",
       "   u'Boothferry': 3.1474075317382812,\n",
       "   u'Boothferry (UK Parliament constituency)': 2.736074924468994,\n",
       "   u'Boothferry (district)': 3.032766103744507,\n",
       "   u'XXNILXX': 0}},\n",
       " u'series of cookbook s and hosted a television series on [microwave] cooking .': {'boosted': 10.0,\n",
       "  u'gold': u'Microwave oven',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.08400297164917,\n",
       "   u'British Telecom microwave network': -4.028772354125977,\n",
       "   u'Cavalier Computer': 0,\n",
       "   u'Electromagnetic spectrum': -2.3091304302215576,\n",
       "   u'Microwave (game)': 0,\n",
       "   u'Microwave chemistry': -3.3997883796691895,\n",
       "   u'Microwave oven': 2.1765763759613037,\n",
       "   u'Microwave radiometer': -3.96148681640625,\n",
       "   u'Microwave spectroscopy': -2.2805328369140625,\n",
       "   u'Microwave transmission': 2.483978033065796,\n",
       "   u'Microwaves': -1.0289256572723389,\n",
       "   u'Waldorf Microwave': 0,\n",
       "   u'XXNILXX': 0}},\n",
       " u'success in the role of Chamo on the VH1 series [I Love New York] , Sanchez is set to star in Bachelor Party 2': {'boosted': 10.0,\n",
       "  u'gold': u'I Love New York (TV series)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.8883490562438965,\n",
       "   u'Division I (NCAA)': 1.1239930391311646,\n",
       "   u'I': -2.8525028228759766,\n",
       "   u'I Love': -1.903118371963501,\n",
       "   u'I Love New': 0,\n",
       "   u'I Love New York': 4.298277378082275,\n",
       "   u'I Love New York (TV series)': 2.9505844116210938,\n",
       "   u'I Love New York (disambiguation)': 0,\n",
       "   u'I Love New York (film)': -1.9774024486541748,\n",
       "   u'I Love New York (season 1)': -2.128324031829834,\n",
       "   u'I Love New York (song)': -0.5407266020774841,\n",
       "   u'I Love New York 2': 2.219193935394287,\n",
       "   u'Independent politician': -2.469843864440918,\n",
       "   u'Iodine': -1.9296982288360596,\n",
       "   u'Italy': 1.7177037000656128,\n",
       "   u'Meistriliiga': -1.4428472518920898,\n",
       "   u'Nemzeti Bajnoks\\xe1g I': -2.2209243774414062,\n",
       "   u'XXNILXX': 0}},\n",
       " u'system , which allowed for local purchases without the complex [Pentagon] procurement system .': {'boosted': 10.0,\n",
       "  u'gold': u'The Pentagon',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.7171101570129395,\n",
       "   u'List of Kinnikuman characters': -3.383275032043457,\n",
       "   u'Pentagon': 4.002371788024902,\n",
       "   u'Pentagon (Swedish television series)': 0,\n",
       "   u'Pentagon (TV series)': 0,\n",
       "   u'Pentagon (Washington Metro)': -1.8553367853164673,\n",
       "   u'Pentagon (computer)': -2.2947723865509033,\n",
       "   u'Pentagon (novel)': 0,\n",
       "   u'Pentagon tiling': 0,\n",
       "   u'Pentag\\xf3n': -0.5999573469161987,\n",
       "   u'The Pentagon': 3.1321234703063965,\n",
       "   u'United States Department of Defense': 1.093076229095459,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"the ESA , in particular its Special Interrogation Unit -LRB- [Greek] : '' '' , tr . '' Eidik n Anakritik\": {'boosted': 10.0,\n",
       "  u'gold': u'Greek language',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.781130790710449,\n",
       "   u'Ancient Greece': 2.204896926879883,\n",
       "   u'Ancient Greek': 0.8234397768974304,\n",
       "   u'Greece': 5.384040355682373,\n",
       "   u'Greek': 2.915614604949951,\n",
       "   u'Greek (TV series)': -4.147604942321777,\n",
       "   u'Greek alphabet': -0.508357048034668,\n",
       "   u'Greek cuisine': -0.1725437194108963,\n",
       "   u'Greek language': 4.035379886627197,\n",
       "   u'Greek literature': 0.551146924495697,\n",
       "   u'Greek mythology': 1.4768081903457642,\n",
       "   u'Greeks': 2.9815211296081543,\n",
       "   u'Koine Greek': 1.3879841566085815,\n",
       "   u'Medieval Greek': -0.5592870116233826,\n",
       "   u'Modern Greek': 0.6427034735679626,\n",
       "   u'XXNILXX': 0}},\n",
       " u'the Kingston upon Hull wards -LRB- which were transferred to [Kingston upon Hull West] -RRB- .': {'boosted': 10.0,\n",
       "  u'gold': u'Hull West (UK Parliament constituency)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -3.6008667945861816,\n",
       "   u'Canadian Football League West Division': -1.6832035779953003,\n",
       "   u'Hull West (UK Parliament constituency)': 1.3454259634017944,\n",
       "   u'Hull West by-election, 1907': 0,\n",
       "   u'Kingston upon Hull West and Hessle (UK Parliament constituency)': 2.620884656906128,\n",
       "   u'West': -3.0727949142456055,\n",
       "   u'West Africa': -1.2931609153747559,\n",
       "   u'West Province (Western Australia)': 0,\n",
       "   u'Western Conference (NBA)': -0.1348399668931961,\n",
       "   u'Western Conference (WNBA)': -3.1584277153015137,\n",
       "   u'Western Ukraine': -2.693614959716797,\n",
       "   u'Western United States': -2.0777437686920166,\n",
       "   u'Western world': -1.9626227617263794,\n",
       "   u'XXNILXX': 0,\n",
       "   u'upon Hull West': 0}},\n",
       " u'the fourth round of a Grand Slam tournament at the [2005 U.S. Open] , defeating Mashona Washington , Maria Elena Camerin and Marion': {'boosted': 10.0,\n",
       "  u'gold': u'2005 US Open (tennis)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -2.919833183288574,\n",
       "   u'2005 U.S. Open': 0,\n",
       "   u'2005 U.S. Open Golf Championship': -0.01986236684024334,\n",
       "   u'2005 US Open (tennis)': 2.322871208190918,\n",
       "   u\"2005 US Open \\u2013 Men's Singles\": 0,\n",
       "   u\"2005 US Open \\u2013 Women's Singles\": 0,\n",
       "   u'2011 US Open (tennis)': 0.021195458248257637,\n",
       "   u'2012 US Open (tennis)': -0.8398621082305908,\n",
       "   u'2013 US Open (tennis)': -3.023175001144409,\n",
       "   u'Open': -2.553891181945801,\n",
       "   u'Open (Shaznay Lewis album)': -4.164793014526367,\n",
       "   u'Open vowel': -4.392828464508057,\n",
       "   u'The Open Championship': -1.4013746976852417,\n",
       "   u'U.S. Open': -0.5198325514793396,\n",
       "   u'U.S. Open (golf)': 1.7482832670211792,\n",
       "   u'US Open (tennis)': 2.5165505409240723,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"the gap between the two peninsulas , and hems in [Green Bay] , Lake Michigan 's largest bay , to the west\": {'boosted': 10.0,\n",
       "  u'gold': u'Green Bay (Lake Michigan)',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -2.5448451042175293,\n",
       "   u\"2013\\u201314 Green Bay Phoenix men's basketball team\": 0,\n",
       "   u'Bay': -2.4088027477264404,\n",
       "   u'Bay (color)': -2.0882489681243896,\n",
       "   u'Bay County, Florida': 0.5376240611076355,\n",
       "   u'Bay County, Michigan': -0.5540042519569397,\n",
       "   u'Bay, Arkansas': -1.2933255434036255,\n",
       "   u'Bay, Laguna': -0.7673580050468445,\n",
       "   u'Bay, Somalia': -0.35652977228164673,\n",
       "   u'Green Bay': 0,\n",
       "   u'Green Bay (Lake Michigan)': 2.919921636581421,\n",
       "   u'Green Bay Gamblers': -1.577569842338562,\n",
       "   u'Green Bay Packers': 1.139625906944275,\n",
       "   u'Green Bay Phoenix': -1.3548307418823242,\n",
       "   u\"Green Bay Phoenix men's basketball\": 0.3198365569114685,\n",
       "   u'Green Bay, Wisconsin': 5.32504940032959,\n",
       "   u'University of Wisconsin\\u2013Green Bay': 0.3564268946647644,\n",
       "   u'XXNILXX': 0}},\n",
       " u'to University of Phoenix Stadium in another Phoenix suburb , [Glendale] , located on the opposite side of the metro area': {'boosted': 10.0,\n",
       "  u'gold': u'Glendale, Arizona',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.618156909942627,\n",
       "   u'Battle of Glendale': -1.3211132287979126,\n",
       "   u'Glendale': 1.7336374521255493,\n",
       "   u'Glendale (LIRR station)': -1.0209754705429077,\n",
       "   u'Glendale Transportation Center': 0.0937080830335617,\n",
       "   u'Glendale, Arizona': 3.4527087211608887,\n",
       "   u'Glendale, Calgary': -1.3522121906280518,\n",
       "   u'Glendale, California': 4.207555294036865,\n",
       "   u'Glendale, Colorado': 1.6957191228866577,\n",
       "   u'Glendale, Kentucky': -0.9201269149780273,\n",
       "   u'Glendale, Missouri': -0.7543529272079468,\n",
       "   u'Glendale, Nevada': 0.9963511824607849,\n",
       "   u'Glendale, New South Wales': -0.322026789188385,\n",
       "   u'Glendale, Ohio': -0.589089035987854,\n",
       "   u'Glendale, Oregon': 1.3262203931808472,\n",
       "   u'Glendale, Queens': 1.257493019104004,\n",
       "   u'Glendale, Salt Lake City': 0.12233094871044159,\n",
       "   u'Glendale, Skye': -2.262549638748169,\n",
       "   u'Glendale, Wisconsin': -1.5092861652374268,\n",
       "   u'XXNILXX': 0}},\n",
       " u\"to the President 's criticism of his administrative oversight of [the economy] .\": {'boosted': 10.0,\n",
       "  u'gold': u'Economy of Kazakhstan',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.607455253601074,\n",
       "   u'Economic system': -1.963536024093628,\n",
       "   u'Economy': -0.36533087491989136,\n",
       "   u'Economy of Azerbaijan': -0.01208038441836834,\n",
       "   u'Economy of Iran': -0.6619507670402527,\n",
       "   u'Economy of Kazakhstan': 0.6948415637016296,\n",
       "   u'Economy of Thailand': -0.6265572905540466,\n",
       "   u'Economy of the Han dynasty': -2.405750274658203,\n",
       "   u'Economy of the United States': 0.8813900351524353,\n",
       "   u'Economy, Indiana': -2.1061413288116455,\n",
       "   u'Economy, Nova Scotia': -4.869400501251221,\n",
       "   u'Economy, Pennsylvania': -3.2050952911376953,\n",
       "   u'Financial crisis of 2007\\u201308': 0.7921378016471863,\n",
       "   u'Great Recession': 1.3812590837478638,\n",
       "   u'Old Economy Village': -2.5690529346466064,\n",
       "   u'The economy': 0,\n",
       "   u'XXNILXX': 0,\n",
       "   u'economics': 1.5108128786087036}},\n",
       " u'us by saying that his feast day is celebrated by [Maronites] on June 3rd and that a church is dedicated to': {'boosted': 10.0,\n",
       "  u'gold': u'Maronite Church',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -5.19619083404541,\n",
       "   u'Christian': 0.8224005103111267,\n",
       "   u'Maronite': 1.4251407384872437,\n",
       "   u'Maronite Christianity in Lebanon': 2.5611720085144043,\n",
       "   u'Maronite Church': 2.2533950805664062,\n",
       "   u'Maronites in Cyprus': 2.127321243286133,\n",
       "   u'Maronites in Israel': -1.2073218822479248,\n",
       "   u'XXNILXX': 0}},\n",
       " u'win their group only to lose to eventual silver medalist [France] in the quarterfinals and finish seventh .': {'boosted': 10.0,\n",
       "  u'gold': u'France national basketball team',\n",
       "  u'training': False,\n",
       "  u'vals': {u'-NIL-': -4.148897647857666,\n",
       "   u'A1 Team France': -2.1026947498321533,\n",
       "   u'Cinema of France': -3.2035152912139893,\n",
       "   u'First French Empire': -1.266619324684143,\n",
       "   u'France': 2.177598476409912,\n",
       "   u'France national basketball team': 0.540280282497406,\n",
       "   u'France national football team': 3.224255084991455,\n",
       "   u'France national rugby league team': 1.1704362630844116,\n",
       "   u'France national rugby union team': 2.472574234008789,\n",
       "   u\"France women's national football team\": -0.6980997323989868,\n",
       "   u'French First Republic': -1.9294390678405762,\n",
       "   u'French Football Federation': 0.35004574060440063,\n",
       "   u'French Rugby Federation': -1.8833568096160889,\n",
       "   u'French Third Republic': -0.7849066853523254,\n",
       "   u'French poetry': -0.3510790467262268,\n",
       "   u'Kingdom of France': -0.5450759530067444,\n",
       "   u'LNB Pro A': 0.08101610839366913,\n",
       "   u'Second French Empire': -2.703517436981201,\n",
       "   u\"Syndicat National de l'\\xc9dition Phonographique\": 0,\n",
       "   u'XXNILXX': 0}}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWrongItems(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_results.append(('testing run', queries_exp.compute_batch(False)))\n",
    "exp_results.append(('training state', evalCurrentState(True, queries_exp.num_training_items)))\n",
    "exp_results.append(('testing state', evalCurrentState(False, queries_exp.num_training_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(4):\n",
    "    res = (i, queries_exp.compute_batch())\n",
    "    print res\n",
    "    exp_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[linear_final_l.W, linear_final_l.b]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.20308971],\n",
       "       [ 2.66998172],\n",
       "       [ 1.08015883],\n",
       "       [ 0.03807513],\n",
       "       [-0.62651217],\n",
       "       [-1.50382102],\n",
       "       [-1.76477301]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2:][0].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_lin_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "findWrongItems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surface_counts['The Silent World of Nicholas Quinn'.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for i in range(5): print queries_exp.compute_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2].get_value(borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries_exp.all_params[-2].get_value(borrow=True)[0] = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
